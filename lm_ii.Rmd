---
title: "Multiple Linear Regression Analysis"
description: |
  Linear Regression
author:
  - name: Tehilla Ostrovsky
    url: https://github.com 
    affiliation: LMU
    #affiliation_url: 
date: "`r Sys.Date()`"
output: distill::distill_article
---


Linear models can be extended to include more than one independent variable, and the equation becomes:


$\color{red}{Y{i}} = \color{blue}{{\alpha}} + \color{green}{{\beta_{1} \times X_{1} + \beta_{2} \times X_{2} + … + \beta_{n} \times X_{n}} + \color{orange}{{\epsilon_{i}}}}$


Where $\color{red}{Y_{i}}$ is the dependent variable (what we aim to predict). 

$\color{blue}\alpha$ is the intercept (the point at which the regression line crosses the y axis).


$\color{green}{{X_{1}, X_{2}, X_{n}}}$ are the independent variables (what we measure). 



$\color{green}{{\beta_{1}, \beta_{2}, ..., \beta_{n}}}$ are the slopes of the respective independent variables (tells us how important the respective variable is).


Since we also are interested in the distribution of the residuals, we also estimate $\color{orange}{{\sigma^2}}$, which represents the standard deviation of the error term $\color{orange}\epsilon$.


### In MLR (Multiple Linear Regression) analysis we ask the followiing questions:

  1) How much variance, relative to the total variability of the criterion, can all predictors explain together?
  2) Which predictor has the largest predictive contribution?
  3) What is the magnitude of the independent predictive contribution of a predictor?
  4) Does the strength, direction, and interpretation of the effect of a predictor change when considering another predictor compared to the multiple regression?
  
  
## We are going to use a fun(ny) example for this topic.  

In a parallel universe, in which Psychology students party during the semester and prior to their exams, a study was conducted...


<img src = "img/student_alcohol.jpg" style = "height:200px; width:300px;border:10px outset silver; display: block; margin-left: auto; margin-right: auto; width: 50%;"></img>



The researchers wanted to know if the amount of alcohol 🍺 consumed consumed a day prior to an exam but also the number of hours spent on preparation 📚📖 affected the student's exam results &#129351;.  
  

And so they asked students to confess about the amount of ml of alcohol (0 to &#8734;😂) they had the day before the exam and the number hours (0-100) they spent studying and how much they scored in the exam (0-💯%). 


### Here is the data they obtained:

```{r}
set.seed(123)
dat <-   data.frame(
      "Alcohol.ml" = round(runif(30, 0, 100), digit <-  0),
      "Study.Hours" = round(runif(30, 1, 50), digit <-  0),
      "Test.Results" = round(rbeta(30, 2,1) * 100, digit <-  0)
)

dat

```


### In this parallel universe the rules are often different but plotting your data is as in ours, simply a must! 

And so they did 


```{r echo=FALSE}
library(shiny)
library(shinyjs)

knitr::include_app("https://tehilla-mechera-ostrovsky.shinyapps.io/3d_LMR/", 
  height = "600px")
```


### They also tested three assumptions before making public statements about their results...


  1) They started off with the LINEARITY assumption. To examine this assumption they plotted the residuals of each UV (alcohol in ml and the number of hours spent on studying)
  
  
```{r}
library(car)
mod <-  lm(Test.Results ~ Alcohol.ml + Study.Hours, data = dat)

crPlots(mod, ylab = "residuals")
```
The blue line represents a linear trend

The pink line represents a flexible function that describes the data as closely as possible. 

When both lines are close to each other, the linearity assumption can be considered fulfilled (in our example the alcohol wins this competition).

  2) The next assumption they tested was the NORMALITY. They plotted a histogram of the residuals. Before plotting these, they remembered (of course) to standardize them.
  
```{r}
library(ggplot2)
stand_residuals <-  data.frame(
                    "stand_resid" = c(rstandard(mod)))

# Create a histogram with density line using ggplot2
ggplot(stand_residuals, aes(x = stand_resid)) +
  geom_histogram(fill = "lightblue", color = "black", bins = 15) +
  geom_density(color = "red") +
  ylim(0, 5) +
  labs(x = "residuals (standardized)", y = "Frequency / Density", 
       title = "Histogram to Examine Homoskedasticity") +
  theme_classic()

```
  This plot did not make them super happy.... 
  
  

  3) They went on 
```{r echo=FALSE}
knitr::include_app("https://tehilla-mechera-ostrovsky.shinyapps.io/Bivariat_3d/", 
  height = "600px")
```