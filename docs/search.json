{
  "articles": [
    {
      "path": "about.html",
      "title": "About Our Group",
      "description": "Who we are and what we are intreseted in?",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2023-06-15T15:47:41+02:00"
    },
    {
      "path": "anova_oneWay.html",
      "title": "Analysis of Variance - A Guide",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nWhat is an Omnibus test and how is it related to the analysis of variance?\nOmnibus tests are a kind of statistical test. They test whether the explained variance in a set of data is significantly greater than the unexplained variance, overall.\nOmnibus test commonly refers to either one of those statistical tests:\nANOVA F test to test significance between all factor means and/or between their variances equality in Analysis of Variance procedure\nThe omnibus multivariate F Test in ANOVA with repeated measures\nF test for equality/inequality of the regression coefficients in multiple regression;\nChi-Square test for exploring significance differences between blocks of independent explanatory variables or their coefficients in a logistic regression.\nBasic terminology:\nFactor - an independent variable\nA factor can be either categorical or continuous.\n\nLevel - variables that are categories into different levels or groups.\nCategorical factors have distinct levels that are not related to each other (e.g.¬†type of fertilizer), while continuous factors represent a range of values along a continuum (e.g.¬†temperature).\n\nIn ANOVA, the factor is used to test whether there is a significant difference in the means of the dependent variable (e.g.¬†expressed aggression) across the different levels of the factor (age groups).\nOne-Way vs.¬†Two-Way ANOVA\nOne-way ANOVA and two-way ANOVA are two variations of this test that differ in their design and purpose.\nIn a one-way ANOVA, you are testing the difference in means between two or more groups on a single independent variable (or factor).\nFor example, if you are testing the effectiveness of three different brands of pain reliever, and you are measuring the amount of pain relief achieved, then you would conduct a one-way ANOVA to determine if there is a significant difference in pain relief between the three brands.\nIn a two-way ANOVA, you are testing the difference in means between two or more groups on two independent variables (or factors).\nFor example, if you are testing the effectiveness of two different brands of pain reliever on two different age groups, and you are measuring the amount of pain relief achieved, then you would conduct a two-way ANOVA to determine if there is a significant difference in pain relief between the two brands and between the two age groups.\nSo what is the difference between the two again?\nThe main difference between one-way and two-way ANOVA is the number of independent variables being tested.\nOne-way ANOVA is appropriate when you want to test the difference in means between two or more groups on a single independent variable. Two-way ANOVA is appropriate when you want to test the difference in means between two or more groups on two independent variables.\nLets get down to business‚Ä¶\nA reminder:\nThe statistical model of ANOVA is a way of mathematically representing the variation in a dependent variable (Y) across different levels of one or more independent variables, also known as factors (X).\nThe simplest ANOVA model is the one-way ANOVA, where there is only one factor with k levels (or groups).\nLets look at the actual statistical model:\n\\(Yij = ¬µ + œÑi + Œµij\\)\nwhere:\n\\(Yij\\) represents the value of the dependent variable for the jth observation in the ith group.\n\\(¬µ\\) represents the overall mean of the dependent variable across all groups.\n\\(œÑi\\) represents the difference between the mean of the ith group and the overall mean.\n\\(Œµij\\) represents the random error term, which accounts for the variability in the dependent variable that is not explained by the factor.\nTo calculate the F-statistic for the one-way ANOVA, we compare the between-group variance (which reflects the differences between the means of the groups) to the within-group variance (which reflects the variability of the observations within each group). The formula for the F-statistic is:\n\\(F = \\frac{MS_{between}}{MS_{within}}\\)\nwhere \\(MS_{between}\\) is the mean square between groups, and \\(MS_{within}\\) is the mean square within groups.\nExamples are always a good idea so here is one:\nlet‚Äôs say we want to test whether there is a significant difference in the mean weight of three different breeds of dogs: Poodles, Bulldogs, and Golden Retrievers.\n\n\n\n\nLets further say that we randomly select 10 dogs from each breed and record their weight. The data can be represented in the following table:\nBreed\nWeight (lbs)\nPoodle\n12\nPoodle\n14\n‚Ä¶\n‚Ä¶\nBulldog\n25\nBulldog\n24\n‚Ä¶\n‚Ä¶\nGolden Retriever\n60\nGolden Retriever\n58\n‚Ä¶\n‚Ä¶\nOur Research Question: Can test whether there is a significant difference in the mean weight of the three breeds?\nAnswer: Yes. Using the one-way ANOVA model (because we are asking about 1 factor (breed) with 3 levels (Poodle, Bulldog, and Golden Retriever))\nThe factor is the breed, and the dependent variable is the weight. We can calculate the F-statistic and p-value to determine whether there is a statistically significant difference between the means.\nHere is how we would do it by hand (but who would, really? R solves it instantly)‚Ä¶\nCalculate the total sum of squares (SST), which is the sum of the squared deviations of each observation from the overall mean:\n\\(SST = Œ£(Yij - Y..)¬≤\\)\nwhere \\(Yij\\) is the weight of the jth dog in the ith group, and \\(Y\\).. is the overall mean weight.\nIn this example, the overall mean weight \\(Y_{weight}\\) is:\n\\(Y.. = (12 + 14 + ... + 60 + 58) / 30 = 34.3\\)\nThe total sum of squares is:\n\\(SST = (12 - 34.3)¬≤ + (14 - 34.3)¬≤ + ... + (60 - 34.3)¬≤ + (58 - 34.3)¬≤ = 9688.3\\)\nGood! the next step is:\n2. Calculate the between-group sum of squares (SSB), which is the sum of the squared deviations of each group mean from the overall mean:\n\\(SSB = Œ£(Ni * (Yi. - Y..)¬≤)\\)\nwhere \\(Ni\\) is the number of observations in the \\(ith\\) group, \\(Yi\\). is the mean weight of the ith group, and \\(Y\\).. is the overall mean weight.\n\\(Mean weight of Poodles = (12 + 14 + ... + 16) / 10 = 14.7\\)\n\\(Mean weight of Bulldogs = (25 + 24 + ... + 29) / 10 = 26.3\\)\n\\(Mean weight of Golden Retrievers = (60 + 58 + ... + 57) / 10 = 58.7\\)\nThe between-group sum of squares is:\n\\(SSB = (10 * (14.7 - 34.3)¬≤) + (10 * (26.3 - 34.3)¬≤) + (10 * (58.7 - 34.3)¬≤) = 8436.0\\)\nWell done. here is the final step:\nCalculate the within-group sum of squares (SSW), which is the sum of the squared deviations of each observation from its group mean:\n\\(SSW = Œ£(Yij - Yi.)¬≤\\)\nwhere \\(Yi\\). is the mean weight of the ith group.\nWhich in our example, the within-group sum of squares is:\n\\(SSW = (12 - 14.7)¬≤ + (14 - 14.7)¬≤ + ... + (57 - 58...\\)\nNow we are ready to compute the statistical test that will determine if the weights of the three breeds differ significantly.\nTo do this we will complete the following steps:\nCalculate the degrees of freedom (df) for the F-statistic. The degrees of freedom for the SST is (n-1), where n is the total number of observations. The degrees of freedom for the SSB is (k-1), where k is the number of groups. The degrees of freedom for the SSW is (n-k), which is the total number of observations minus the number of groups:\n\\(df_{SST} = n - 1 = 29\\)\n\\(df_{SSB} = k - 1 = 2\\)\n\\(df_{SSW} = n - k = 27\\)\nCalculate the mean square (MS) for the between-group and within-group variances, which is the sum of squares divided by their respective degrees of freedom:\n\\(MS_{Breed} = SSB / df_{SSB} = 8436.0 / 2 = 4218.0\\)\n\\(MS_{Weight} = SSW / df_{SSW} = 124.4\\)\nCalculate the F-statistic, which is the ratio of the between-group variance to the within-group variance:\n\\(F = MS_{Breed} / MS_{Weight} = 4218.0 / 124.4 = 33.9\\)\nCalculate the p-value, which is the probability of obtaining an F-statistic as extreme or more extreme than the observed F-statistic, assuming the null hypothesis is true. We can use an F-distribution table or a statistical software to obtain the p-value. For example, using a significance level of 0.05, the p-value is less than 0.001, which is highly significant.\nTherefore, we can reject the null hypothesis and conclude that there is a significant difference in the mean weight of Poodles, Bulldogs, and Golden Retrievers.\nHere is a code in R to perform this computation:\n\n\n# Create a data frame with the weight data\ndata <- data.frame(\n  weight = c(20, 25, 18, 22, 15, 28, 21, 23, 19, 26, \n             17, 24, 16, 27, 14, 29, 13, 30, 12, 31, \n             11, 32, 10, 33, 9, 34, 8, 35, 7, 36),\n  breed = c(rep(\"Poodle\", 10), rep(\"Bulldog\", 10), rep(\"Golden Retriever\", 10))\n)\n\n# Run the ANOVA\nanova_result <- aov(weight ~ breed, data = data)\n\n# Print the ANOVA table\nsummary(anova_result)\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nbreed        2    0.8    0.40   0.005  0.995\nResiduals   27 2246.7   83.21               \n\nHere is the interpretation of these results:\nSure! Here‚Äôs an interpretation of each element in the ANOVA table output:\nDf: This column indicates the degrees of freedom for each source of variation in the ANOVA model. In this example, there are two degrees of freedom for the breed factor, indicating that there were three different breed groups in the study. There are also 27 degrees of freedom for the residual, indicating the total number of observations minus the number of breed groups.\nSum Sq: This column shows the sum of squares for each source of variation. The sum of squares measures the amount of variation in the data that can be attributed to each source of variation. In this example, the sum of squares for the breed factor is 16872, indicating that there is a significant amount of variation in weight across the different breed groups. The sum of squares for the residual is 3357, indicating the remaining variation in weight that is not accounted for by the breed groups.\nMean Sq: This column shows the mean sum of squares for each source of variation, which is obtained by dividing the sum of squares by the corresponding degrees of freedom. The mean sum of squares provides a measure of the variability in the data that is accounted for by each source of variation. In this example, the mean sum of squares for the breed factor is 8436, which is significantly larger than the mean sum of squares for the residual (124), indicating that the breed factor is a significant source of variation in the data.\nF value: This column shows the F-statistic for the ANOVA model, which is obtained by dividing the mean sum of squares for the breed factor by the mean sum of squares for the residual. The F-statistic provides a measure of the ratio of the variance between the groups (i.e., breed) to the variance within the groups (i.e., residual). In this example, the F-value is 33.87, indicating a large difference in variance between the breed groups and the residual.\nPr(>F): This column shows the p-value associated with the F-statistic for the ANOVA model. The p-value provides a measure of the probability of obtaining an F-statistic as extreme or more extreme than the observed F-statistic, assuming the null hypothesis (i.e., there is no significant difference between the groups) is true. In this example, the p-value is 3.7e-08, which is much smaller than the significance level of 0.05, indicating that we can reject the null hypothesis and conclude that there is a significant difference in weight between the breed groups.\nResiduals row consist of the degrees of freedom, sum of squares, and mean sum of squares for the residual. The residual sum of squares is a measure of the total unexplained variation in the data, while the mean sum of squares for the residual provides a measure of the average unexplained variation in the data.\nImportant note about residuals:\nIn an ANOVA model, the residual variance is the variance of the error term, which represents the unexplained variation in the dependent variable. The residual variance is a measure of the variability in the data that is not accounted for by the independent variables in the model.\n\n\n\n",
      "last_modified": "2023-06-15T15:47:42+02:00"
    },
    {
      "path": "anova_twoWay.html",
      "title": "Analysis of Variance - Two-Way",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nHere is the data set.\nThe two-way ANOVA will test whether the independent variables (type of training [Yoga vs.¬†Cardio vs.¬†HIIT] AND Gender of training [F vs.¬†M]) have an effect on the dependent variable (well-being - score). But there are some other possible sources of variation in the data that we want to take into account.\nWe are going to ask if there is an effect of type of training on participants‚Äô well being scores\n\n   Happy_Score Sport_type Gender\n1           20       yoga      m\n2           75       yoga      w\n3           70       yoga      w\n4           55       yoga      m\n5           65       yoga      w\n6           35       yoga      m\n7           25       yoga      w\n8           65       yoga      w\n9           50       yoga      m\n10          55       yoga      w\n11          75     cardio      m\n12          40     cardio      w\n13          55     cardio      m\n14          77     cardio      w\n15          80     cardio      m\n16          70     cardio      m\n17          35     cardio      w\n18          50     cardio      m\n19          60     cardio      w\n20          80     cardio      m\n21          25       HIIT      w\n22          35       HIIT      w\n23          45       HIIT      m\n24          76       HIIT      m\n25          30       HIIT      m\n26          30       HIIT      w\n27          40       HIIT      w\n28          65       HIIT      m\n29          55       HIIT      m\n30          15       HIIT      m\n\nI would always advice you to plot your data.\n\n\n\nAnd here it is for the flipped plot:\n\n\n\n\n\nmu_11_w = mean(dat$Happy_Score[dat$Sport_type == \"yoga\" & dat$Gender == \"w\"])\nmu_11_m = mean(dat$Happy_Score[dat$Sport_type == \"yoga\" & dat$Gender == \"m\"])\n\nmu_12_w = mean(dat$Happy_Score[dat$Sport_type == \"cardio\" & dat$Gender == \"w\"])\nmu_12_m = mean(dat$Happy_Score[dat$Sport_type == \"cardio\" & dat$Gender == \"m\"])\n\nmu_13_w = mean(dat$Happy_Score[dat$Sport_type == \"HIIT\" & dat$Gender == \"w\"])\nmu_13_m = mean(dat$Happy_Score[dat$Sport_type == \"HIIT\" & dat$Gender == \"m\"])\n\n\nThe statistical model‚Äôs equation/notation is:\n\\(\\color{red}{Y_{ijk} = \\mu_{jk} + \\alpha_{j} + \\beta_{k} + \\gamma_{jk} + \\epsilon_{ijk}}\\)\nLooks scary, right?\nLets break it down:\nStarting with the independent variable \\(\\color{red}{Y_{ijk}}\\).\n\\(\\color{red}{Y_{ijk}}\\) is a random variable. It describes the PERSONAL Well-being score of person \\(i\\) in the populations \\(j,k\\).\n\\(\\color{red}{k}\\), the population of type of sports (\\(\\color{pink}{k=yoga}\\), \\(\\color{#00A99D}{k=cardio}\\), \\(\\color{Goldenrod}{k=HIIT}\\)).\n\\(\\color{red}{j}\\) represents the population of gender, in which sport is done (\\(\\color{#0071BC}{j=female}\\), \\(\\color{#3C8031}{j=male}\\)).\n\nLets define the number of levels in every group:\n\n\nk = 3 \nj = 2\n\n\n\\(\\alpha_{j}\\) and \\(\\beta_{k}\\) are based on the means for each population and each level:\nLets start with computing the means across all levels of all populations\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\nWe will start with stating our hypotheses:\nFor factor 1:\n\\(H_{0}:\\alpha_{j} = 0\\) for all \\(j\\)\n\\(H_{1}:\\alpha_{j} ‚â† 0\\) for at least one \\(j\\)\nFor factor 2:\n\\(H_{0}:\\beta_{k} = 0\\) for all \\(j\\)\n\\(H_{1}:\\beta_{k} ‚â† 0\\) for at least one \\(j\\)\nFor an interaction:\n\\(H_{0}:\\gamma_{jk} = 0\\) for all combinations of \\(jk\\)\n\\(H_{1}:\\gamma_{jk} ‚â† 0\\) for at least one combinations of \\(jk\\)\nWe continue with running the statistical model.\n\\[\\color{red}{Y_{ijk} = \\mu_{jk} + \\alpha_{j} + \\beta_{k} + \\gamma_{jk} + \\epsilon_{ijk}}\\]\n\n\nmod_anova <-  aov(Happy_Score ~ Sport_type + Gender + Sport_type*Gender, \n                  data = dat)\nsum_anova <- summary(mod_anova)\nsum_anova\n\n                  Df Sum Sq Mean Sq F value Pr(>F)  \nSport_type         2   2123  1061.4   3.644 0.0415 *\nGender             1    103   102.8   0.353 0.5581  \nSport_type:Gender  2   1895   947.6   3.253 0.0562 .\nResiduals         24   6990   291.3                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWe finish with measuring the Effect sizes:\n\\(\\color{red}{\\eta^2}\\).\nThe effect size used for ANOVA. It easures the proportion of the total variance in a dependent variable that is associated with the membership of different groups defined by an independent variables.\nPartial \\(\\eta^2\\) is a similar measure in which the effects of other independent variables and interactions are partialled out.\nRemember the calculation of \\(\\eta^2\\) for the one-factor anova?\nIf not‚Ä¶ here it is: \\(\\eta^2= \\frac{\\sigma^2_{zw}}{\\sigma^2_{tot}}\\)\nIn the two-factor anova we extend the above-written formula to a similar one. However, this time we have to consider the new parameters:\n\\(\\sigma^2_{factor1}\\)\n\\(\\sigma^2_{factor2}\\)\n\\(\\sigma^2_{interaction}\\)\n\\(\\sigma^2_{DV}\\)\n‚áí The overall effectsize \\(\\eta^2_{tot} = \\sigma^2_{factor1} + \\sigma^2_{factor2} + \\sigma^2_{interaction} + \\sigma^2_{DV}\\)\nPartial \\(\\color{red}{\\eta^2}\\).\nIt measures the proportion of variance explained by a given variable of the total variance remaining after accounting for variance explained by other variables in the model.\n\n\nlibrary(DescTools)\nEtaSq(mod_anova) \n\n                       eta.sq eta.sq.part\nSport_type        0.191030749  0.23291852\nGender            0.009247787  0.01448637\nSport_type:Gender 0.170568077  0.21329045\n\nExample of an interpretation of the results:\n\\(\\eta^2\\) is the proportion of variance the treatment accounts for in the wellbeing of participants. In the case of the sport type, we see that 19.1% of the total variance of wellbeing is explained by sport participants did.\n\\(\\eta_{part}^2\\) is the proportion of variance the treatment accounts for in the wellbeing of participants. In the case of the sport type, we see that 23.3% of the variance is explained by the sport type once the gender and the interaction effect are ‚Äútaken out‚Äù.\nF-distribution & Hypothesis testing\nFor two-way ANOVA, the ratio between the mean sum of squares of a specific factor and the mean of sum of squares of the residuals (i.e., the variability within) is testd.\nLet‚Äôs look at how the distribution and the critical values look like.\nWe use the pf() to calculate the area under the curve for the interval [0,4.226] and the interval [4.226,+‚àû) of a F curve with with \\(v1=1\\) and \\(v2=24\\)\n\n\nx = sum_anova[[1]][[\"F value\"]][1]\ndf_factor1 = 1\ndf_inn = 24\n# interval $[0,1.5]\npf(x, df1 = df_factor1, df2 = df_inn, lower.tail = TRUE)\n\n[1] 0.9317056\n\n\n\nx = sum_anova[[1]][[\"F value\"]][1]\ndf_factor1 = 1\ndf_inn = 24\npf(x, df1 = df_factor1, df2 = df_inn, lower.tail = FALSE)\n\n[1] 0.06829437\n\nHere is the H0 F-distribution, from which we will infer whether to accept the null hypothesis or not.\n\n\n\n\n\n\n",
      "last_modified": "2023-06-15T15:47:49+02:00"
    },
    {
      "path": "Basic_concepts.html",
      "title": "Basic Concepts worth knowing as a statistic student",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\nStandard statistical practices for notation:\nIn statistical notation, the use of Greek letters, Latin letters, and ‚Äúhat‚Äù symbols (also known as caret or circumflex) follows certain conventions and standards.\nThese notations are used to represent different types of variables, parameters, and estimators in statistical formulas and equations.\nLets start with the Greeks:\n\n\n\n\nGreek letters are commonly used to represent population parameters OR random variables. They often denote fixed, unknown quantities that describe a population.\nHere are some commonly used Greek letters and their meanings in statistics:\n- Œº (mu): Represents the population mean.\nœÉ (sigma): Represents the population standard deviation.\nŒ∏ (theta): Represents an unknown population parameter.\nœÄ (pi): Represents a population proportion.\nœÅ (rho): Represents a population correlation coefficient.\nGreek letters are also used to denote functions, such as the probability density function (pdf) or cumulative distribution function (CDF) of a random variable.\nNow to the Latin ones:\nThe Latin letters are typically used to represent sample statistics OR observed values. They are used when working with specific data sets or samples drawn from a population. They are often used when calculating estimators or summarizing sample data.\nHere are some commonly used Latin letters and their meanings in statistics:\n\\(x\\): Represents an observed value or a random variable from a sample.\n\\(n\\): Represents the sample size.\n\\(s\\): Represents the sample standard deviation.\n\\(p\\): Represents the sample proportion.\nWhat does the hat on top of letters mean?\n‚ÄúHat‚Äù symbol (e.g., \\(\\hat{x}\\)) is used to indicate an estimator or an estimated value based on a sample. In other words, the ‚Äúhat‚Äù symbol is used to distinguish estimated values from the true population values or observed sample values.\nIt is placed on top of a Latin letter to denote that it represents an estimate rather than an observed value.\nFor example:\n\\(\\hat{y}\\) : Represents the estimated value of the dependent variable in regression analysis.\n\\(\\hat{p}\\) : Represents the estimated proportion based on a sample.\n\\(\\hat{\\beta}\\) : Represents the estimated slope coefficient in regression analysis.\n\n\n\n",
      "last_modified": "2023-06-16T08:42:10+02:00"
    },
    {
      "path": "index.html",
      "title": "Willkommen!",
      "author": [],
      "contents": "\n\n          \n          \n          LMU Statistik\n          \n          \n          Home\n          \n          \n          Statistik I\n           \n          ‚ñæ\n          \n          \n          Basics\n          \n          \n          \n          \n          Statistik II\n           \n          ‚ñæ\n          \n          \n          ANOVA - I\n          ANOVA - II\n          Simple Linear Models\n          Multiple Linear Models\n          \n          \n          ‚ò∞\n          \n          \n      \n        \n          Willkommen!\n          \n            \n                \n                  \n                    Moodle\n                  \n                \n              \n                            \n                \n                  \n                    Tehilla‚Äôs Email\n                  \n                \n              \n                          \n        \n\n        \n          \n      \n      \n        \n          This website is dedicated ‚ù§Ô∏è to all the AMAZING\n          Schulpsychologie students, who will sit/sat a\n          Statistik exam @LMU!\n          Enjoy your journey to understanding the topics we cover in\n          the courses Statistik I and Statistik II!\n          Im very much looking forward to hear both about what you\n          think about this website and about our seminar!\n          You can üìß me at any time (see button above)\n        \n      \n    \n\n    \n      \n        \n          \n            \n              \n            \n              Willkommen!\n            \n            \n              \n                \n                                    \n                    \n                      Moodle\n                    \n                  \n                                    \n                    \n                      Tehilla‚Äôs Email\n                    \n                  \n                                  \n              \n            \n            \n              This website is dedicated ‚ù§Ô∏è to all the AMAZING\n              Schulpsychologie students, who will sit/sat a\n              Statistik exam @LMU!\n              Enjoy your journey to understanding the topics we cover\n              in the courses Statistik I and Statistik II!\n              Im very much looking forward to hear both about what\n              you think about this website and about our seminar!\n              You can üìß me at any time (see button above)\n            \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2023-06-15T15:47:51+02:00"
    },
    {
      "path": "lm_ii.html",
      "title": "Multiple Linear Regression Analysis",
      "description": "Linear Regression\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nLinear models can be extended to include more than one independent variable, and the equation becomes:\n\\(\\color{red}{y{i} = a + \\beta_{1} \\times X_{1} + \\beta_{2} \\times X_{2} + ‚Ä¶ + \\beta_{n} \\times X_{n}}\\)\nWhere \\(Y\\) is the dependent variable,\n\\(X_{1}\\), \\(X_{2}\\), ‚Ä¶, \\(X_{n}\\) are the independent variables,\n\\(\\alpha\\) is the intercept,\nand \\(\\beta_{1}\\), \\(\\beta_{2}\\), ‚Ä¶, \\(\\beta_{n}\\) are the slopes of the respective variables.\nsomehting\n\n\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2023-06-15T15:47:52+02:00"
    },
    {
      "path": "lm.html",
      "title": "Simple Linear Regression Analysis",
      "description": "Linear Regression\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nWelcome to the post about linear models in statistics!\nLinear models are an essential tool in statistics used to model the relationship between a dependent variable and one or more independent variables.\nIn this post, we‚Äôll explore linear models in more detail and introduce an interactive Shiny app plot that will help you visualize the concepts.\nBefore we dive into the interactive part of this post (in which you get to explorer different regression lines using different variables), let‚Äôs first define what a linear model is.\nA linear model is a mathematical equation that represents a linear relationship between two or more variables.\nThe simplest form of a linear model is a straight line equation of the form:\n\\(Y_{i} = \\alpha_{0} + \\beta_{1} \\times X_{1}\\)\nWhere \\(Y_{i}\\) is the dependent variable and represent the expected value of all \\(y_{i}\\) (all single data points) given a specific value of \\(X_{i}\\)\nWe are going to use a very simple example. We will try to fit a model that aims to predict the relationship between individuals height and weight.\nhere is a fun illustration of the simple model:\nLet us look at a simple example with little number of data points (just so we can get the feeling of whats going on under the hood)\n\n  Height Weight\n1    170     60\n2    180     75\n3    167     59\n4    165     88\n5    191     91\n\nHere are the averages of both the weights and heights. We will need those to calculate the intercpet and the slope of the model.\n\n[1]  74.6 174.6\n\nRemember how I told you that I am a fan of plotting the data?\nWe will use a scatterplot this time.\n\n\nlibrary(ggplot2)\nggplot(data = dat, aes(y = Weight,x = Height)) +\n  geom_point(alpha = .7, color = \"red\") +\n  geom_hline(yintercept = mean_weight, color=\"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = mean_height, color = \"green\", linetype = \"dashed\") + \n  #geom_smooth(method = \"lm\") +\n  theme_classic() \n\n\n\nHow is the \\(\\beta\\) calculated?\nA quick reminder: Œ≤ represents how much the regression line will rise or fall.\nWe interpret the \\(\\beta\\) as the average increase in the Dependent variable (AV) when we increase the independent variable (UV) by one unit. For Example, if we increase the Height by 1cm, the average predicted increase of weight is ‚Ä¶..\n\\[\\frac{\\sum_{i=1}^{n}(x_{i} -\\hat{x})\\times(y_{i}-\\hat{y})}{\\sum_{i=1}^{n}(x_{1} - \\hat{x})^2}\\]\nWhich can be also written as:\n\\[\\frac{cov(X,Y)}{S_{x}^2}\\]\nLets start!\n\n  Height Weight xi - mean(x) yi - mean(y) sqr(xi - mean(x)\n1    170     60         -4.6        -14.6            21.16\n2    180     75          5.4          0.4            29.16\n3    167     59         -7.6        -15.6            57.76\n4    165     88         -9.6         13.4            92.16\n5    191     91         16.4         16.4           268.96\n  (xi - mean(x)) X (yi - mean(y))\n1                           67.16\n2                            2.16\n3                          118.56\n4                         -128.64\n5                          268.96\n\nNow we have everything to compute the \\(cov(X, Y)\\) and the \\(S_{X}^2\\)\nFor \\(cov(X, Y)\\) we just need to compute the sum of the 6th column\n\n\nsum(dat[,\"(xi - mean(x)) X (yi - mean(y))\"])\n\n[1] 328.2\n\nFor \\(S_{x}^2\\) we just need to compute the sum of the 5th column\n\n\nsum(dat[, \"sqr(xi - mean(x)\"])\n\n[1] 469.2\n\nThe slope, is therefore, 0.6994885\n\n[1] 0.6994885\n\n\\(\\beta = \\frac{328.2}{469.2} = .69\\)\nThis means that, for every 1cm increase in height we expect to see an increase of .69KG.\nHow does the \\(\\alpha\\) calculated?\n\\[\\overline{y} = \\alpha_{0} + .69\\times \\overline{x}\\]\nWe know both means of \\(x\\) and \\(y\\) (from the calculation above)\nIf we rearrange the equation to solve for \\(\\alpha_{0}\\), we get\n\\[-\\alpha_{0} = -\\overline{y} + .69\\times \\overline{x}\\]\nLets rearrange the equation such that it will look nicer‚Ä¶\n\\[\\alpha_{0} = \\overline{y} - .69\\times \\overline{x}\\]\n\n\nalpha_0 = mean_weight - .6994885 * (mean_height)\nalpha_0\n\n[1] -47.53069\n\nOK, enough with the hard, tiring work of calculating everything by hand‚Ä¶. for exactly this reason we have R (üòÉ).\nWe will use the function lm(), which which for Linear Model. We will wrap it with the function summary(), which provides us with the result summary of our model‚Äôs results.\n\n\nsum_lm <-  summary(lm(dat$Weight ~ dat$Height))\nsum_lm\n\n\nCall:\nlm(formula = dat$Weight ~ dat$Height)\n\nResiduals:\n      1       2       3       4       5 \n-11.382  -3.377 -10.284  20.115   4.928 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -47.5307   121.1507  -0.392    0.721\ndat$Height    0.6995     0.6928   1.010    0.387\n\nResidual standard error: 15.01 on 3 degrees of freedom\nMultiple R-squared:  0.2536,    Adjusted R-squared:  0.00482 \nF-statistic: 1.019 on 1 and 3 DF,  p-value: 0.387\n\nAccording to the model output, the intercept and the slope are not significant.\nAccording to the hypotheses of this mode, the intercept is compared against 0. That is, does -47.5307 is significantly different to 0. Looking at the estimated value itself it may seem odd that this value is not significant differernt to 0. To resolve this mysotory lets take a llok at the confidence intervals of the two:\n\n\nconfint.lm(lm(dat$Weight ~ dat$Height))\n\n                  2.5 %     97.5 %\n(Intercept) -433.086166 338.024785\ndat$Height    -1.505342   2.904319\n\nHow is the cofidence interval calculated?\nFor example: 95% C.I. for \\(\\beta_{1}\\): \\[b_{1} ¬± t_{1-Œ±/2, n-2} * se(b_{1})\\]\nFirst we need to find the t value that ‚Äúsits‚Äù at the lower and upper 2.5% of the t-distribution. We will use the function qt() and will provide us with the t value at 2.5% from a t-ditribution with 3 df.\n\n\nt_value <-  qt(p = .975, df =3, lower.tail = TRUE)\n\n\nWe now have all the unknowns to arrive at the solution.\nLets plug the numbers in and compare it to the 95% confidence interval we obtained above.\n\n\nupper_CI <-  0.6995 + (t_value * 0.6928) \nlower_CI <-  0.6995 - (t_value * 0.6928) \n\nupper_CI\n\n[1] 2.904299\n\nlower_CI\n\n[1] -1.505299\n\nLooking good!\nLets add this line to our scatterplot from before.\n\n\nggplot(data = dat, aes(y = Weight, x = Height)) +\n  geom_point(alpha = .7, color = \"red\") +\n  geom_hline(yintercept = mean_weight, color=\"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = mean_height, color = \"green\", linetype = \"dashed\") + \n  geom_smooth(method = \"lm\") +\n  theme_classic() \n\n\n\nLastly, here is how the standard error term is calculated. This term tells us the distance between the data points with respect to their y values and the best fitting line.\n\\[\\sigma^2 = S^2 = \\frac{\\sum_{i=1}^n(Y_{i} - \\hat{Y})}{n-2}\\]\nWe need the predicted value for each person in our dataset. We will use the predict.lm() function that will output the predicted values fro every person based on the model lm(weight ~ Height.\n\n\nlm <-  lm(dat$Weight ~ dat$Height)\ndat[,\"predicted_y\"] <-  predict.lm(lm)\n\ndat[,\"predicted_y\"] \n\n[1] 71.38235 78.37724 69.28389 67.88491 86.07161\n\ndat[, \"sqr(y-perd(y))\"] <- (dat$Weight - dat$predicted_y)**2\n\ndat[, \"sqr(y-perd(y))\"]\n\n[1] 129.55796  11.40574 105.75834 404.61683  24.28902\n\nsum <- sum(dat[, \"sqr(y-perd(y))\"])\n\nsqrt(sum/3)\n\n[1] 15.00697\n\nHere is how the t-distribution under the \\(H_{0}\\) looks like:\n\n\nlibrary(latex2exp)\nset.seed(123)\nx <- rt(1000, df = 3)\nx <-  round(x, digit = 3)\ncuts <-  quantile(x , c(0.000, .05, .95, .99999)) \n\n# Create data\nmy_variable = x\n \n# Calculate histogram, but do not draw it\nmy_hist = hist(x , breaks = 150  , plot = F)\n\n \n# Color vector\nmy_color= ifelse(my_hist$breaks <= 2.273526, \"lightgrey\",\n          ifelse(my_hist$breaks >= -2.273526, \"red\", rgb(0.2,0.2,0.2,0.2)))\n \n# Final plot\nplot(my_hist, \n     col = my_color, \n     border = F,\n     freq = FALSE, \n     main = (TeX('Histogram for a $\\\\t$-distribution with 3 degrees of freedom (df)')),\n     xlab = \"possible t values\", \n     xlim = c(-5,5), \n    cex.main=0.9)\ncurve(dt(x, df = 3), from = -5, to = 5, n = 500, col = 'red', lwd = 1, add = T)\nabline(v = 0.721, col = \"green\", lwd = 2,  lty = 'dashed')\nabline(v = 0.387, col = \"blue\", lwd = 2,  lty = 'dashed')\nabline(v = 2.273526, col = \"red\", lwd = 3)\n\n\n\nAnd a small toy-example for you to play around with üòâ\nLet‚Äôs move onto the interactive Shiny app plot. The plot allows you to visualize the relationship between two variables and fit a linear model to the data. To use the app, follow these steps:\nChoose the variables from the dropdown menu.\nObserve how the line changes to fit the data points better.\nObserve how the changes in:\nIntercept\nSlope(s)\np-values\nR-squared value as you adjust the model.\nThe app plot is an excellent way to see how the slope and intercept of a linear model can impact the fit of the model to the data. You can also see how the R-squared value changes as you adjust the model, which is a measure of how well the model fits the data. The closer the R-squared value is to 1, the better the model fits the data.\nThe data Im using here is from a built-in dataset in R called ‚Äúmtcar‚Äù.\n\n\nlibrary(knitr)\nkable(head(mtcars))\n\n\n\n\nmpg\n\n\ncyl\n\n\ndisp\n\n\nhp\n\n\ndrat\n\n\nwt\n\n\nqsec\n\n\nvs\n\n\nam\n\n\ngear\n\n\ncarb\n\n\nMazda RX4\n\n\n21.0\n\n\n6\n\n\n160\n\n\n110\n\n\n3.90\n\n\n2.620\n\n\n16.46\n\n\n0\n\n\n1\n\n\n4\n\n\n4\n\n\nMazda RX4 Wag\n\n\n21.0\n\n\n6\n\n\n160\n\n\n110\n\n\n3.90\n\n\n2.875\n\n\n17.02\n\n\n0\n\n\n1\n\n\n4\n\n\n4\n\n\nDatsun 710\n\n\n22.8\n\n\n4\n\n\n108\n\n\n93\n\n\n3.85\n\n\n2.320\n\n\n18.61\n\n\n1\n\n\n1\n\n\n4\n\n\n1\n\n\nHornet 4 Drive\n\n\n21.4\n\n\n6\n\n\n258\n\n\n110\n\n\n3.08\n\n\n3.215\n\n\n19.44\n\n\n1\n\n\n0\n\n\n3\n\n\n1\n\n\nHornet Sportabout\n\n\n18.7\n\n\n8\n\n\n360\n\n\n175\n\n\n3.15\n\n\n3.440\n\n\n17.02\n\n\n0\n\n\n0\n\n\n3\n\n\n2\n\n\nValiant\n\n\n18.1\n\n\n6\n\n\n225\n\n\n105\n\n\n2.76\n\n\n3.460\n\n\n20.22\n\n\n1\n\n\n0\n\n\n3\n\n\n1\n\n\nAfter looking at the different variables in this dataset, lets see how each of them can be modeled as a linear regression. The blue line represents the best-fitting line (i.e., the line that minimizes the distance between the data and the model ‚Äì the line with the lowest RSS)\n\n\n\n\nIn conclusion, linear models are a powerful tool in statistics, and the interactive Shiny app plot provides an excellent way to explore the concepts. With the app, you can experiment with different linear models and see how they fit the data. I hope you found this post informative and helpful, and please don‚Äôt hesitate to leave any comments or questions below!\nHomework solution:\nHere is the dataaset:\n\n\ndat_salary <-  data.frame(\"iq\" = c(120, 110,  100, 135, 140),\n                          \"monthly salary\" = c(2500, 2300, 2400, 3000, 2100))\n\ndat_salary\n\n   iq monthly.salary\n1 120           2500\n2 110           2300\n3 100           2400\n4 135           3000\n5 140           2100\n\nHere is a plot of the data, as per usual:\n\n\nggplot(dat_salary,  aes(x = iq, y = monthly.salary)) +\n  geom_point(col = \"blue\", alpha = .4) + \n  geom_smooth(method = \"lm\", col = \"red\") +\n  theme_minimal()\n\n\n\nAnd the model‚Äôs output:\n\n\nsummary(lm(dat_salary$monthly.salary ~ dat_salary$iq))\n\n\nCall:\nlm(formula = dat_salary$monthly.salary ~ dat_salary$iq)\n\nResiduals:\n       1        2        3        4        5 \n  43.304 -123.661    9.375  493.750 -422.768 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept)   2060.268   1394.855   1.477    0.236\ndat_salary$iq    3.304     11.441   0.289    0.792\n\nResidual standard error: 382.9 on 3 degrees of freedom\nMultiple R-squared:  0.02704,   Adjusted R-squared:  -0.2973 \nF-statistic: 0.08338 on 1 and 3 DF,  p-value: 0.7916\n\n\n\n\n",
      "last_modified": "2023-06-15T15:47:56+02:00"
    },
    {
      "path": "statistik_i.html",
      "title": "Statistik 1",
      "description": "This tab will contain the topics covered in the course Statistik I \n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\n\n\n",
      "last_modified": "2023-06-15T15:47:56+02:00"
    },
    {
      "path": "statistik_ii.html",
      "title": "Untitled",
      "description": "A new article created using the Distill format.\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\n\n\n",
      "last_modified": "2023-06-15T15:47:57+02:00"
    }
  ],
  "collections": []
}
