{
  "articles": [
    {
      "path": "about.html",
      "title": "About Our Group",
      "description": "Who we are and what we are intreseted in?",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2023-06-29T12:56:29+02:00"
    },
    {
      "path": "anova_oneWay.html",
      "title": "Analysis of Variance - A Guide",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nWhat is an Omnibus test and how is it related to the analysis of variance?\nOmnibus tests are a kind of statistical test. They test whether the explained variance in a set of data is significantly greater than the unexplained variance, overall.\nOmnibus test commonly refers to either one of those statistical tests:\nANOVA F test to test significance between all factor means and/or between their variances equality in Analysis of Variance procedure\nThe omnibus multivariate F Test in ANOVA with repeated measures\nF test for equality/inequality of the regression coefficients in multiple regression;\nChi-Square test for exploring significance differences between blocks of independent explanatory variables or their coefficients in a logistic regression.\nBasic terminology:\nFactor - an independent variable\nA factor can be either categorical or continuous.\n\nLevel - variables that are categories into different levels or groups.\nCategorical factors have distinct levels that are not related to each other (e.g.¬†type of fertilizer), while continuous factors represent a range of values along a continuum (e.g.¬†temperature).\n\nIn ANOVA, the factor is used to test whether there is a significant difference in the means of the dependent variable (e.g.¬†expressed aggression) across the different levels of the factor (age groups).\nOne-Way vs.¬†Two-Way ANOVA\nOne-way ANOVA and two-way ANOVA are two variations of this test that differ in their design and purpose.\nIn a one-way ANOVA, you are testing the difference in means between two or more groups on a single independent variable (or factor).\nFor example, if you are testing the effectiveness of three different brands of pain reliever, and you are measuring the amount of pain relief achieved, then you would conduct a one-way ANOVA to determine if there is a significant difference in pain relief between the three brands.\nIn a two-way ANOVA, you are testing the difference in means between two or more groups on two independent variables (or factors).\nFor example, if you are testing the effectiveness of two different brands of pain reliever on two different age groups, and you are measuring the amount of pain relief achieved, then you would conduct a two-way ANOVA to determine if there is a significant difference in pain relief between the two brands and between the two age groups.\nSo what is the difference between the two again?\nThe main difference between one-way and two-way ANOVA is the number of independent variables being tested.\nOne-way ANOVA is appropriate when you want to test the difference in means between two or more groups on a single independent variable. Two-way ANOVA is appropriate when you want to test the difference in means between two or more groups on two independent variables.\nLets get down to business‚Ä¶\nA reminder:\nThe statistical model of ANOVA is a way of mathematically representing the variation in a dependent variable (Y) across different levels of one or more independent variables, also known as factors (X).\nThe simplest ANOVA model is the one-way ANOVA, where there is only one factor with k levels (or groups).\nLets look at the actual statistical model:\n\\(Yij = ¬µ + œÑi + Œµij\\)\nwhere:\n\\(Yij\\) represents the value of the dependent variable for the jth observation in the ith group.\n\\(¬µ\\) represents the overall mean of the dependent variable across all groups.\n\\(œÑi\\) represents the difference between the mean of the ith group and the overall mean.\n\\(Œµij\\) represents the random error term, which accounts for the variability in the dependent variable that is not explained by the factor.\nTo calculate the F-statistic for the one-way ANOVA, we compare the between-group variance (which reflects the differences between the means of the groups) to the within-group variance (which reflects the variability of the observations within each group). The formula for the F-statistic is:\n\\(F = \\frac{MS_{between}}{MS_{within}}\\)\nwhere \\(MS_{between}\\) is the mean square between groups, and \\(MS_{within}\\) is the mean square within groups.\nExamples are always a good idea so here is one:\nlet‚Äôs say we want to test whether there is a significant difference in the mean weight of three different breeds of dogs: Poodles, Bulldogs, and Golden Retrievers.\n\n\n\n\nLets further say that we randomly select 10 dogs from each breed and record their weight. The data can be represented in the following table:\nBreed\nWeight (lbs)\nPoodle\n12\nPoodle\n14\n‚Ä¶\n‚Ä¶\nBulldog\n25\nBulldog\n24\n‚Ä¶\n‚Ä¶\nGolden Retriever\n60\nGolden Retriever\n58\n‚Ä¶\n‚Ä¶\nOur Research Question: Can test whether there is a significant difference in the mean weight of the three breeds?\nAnswer: Yes. Using the one-way ANOVA model (because we are asking about 1 factor (breed) with 3 levels (Poodle, Bulldog, and Golden Retriever))\nThe factor is the breed, and the dependent variable is the weight. We can calculate the F-statistic and p-value to determine whether there is a statistically significant difference between the means.\nHere is how we would do it by hand (but who would, really? R solves it instantly)‚Ä¶\nCalculate the total sum of squares (SST), which is the sum of the squared deviations of each observation from the overall mean:\n\\(SST = Œ£(Yij - Y..)¬≤\\)\nwhere \\(Yij\\) is the weight of the jth dog in the ith group, and \\(Y\\).. is the overall mean weight.\nIn this example, the overall mean weight \\(Y_{weight}\\) is:\n\\(Y.. = (12 + 14 + ... + 60 + 58) / 30 = 34.3\\)\nThe total sum of squares is:\n\\(SST = (12 - 34.3)¬≤ + (14 - 34.3)¬≤ + ... + (60 - 34.3)¬≤ + (58 - 34.3)¬≤ = 9688.3\\)\nGood! the next step is:\n2. Calculate the between-group sum of squares (SSB), which is the sum of the squared deviations of each group mean from the overall mean:\n\\(SSB = Œ£(Ni * (Yi. - Y..)¬≤)\\)\nwhere \\(Ni\\) is the number of observations in the \\(ith\\) group, \\(Yi\\). is the mean weight of the ith group, and \\(Y\\).. is the overall mean weight.\n\\(Mean weight of Poodles = (12 + 14 + ... + 16) / 10 = 14.7\\)\n\\(Mean weight of Bulldogs = (25 + 24 + ... + 29) / 10 = 26.3\\)\n\\(Mean weight of Golden Retrievers = (60 + 58 + ... + 57) / 10 = 58.7\\)\nThe between-group sum of squares is:\n\\(SSB = (10 * (14.7 - 34.3)¬≤) + (10 * (26.3 - 34.3)¬≤) + (10 * (58.7 - 34.3)¬≤) = 8436.0\\)\nWell done. here is the final step:\nCalculate the within-group sum of squares (SSW), which is the sum of the squared deviations of each observation from its group mean:\n\\(SSW = Œ£(Yij - Yi.)¬≤\\)\nwhere \\(Yi\\). is the mean weight of the ith group.\nWhich in our example, the within-group sum of squares is:\n\\(SSW = (12 - 14.7)¬≤ + (14 - 14.7)¬≤ + ... + (57 - 58...\\)\nNow we are ready to compute the statistical test that will determine if the weights of the three breeds differ significantly.\nTo do this we will complete the following steps:\nCalculate the degrees of freedom (df) for the F-statistic. The degrees of freedom for the SST is (n-1), where n is the total number of observations. The degrees of freedom for the SSB is (k-1), where k is the number of groups. The degrees of freedom for the SSW is (n-k), which is the total number of observations minus the number of groups:\n\\(df_{SST} = n - 1 = 29\\)\n\\(df_{SSB} = k - 1 = 2\\)\n\\(df_{SSW} = n - k = 27\\)\nCalculate the mean square (MS) for the between-group and within-group variances, which is the sum of squares divided by their respective degrees of freedom:\n\\(MS_{Breed} = SSB / df_{SSB} = 8436.0 / 2 = 4218.0\\)\n\\(MS_{Weight} = SSW / df_{SSW} = 124.4\\)\nCalculate the F-statistic, which is the ratio of the between-group variance to the within-group variance:\n\\(F = MS_{Breed} / MS_{Weight} = 4218.0 / 124.4 = 33.9\\)\nCalculate the p-value, which is the probability of obtaining an F-statistic as extreme or more extreme than the observed F-statistic, assuming the null hypothesis is true. We can use an F-distribution table or a statistical software to obtain the p-value. For example, using a significance level of 0.05, the p-value is less than 0.001, which is highly significant.\nTherefore, we can reject the null hypothesis and conclude that there is a significant difference in the mean weight of Poodles, Bulldogs, and Golden Retrievers.\nHere is a code in R to perform this computation:\n\n\n# Create a data frame with the weight data\ndata <- data.frame(\n  weight = c(20, 25, 18, 22, 15, 28, 21, 23, 19, 26, \n             17, 24, 16, 27, 14, 29, 13, 30, 12, 31, \n             11, 32, 10, 33, 9, 34, 8, 35, 7, 36),\n  breed = c(rep(\"Poodle\", 10), rep(\"Bulldog\", 10), rep(\"Golden Retriever\", 10))\n)\n\n# Run the ANOVA\nanova_result <- aov(weight ~ breed, data = data)\n\n# Print the ANOVA table\nsummary(anova_result)\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nbreed        2    0.8    0.40   0.005  0.995\nResiduals   27 2246.7   83.21               \n\nHere is the interpretation of these results:\nSure! Here‚Äôs an interpretation of each element in the ANOVA table output:\nDf: This column indicates the degrees of freedom for each source of variation in the ANOVA model. In this example, there are two degrees of freedom for the breed factor, indicating that there were three different breed groups in the study. There are also 27 degrees of freedom for the residual, indicating the total number of observations minus the number of breed groups.\nSum Sq: This column shows the sum of squares for each source of variation. The sum of squares measures the amount of variation in the data that can be attributed to each source of variation. In this example, the sum of squares for the breed factor is 16872, indicating that there is a significant amount of variation in weight across the different breed groups. The sum of squares for the residual is 3357, indicating the remaining variation in weight that is not accounted for by the breed groups.\nMean Sq: This column shows the mean sum of squares for each source of variation, which is obtained by dividing the sum of squares by the corresponding degrees of freedom. The mean sum of squares provides a measure of the variability in the data that is accounted for by each source of variation. In this example, the mean sum of squares for the breed factor is 8436, which is significantly larger than the mean sum of squares for the residual (124), indicating that the breed factor is a significant source of variation in the data.\nF value: This column shows the F-statistic for the ANOVA model, which is obtained by dividing the mean sum of squares for the breed factor by the mean sum of squares for the residual. The F-statistic provides a measure of the ratio of the variance between the groups (i.e., breed) to the variance within the groups (i.e., residual). In this example, the F-value is 33.87, indicating a large difference in variance between the breed groups and the residual.\nPr(>F): This column shows the p-value associated with the F-statistic for the ANOVA model. The p-value provides a measure of the probability of obtaining an F-statistic as extreme or more extreme than the observed F-statistic, assuming the null hypothesis (i.e., there is no significant difference between the groups) is true. In this example, the p-value is 3.7e-08, which is much smaller than the significance level of 0.05, indicating that we can reject the null hypothesis and conclude that there is a significant difference in weight between the breed groups.\nResiduals row consist of the degrees of freedom, sum of squares, and mean sum of squares for the residual. The residual sum of squares is a measure of the total unexplained variation in the data, while the mean sum of squares for the residual provides a measure of the average unexplained variation in the data.\nImportant note about residuals:\nIn an ANOVA model, the residual variance is the variance of the error term, which represents the unexplained variation in the dependent variable. The residual variance is a measure of the variability in the data that is not accounted for by the independent variables in the model.\n\n\n\n",
      "last_modified": "2023-06-29T12:56:30+02:00"
    },
    {
      "path": "anova_twoWay.html",
      "title": "Analysis of Variance - Two-Way",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nHere is the data set.\nThe two-way ANOVA will test whether the independent variables (type of training [Yoga vs.¬†Cardio vs.¬†HIIT] AND Gender of training [F vs.¬†M]) have an effect on the dependent variable (well-being - score). But there are some other possible sources of variation in the data that we want to take into account.\nWe are going to ask if there is an effect of type of training on participants‚Äô well being scores\n\n   Happy_Score Sport_type Gender\n1           20       yoga      m\n2           75       yoga      w\n3           70       yoga      w\n4           55       yoga      m\n5           65       yoga      w\n6           35       yoga      m\n7           25       yoga      w\n8           65       yoga      w\n9           50       yoga      m\n10          55       yoga      w\n11          75     cardio      m\n12          40     cardio      w\n13          55     cardio      m\n14          77     cardio      w\n15          80     cardio      m\n16          70     cardio      m\n17          35     cardio      w\n18          50     cardio      m\n19          60     cardio      w\n20          80     cardio      m\n21          25       HIIT      w\n22          35       HIIT      w\n23          45       HIIT      m\n24          76       HIIT      m\n25          30       HIIT      m\n26          30       HIIT      w\n27          40       HIIT      w\n28          65       HIIT      m\n29          55       HIIT      m\n30          15       HIIT      m\n\nI would always advice you to plot your data.\n\n\n\nAnd here it is for the flipped plot:\n\n\n\n\n\nmu_11_w = mean(dat$Happy_Score[dat$Sport_type == \"yoga\" & dat$Gender == \"w\"])\nmu_11_m = mean(dat$Happy_Score[dat$Sport_type == \"yoga\" & dat$Gender == \"m\"])\n\nmu_12_w = mean(dat$Happy_Score[dat$Sport_type == \"cardio\" & dat$Gender == \"w\"])\nmu_12_m = mean(dat$Happy_Score[dat$Sport_type == \"cardio\" & dat$Gender == \"m\"])\n\nmu_13_w = mean(dat$Happy_Score[dat$Sport_type == \"HIIT\" & dat$Gender == \"w\"])\nmu_13_m = mean(dat$Happy_Score[dat$Sport_type == \"HIIT\" & dat$Gender == \"m\"])\n\n\nThe statistical model‚Äôs equation/notation is:\n\\(\\color{red}{Y_{ijk} = \\mu_{jk} + \\alpha_{j} + \\beta_{k} + \\gamma_{jk} + \\epsilon_{ijk}}\\)\nLooks scary, right?\nLets break it down:\nStarting with the independent variable \\(\\color{red}{Y_{ijk}}\\).\n\\(\\color{red}{Y_{ijk}}\\) is a random variable. It describes the PERSONAL Well-being score of person \\(i\\) in the populations \\(j,k\\).\n\\(\\color{red}{k}\\), the population of type of sports (\\(\\color{pink}{k=yoga}\\), \\(\\color{#00A99D}{k=cardio}\\), \\(\\color{Goldenrod}{k=HIIT}\\)).\n\\(\\color{red}{j}\\) represents the population of gender, in which sport is done (\\(\\color{#0071BC}{j=female}\\), \\(\\color{#3C8031}{j=male}\\)).\n\nLets define the number of levels in every group:\n\n\nk = 3 \nj = 2\n\n\n\\(\\alpha_{j}\\) and \\(\\beta_{k}\\) are based on the means for each population and each level:\nLets start with computing the means across all levels of all populations\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\nWe will start with stating our hypotheses:\nFor factor 1:\n\\(H_{0}:\\alpha_{j} = 0\\) for all \\(j\\)\n\\(H_{1}:\\alpha_{j} ‚â† 0\\) for at least one \\(j\\)\nFor factor 2:\n\\(H_{0}:\\beta_{k} = 0\\) for all \\(j\\)\n\\(H_{1}:\\beta_{k} ‚â† 0\\) for at least one \\(j\\)\nFor an interaction:\n\\(H_{0}:\\gamma_{jk} = 0\\) for all combinations of \\(jk\\)\n\\(H_{1}:\\gamma_{jk} ‚â† 0\\) for at least one combinations of \\(jk\\)\nWe continue with running the statistical model.\n\\[\\color{red}{Y_{ijk} = \\mu_{jk} + \\alpha_{j} + \\beta_{k} + \\gamma_{jk} + \\epsilon_{ijk}}\\]\n\n\nmod_anova <-  aov(Happy_Score ~ Sport_type + Gender + Sport_type*Gender, \n                  data = dat)\nsum_anova <- summary(mod_anova)\nsum_anova\n\n                  Df Sum Sq Mean Sq F value Pr(>F)  \nSport_type         2   2123  1061.4   3.644 0.0415 *\nGender             1    103   102.8   0.353 0.5581  \nSport_type:Gender  2   1895   947.6   3.253 0.0562 .\nResiduals         24   6990   291.3                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWe finish with measuring the Effect sizes:\n\\(\\color{red}{\\eta^2}\\).\nThe effect size used for ANOVA. It easures the proportion of the total variance in a dependent variable that is associated with the membership of different groups defined by an independent variables.\nPartial \\(\\eta^2\\) is a similar measure in which the effects of other independent variables and interactions are partialled out.\nRemember the calculation of \\(\\eta^2\\) for the one-factor anova?\nIf not‚Ä¶ here it is: \\(\\eta^2= \\frac{\\sigma^2_{zw}}{\\sigma^2_{tot}}\\)\nIn the two-factor anova we extend the above-written formula to a similar one. However, this time we have to consider the new parameters:\n\\(\\sigma^2_{factor1}\\)\n\\(\\sigma^2_{factor2}\\)\n\\(\\sigma^2_{interaction}\\)\n\\(\\sigma^2_{DV}\\)\n‚áí The overall effectsize \\(\\eta^2_{tot} = \\sigma^2_{factor1} + \\sigma^2_{factor2} + \\sigma^2_{interaction} + \\sigma^2_{DV}\\)\nPartial \\(\\color{red}{\\eta^2}\\).\nIt measures the proportion of variance explained by a given variable of the total variance remaining after accounting for variance explained by other variables in the model.\n\n\nlibrary(DescTools)\nEtaSq(mod_anova) \n\n                       eta.sq eta.sq.part\nSport_type        0.191030749  0.23291852\nGender            0.009247787  0.01448637\nSport_type:Gender 0.170568077  0.21329045\n\nExample of an interpretation of the results:\n\\(\\eta^2\\) is the proportion of variance the treatment accounts for in the wellbeing of participants. In the case of the sport type, we see that 19.1% of the total variance of wellbeing is explained by sport participants did.\n\\(\\eta_{part}^2\\) is the proportion of variance the treatment accounts for in the wellbeing of participants. In the case of the sport type, we see that 23.3% of the variance is explained by the sport type once the gender and the interaction effect are ‚Äútaken out‚Äù.\nF-distribution & Hypothesis testing\nFor two-way ANOVA, the ratio between the mean sum of squares of a specific factor and the mean of sum of squares of the residuals (i.e., the variability within) is testd.\nLet‚Äôs look at how the distribution and the critical values look like.\nWe use the pf() to calculate the area under the curve for the interval [0,4.226] and the interval [4.226,+‚àû) of a F curve with with \\(v1=1\\) and \\(v2=24\\)\n\n\nx = sum_anova[[1]][[\"F value\"]][1]\ndf_factor1 = 1\ndf_inn = 24\n# interval $[0,1.5]\npf(x, df1 = df_factor1, df2 = df_inn, lower.tail = TRUE)\n\n[1] 0.9317056\n\n\n\nx = sum_anova[[1]][[\"F value\"]][1]\ndf_factor1 = 1\ndf_inn = 24\npf(x, df1 = df_factor1, df2 = df_inn, lower.tail = FALSE)\n\n[1] 0.06829437\n\nHere is the H0 F-distribution, from which we will infer whether to accept the null hypothesis or not.\n\n\n\n\n\n\n",
      "last_modified": "2023-06-29T12:56:37+02:00"
    },
    {
      "path": "Basic_concepts_ii.html",
      "title": "Distributions in Statistics",
      "description": "An introduction to Distributions in Statistics.\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nHere is a list of distributions used in hypothesis testing, along with Examples and their Parameters:\n1) Normal Distribution:\nExample: Testing the mean weight of a population.\nParameters: Mean (Œº) and standard deviation (œÉ).\n\n\n\n2) t-Distribution:\nExample: Testing the mean score of a small sample.\nParameters: Degrees of freedom (df), which depend on the sample size.\nThe t-distribution test is used to test the significance of individual coefficients or variables in a statistical model.\nIt evaluates the null hypothesis that a specific coefficient is zero, indicating that the corresponding variable has no significant effect on the dependent variable.\n\n\n\n3) Chi-Square Distribution:\nExample: Testing the independence of categorical variables.\nParameters: Degrees of freedom (df), which depend on the number of categories being compared.\n\n\n\n4) F-Distribution:\nExample: Testing the equality of variances in ANOVA OR in linear regression models.\nParameters: Degrees of freedom for the numerator and denominator (\\(df_1\\), \\(df_2\\)), which depend on the number of groups being compared.\nIn statistical analysis, an omnibus (the word ‚Äúomnibus‚Äù comes from Latin and it means ‚Äúfor all‚Äù or ‚Äúincluding everything) test examines the overall significance of a group of variables or model coefficients rather than evaluating them individually. This comparison among multiple Parameters follows an F-distribution.\nIn the F-distribution, there are two types of degrees of freedom:\nNumerator degrees of freedom (\\(df_1\\)): This represents the degrees of freedom associated with the variability explained by the model or the effect of interest. In the context of a multiple linear regression, the numerator degrees of freedom are typically associated with the number of predictors or independent variables in the model.\nDenominator degrees of freedom (\\(df_2\\)): This represents the degrees of freedom associated with the variability within the model or the residual error. In a multiple linear regression, the denominator degrees of freedom are often associated with the number of observations minus the number of predictors.\nIn summary: The F-distribution is used in hypothesis testing and significance testing in statistical analyses such as analysis of variance (ANOVA) and regression analysis.\n\n\n\n5) Binomial Distribution:\nExample: Testing the proportion of success/failure outcomes.\nParameters: Number of trials (n) and probability of success (p).\n\n\n\n6) Poisson Distribution:\nExample: Testing the occurrence of rare events.\nParameter: Rate parameter (Œª), which represents the average rate of event occurrence.\n7) Exponential Distribution:\nExample: Testing the time between events.\nParameter: Rate parameter (Œª), which represents the average rate of event occurrence.\n8) Gamma Distribution:\nExample: Testing the shape parameter of a distribution.\nParameters: Shape parameter (Œ±) and rate parameter (Œ≤), which determine the shape and scale of the distribution.\n\n\n\n",
      "last_modified": "2023-06-29T12:56:38+02:00"
    },
    {
      "path": "Basic_concepts.html",
      "title": "Basic Concepts worth knowing as a statistics student",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\nStandard statistical practices for notation:\nIn statistical notation, the use of Greek letters, Latin letters, and ‚Äúhat‚Äù symbols (also known as caret or circumflex) follows certain conventions and standards.\nThese notations are used to represent different types of variables, parameters, and estimators in statistical formulas and equations.\nLets start with the Greeks:\n\n\n\n\nGreek letters are commonly used to represent population parameters OR random variables. They often denote fixed, unknown quantities that describe a population.\nHere are some commonly used Greek letters and their meanings in statistics:\n- Œº (mu): Represents the population mean.\nœÉ (sigma): Represents the population standard deviation.\nŒ∏ (theta): Represents an unknown population parameter.\nœÄ (pi): Represents a population proportion.\nœÅ (rho): Represents a population correlation coefficient.\nGreek letters are also used to denote functions, such as the probability density function (pdf) or cumulative distribution function (CDF) of a random variable.\nNow to the Latin ones:\nThe Latin letters are typically used to represent sample statistics OR observed values. They are used when working with specific data sets or samples drawn from a population. They are often used when calculating estimators or summarizing sample data.\nHere are some commonly used Latin letters and their meanings in statistics:\n\\(x\\): Represents an observed value or a random variable from a sample.\n\\(n\\): Represents the sample size.\n\\(s\\): Represents the sample standard deviation.\n\\(p\\): Represents the sample proportion.\nWhat does the hat on top of letters mean?\n‚ÄúHat‚Äù symbol (e.g., \\(\\hat{x}\\)) is used to indicate an estimator or an estimated value based on a sample. In other words, the ‚Äúhat‚Äù symbol is used to distinguish estimated values from the true population values or observed sample values.\nIt is placed on top of a Latin letter to denote that it represents an estimate rather than an observed value.\nFor example:\n\\(\\hat{y}\\) : Represents the estimated value of the dependent variable in regression analysis.\n\\(\\hat{p}\\) : Represents the estimated proportion based on a sample.\n\\(\\hat{\\beta}\\) : Represents the estimated slope coefficient in regression analysis.\nMathematicians often use set notation to denote all values that can be taken by a variable or a range of values. The notation typically involves curly braces { } and a condition or rule specifying the characteristics of the values. Here are a few common notations:\nSet Builder Notation:\n{x | condition} denotes a set of values ‚Äúx‚Äù that satisfy the specified condition. For\nexample, {x | x > 0} represents the set of all positive real numbers.\nInterval Notation:\n(a, b) represents an open interval that includes all real numbers greater than ‚Äúa‚Äù and less than ‚Äúb‚Äù. For example, (0, 1) represents the interval between 0 and 1 (excluding the endpoints).\n[a, b] represents a closed interval that includes all real numbers greater than or equal to ‚Äúa‚Äù and less than or equal to ‚Äúb‚Äù. For example, [0, 1] represents the interval including both 0 and 1.\nEnumeration Notation:\n{x‚ÇÅ, x‚ÇÇ, x‚ÇÉ, ‚Ä¶} denotes a set of specific values ‚Äúx‚ÇÅ, x‚ÇÇ, x‚ÇÉ, ‚Ä¶‚Äù where the ellipsis (‚Ä¶) indicates that the sequence continues indefinitely.\ni ‚àà {1, 2, 3, ‚Ä¶, n}:\nThis notation indicates that the index ‚Äúi‚Äù belongs to the set of values {1, 2, 3, ‚Ä¶, n}.\nThe ellipsis (‚Ä¶) represents a continuation of the sequence until ‚Äún‚Äù. For example, if you have a dataset with 100 observations, the notation would be:\ni ‚àà {1, 2, 3, ‚Ä¶, 100}.\n\n\n\n",
      "last_modified": "2023-06-29T12:56:39+02:00"
    },
    {
      "path": "Decision_diagramm.html",
      "title": "How to decide which statistical test to run?",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\n\n\n\n\n\n\n",
      "last_modified": "2023-07-05T11:44:37+02:00"
    },
    {
      "path": "effectsize_lm.html",
      "title": "Effectsize(s) in Simple Linear Regression",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nA short introduction to effect sizes and their role in linear regression:\nIn linear regression analysis, effect sizes provide valuable information about the strength and direction of the relationship between the dependent variable and the independent variables. They quantify the magnitude of the effect of the independent variables on the outcome of interest.\nBefore we dive into the task of calculating the effect size for our linear models I must remind you of 4 assumptions we rely on for running linear models:\nLinearity: Linearity: The relationship between the dependent variable and the independent variables is assumed to be linear. This means that the effect of each independent variable on the dependent variable is constant and additive.\nNormality: The residuals (the differences between the observed values and the predicted values) are assumed to follow a normal distribution. This assumption implies that the errors are normally distributed with a mean of zero.\nHomoscedasticity: The variance of the errors (residuals) is assumed to be constant across all levels of the independent variables. In other words, the spread or dispersion of the residuals should be roughly the same for all values of the independent variables.\nNo outliers\nWe shall discuss these at the end of this blog so you‚Äôll need to be patient üòÉ\n\n\n\nOK! \nBack\nto\nEffect Sizes\n\n\n\nOne commonly used effect size in linear regression is R-squared (R¬≤). This is the effect size that you will find in your R-output when you calculate your linear regression in R (e.g., with the lm(AV ~ UV, data = datensatz))\nImportant facts aboput R squared\n\\(R^2\\) represents the proportion of variance in the dependent variable that can be explained by the independent variables in the regression model.\n\\(R^2\\) ranges from 0 to 1, with higher values indicating a stronger relationship between the variables.\n\\(R^2\\) provides an overall measure of the goodness of fit of the regression model and helps assess the amount of variation in the dependent variable that is accounted for by the independent variables.\nTo compute \\(R^2\\) for the regression model we need 3 pieces of information:\nThe OBSERVED values of the observed dependent variable \\(y\\) for each participant\n\\(\\bar{y}\\) which is the mean across all observed \\(\\y\\) values.\n\\(\\hat{y}\\) the predicted value of \\(y\\) given the regression line.\nHere is the formula to compute the \\(R^2\\):\n\\(R¬≤ = \\frac{\\hat{\\sigma_{\\mu_{i}}}^2}{\\hat{\\sigma_{tot}}^2} = QS_{residuen} / QS_{total}\\)\nAs always, we try to break it down to better understadn the componennts and their effect in the resulted value of \\(R^2\\).\n\\(QS_{residuen} = Œ£(\\hat{y} - \\bar{y})¬≤\\) SSR quantifies the amount of unexplained or residual variation in the dependent variable OR how far are the points from the regression line.\n\\(QS_{total} = Œ£(y·µ¢ - \\bar{y})¬≤\\) is a measure of the total variability in the dependent variable OR how far away the observed y values are from the .\nExample (which we worked through last week üòÉ):\nThe data set:\n\n  Height Weight\n1    170     60\n2    180     75\n3    167     59\n4    165     88\n5    191     91\n\nPLOTTING TIME !!\n\n\n\nMODELLING TIME!\nWe start with running the model with the (good, old known) unstandardized variables\n\n\nlm <-  lm(dat$Weight ~ dat$Height)\nsum_lm <-  summary(lm)\nsum_lm\n\n\nCall:\nlm(formula = dat$Weight ~ dat$Height)\n\nResiduals:\n      1       2       3       4       5 \n-11.382  -3.377 -10.284  20.115   4.928 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -47.5307   121.1507  -0.392    0.721\ndat$Height    0.6995     0.6928   1.010    0.387\n\nResidual standard error: 15.01 on 3 degrees of freedom\nMultiple R-squared:  0.2536,    Adjusted R-squared:  0.00482 \nF-statistic: 1.019 on 1 and 3 DF,  p-value: 0.387\n\nWe continue with computing the confidence interval for the \\(\\alpha\\) and \\(\\beta\\)\n\n\nconfint(lm)\n\n                  2.5 %     97.5 %\n(Intercept) -433.086166 338.024785\ndat$Height    -1.505342   2.904319\n\nGreat, here are the \\(y\\) values:\n\n\ny_values <- dat$Weight \ny_values\n\n[1] 60 75 59 88 91\n\nAnd the \\(\\bar{y}\\) value:\n\n\nmean_y <-  mean(dat$Weight)\nmean_y\n\n[1] 74.6\n\nAnd the \\(\\hat{y}\\) value:\n\n\npredicted_y <-  predict.lm(lm(dat$Weight ~ dat$Height))\npredicted_y\n\n       1        2        3        4        5 \n71.38235 78.37724 69.28389 67.88491 86.07161 \n\nWe are now ready to calculate the \\(\\color{red}{R^2}\\):\n\n\nqs_res <-  (predicted_y - mean_y)**2\nqs_res <-  sum(qs_res)\nqs_tot <- (y_values - mean_y)**2\nqs_tot <-  sum(qs_tot)\nr_squared <- qs_res / qs_tot\n\nr_squared\n\n[1] 0.2536148\n\nInterpretation of the \\(R^2\\) effect size is:\nIn a simple linear regression, the \\(R^2\\) provides insight into the proportion of variance in the dependent variable (AV) that can be explained by the independent variable (UV), indicating the model‚Äôs goodness of fit and the strength of the relationship between the variables.\nAnother way to calculaet effect size for the estimated slope in Simple Linear Regression Models.\nThe method relies on the standardized data.\nLets see what changes in the relationship between the AV and the UV if we scale both (spoiler: absolutely nothing‚Ä¶.):\n\n\ng_standardized <-  \n  ggplot(data = dat, aes(x = scale(Height), y = scale(Weight)))+\n  geom_point(color = \"darkgreen\", size = 4)+\n  geom_smooth(method = \"lm\")+\n  theme_classic()\n\n\ncombined_plot <- grid.arrange(g_unstandardized, g_standardized, nrow = 1)\n\n\n\nThe Advantages to This Method:\nA standardized beta allows for a direct comparison of the relative importance of different predictor variables within a regression model. Since both the predictor and criterion variables are standardized, the magnitude of the standardized beta represents the change in the criterion variable in terms of standard deviations when the predictor variable changes by one standard deviation.\nUnit Independence: The standardized beta is not influenced by the specific units of measurement used for the predictor and criterion variables. This makes it easier to compare the effects of different variables, even if they are measured on different scales or have different units.\nGeneralizability: The effect size (\\(\\beta_{z}\\)) represents the magnitude of the relationship between the predictor and criterion variables in standardized units. This allows for better generalizability across different samples, populations, or studies, as it is not dependent on the specific measurement units used.\nComparability: Standardized betas and effect sizes can be compared across different studies or analyses, providing a standardized measure of the strength of the relationships. This comparability facilitates meta-analyses or synthesis of results from multiple studies.\nHere‚Äôs a Step-by-Step Guide to Salculating \\(\\beta_{z}\\):\n1) Compute the mean (\\(\\hat{x}\\)) of the SCALED independent variable (UV).\n\n\nscale_mean_x <-  mean(scale(dat$Height))\n\nscale_mean_x\n\n[1] 4.996004e-16\n\n2) Compute the mean (\\(\\hat{y}\\)) of the SCALED dependent variable (AV).\n\n\nscaled_mean_y <-  mean(scale(dat$Weight))\n\nscaled_mean_y\n\n[1] 4.066192e-16\n\n3) Calculate the covariance between scaled (standardized) AV and the scaled (standardized) UV using the formula:\n\\[cov(x,y) = \\frac{\\sum{(x_{i}-\\hat{x})\\times (y_{i}-\\hat{y})}}{n-1}\\]\n\n\ncov <-  sum((scale.default(dat$Height) - scale_mean_x) * (scale(dat$Weight) - scaled_mean_y))\ncov <-  cov/3\n\ncov\n\n[1] 0.6714691\n\n4) Calculate the variance of UV using the formula:\n\\[var(x) = \\frac{\\sum{(x_{i}-\\hat{x})^2}}{n-1}\\]\n\n\nvar_x <-  sum((scale(dat$Height) - scale_mean_x)^2)\n\nvar_x <-  var_x/3\n\nvar_x\n\n[1] 1.333333\n\n5) Calculate the standardized beta (\\(\\beta_{z}\\)):\n\n\nbeta_z <-  cov/var_x\nbeta_z\n\n[1] 0.5036018\n\n6) Compare your results with R-output:\n\n\nlm_z <-  lm(scale(dat$Weight) ~ scale(dat$Height))\n\nsummary(lm_z)\n\n\nCall:\nlm(formula = scale(dat$Weight) ~ scale(dat$Height))\n\nResiduals:\n      1       2       3       4       5 \n-0.7566 -0.2245 -0.6836  1.3371  0.3276 \nattr(,\"scaled:center\")\n[1] 74.6\nattr(,\"scaled:scale\")\n[1] 15.04\n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)\n(Intercept)       3.536e-17  4.461e-01    0.00    1.000\nscale(dat$Height) 5.036e-01  4.988e-01    1.01    0.387\n\nResidual standard error: 0.9976 on 3 degrees of freedom\nMultiple R-squared:  0.2536,    Adjusted R-squared:  0.00482 \nF-statistic: 1.019 on 1 and 3 DF,  p-value: 0.387\n\n7) Note that the \\(R^2\\) did not change either!\nWe continue with computing the confidence interval for the \\(\\alpha_{z}\\) and \\(\\beta_{z}\\)\n\n\nconfint(lm_z)\n\n                      2.5 %   97.5 %\n(Intercept)       -1.419799 1.419799\nscale(dat$Height) -1.083782 2.090986\n\n\n\n\nNow\n.\n.\n.\n\n\n\nWe know that since we have ‚Äúcollected‚Äù only 5 observations, this experiment might suffer from low power ü•µü¶æ‚Ä¶ We even have a good reason to believe so because the relationship between height and weight may hold in reality‚Ä¶\nWe can determine the sample size we will need to achieve significant results\nBUT Careful! this method is no magic! ü™Ñ we will only be likely to obtain significant results only if there is, indeed, a relationship between height and weight)\nWe have to first calculate the\n\n\nrho_squared = coef(lm_z)[\"scale(dat$Height)\"]^2\n\nf_squared = rho_squared/(1-rho_squared)\n\nf_squared\n\nscale(dat$Height) \n        0.3397908 \n\nWe use the power calculation function pwr.f2.test(). It takes the following arguments: \\(u\\) which is the number of predictors we have in our model. In a simple linear regression we always ahve only 1, \\(f2\\) which is the effect size, f squared, which we calculated above. The other arguments are \\(sig.level\\) that determines the significance level (Type I error probability) and \\(power\\) represents the power we wish to achieve (1 minus Type II error probability)\n\n\nlibrary(pwr)\npwr.f2.test(u = 1, f2 = f_squared, sig.level = 0.005, power = 0.8)\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 41.23813\n             f2 = 0.3397908\n      sig.level = 0.005\n          power = 0.8\n\nInterpretation: Because ùúà = ùëõ ‚àí 2 (we estimate 2 variables), we must add those 2 to know what the required sample (i.e., ùë£ + 2). In our case, we will need 43 subjects (41. + ).\n\n\n\nThings\nto\nremember\nabout \\({R^2}\\)\n\n\n\nRanges between 0 and 1.\n1.1) A value closer to 1 indicates a stronger relationship between the independent (UV) and dependent (AV) variables, meaning that more of the variance in the dependent variable can be explained by the independent variable(s).\n1.2) Conversely, a value closer to 0 indicates a weaker relationship.\nThe estimated value \\(r^2\\) (which is the realised value of \\(R^2\\)) within the scope of the SLR (simple linear regression) is equivalent to the squared Pearson correlation.\n\\(r^2\\) is also referred to as the coefficient of determination.\nInterpretation of the standardized \\(\\beta_{z}\\): If the predictor variable (AV) increases by one standard deviation, the criterion variable (UV), on average, increases by \\(\\beta_{z}\\) standard deviations.\nBack to the summptions I mentioned at the beginning of this blog:\nLinearity. We will test this assumption with a simple scatter plot. To make the point here, I will create a new, richer data set.\n\n\n\nIn order to examine those assumptions, we will runa. linear regression (we will need it to exmain the distribution of the residuals).\n\n\nlibrary(gridExtra)\n\nlm_2 <-  lm(Weight ~ Height, data = dat_rich)\n \nsummary(lm_2)\n\n\nCall:\nlm(formula = Weight ~ Height, data = dat_rich)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.31926 -0.60302  0.02559  0.49829  2.40082 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  6.49342    7.58719   0.856    0.394    \nHeight       0.58495    0.07573   7.724 9.79e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8047 on 98 degrees of freedom\nMultiple R-squared:  0.3784,    Adjusted R-squared:  0.3721 \nF-statistic: 59.66 on 1 and 98 DF,  p-value: 9.793e-12\n\nLets plot all the figures we need to judge of the Linear regression assumptions are met.\n\n\nlibrary(ggplot2)\nlibrary(gridExtra)\nstand_res <-  as.data.frame(rstandard(lm_2))\n\n\np_1 <-  ggplot(data = dat_rich, aes(x = Height, y = Weight)) +\n  geom_point(col = \"lightblue\", size = 3) +\n  geom_smooth(method = \"lm\", ) +\n  ggtitle(\"Linearity Assumption\")+\n  theme_classic()\n\n\n\npredicted <-  as.data.frame(predict(lm_2, interval = \"prediction\", level = .95))\npredicted$stand_res <-  stand_res$`rstandard(lm_2)`\n\np_2 <-   ggplot(data = predicted, aes(fit, stand_res))+\n  geom_point(col = \"lightblue\")+\n  ggtitle(\"Homoskedasticity Assumption\") +\n  geom_hline(yintercept = 0) + \n  xlab(\"Predicted Value of DV\") +\n  ylab(\"Standardized Residuals\") +\n  theme_classic()\n\ncook_dat <-  as.data.frame(round(cooks.distance(lm_2), digits = 3))\ncook_dat$id <-  cbind(1:sample_size)\n\np_3 <-   ggplot(data = cook_dat, aes(x = id, y = `round(cooks.distance(lm_2), digits = 3)`))+\n  geom_point(col = \"lightblue\")+\n  ggtitle(\"Outliers Assumption\") +\n  geom_hline(yintercept = 0.04) + \n  xlab(\"Observation IDs\") +\n  ylab(\"Cooks Distances\") +\n  theme_classic()\n\n# Arrange the plots in a grid\ncombined_plots <- grid.arrange(p_1, p_2, p_3, layout_matrix = rbind(c(1, NA), c(2, 3)), \n                               heights = c(3,3), widths = c(5, 5))\n\n\n\nA quick reminder of the way Cooks Distance is computed:\nCook‚Äôs D(i) = \\(\\frac{{\\Delta \\hat{y_i}^2}}{{p}} \\times \\frac{{h_{ii}}}{{(1 - h_{ii})}}\\)\nAnd lastly, (for the statistics nurds among us‚Ä¶.üòâ), here is the model with the standardized coefficients:\n\n\nscaled_dat_rich <-  as.data.frame(round(scale(dat_rich), digits = 2))\nlm_z_2 <-  lm(scaled_dat_rich$Weight ~scaled_dat_rich$Height)\n\nsummary(lm_z_2)\n\n\nCall:\nlm(formula = scaled_dat_rich$Weight ~ scaled_dat_rich$Height)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.28128 -0.59211  0.02353  0.48878  2.36279 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            0.0001846  0.0792326   0.002    0.998    \nscaled_dat_rich$Height 0.6152039  0.0796207   7.727 9.67e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7923 on 98 degrees of freedom\nMultiple R-squared:  0.3786,    Adjusted R-squared:  0.3722 \nF-statistic:  59.7 on 1 and 98 DF,  p-value: 9.669e-12\n\nconfint.lm(lm_z_2)\n\n                            2.5 %    97.5 %\n(Intercept)            -0.1570500 0.1574191\nscaled_dat_rich$Height  0.4571992 0.7732086\n\n\n\n\n",
      "last_modified": "2023-06-29T12:56:42+02:00"
    },
    {
      "path": "index.html",
      "title": "Willkommen!",
      "author": [],
      "contents": "\n\n          \n          \n          LMU Statistik\n          \n          \n          Home\n          \n          \n          Statistik I\n           \n          ‚ñæ\n          \n          \n          Basics\n          Statistical Tests - Distributions\n          \n          \n          \n          \n          Statistik II\n           \n          ‚ñæ\n          \n          \n          ANOVA - I\n          ANOVA - II\n          Simple Linear Models\n          Multiple Linear Models\n          Multiple Linear Models - Part II\n          Simple Linear Models: Effectsize\n          \n          \n          ‚ò∞\n          \n          \n      \n        \n          Willkommen!\n          \n            \n                \n                  \n                    Moodle\n                  \n                \n              \n                            \n                \n                  \n                    Tehilla‚Äôs Email\n                  \n                \n              \n                          \n        \n\n        \n          \n      \n      \n        \n          This website is dedicated ‚ù§Ô∏è to all the AMAZING\n          Schulpsychologie students, who will sit/sat a\n          Statistik exam @LMU!\n          Enjoy your journey to understanding the topics we cover in\n          the courses Statistik I and Statistik II!\n          Im very much looking forward to hear both about what you\n          think about this website and about our seminar!\n          You can üìß me at any time (see button above)\n        \n      \n    \n\n    \n      \n        \n          \n            \n              \n            \n              Willkommen!\n            \n            \n              \n                \n                                    \n                    \n                      Moodle\n                    \n                  \n                                    \n                    \n                      Tehilla‚Äôs Email\n                    \n                  \n                                  \n              \n            \n            \n              This website is dedicated ‚ù§Ô∏è to all the AMAZING\n              Schulpsychologie students, who will sit/sat a\n              Statistik exam @LMU!\n              Enjoy your journey to understanding the topics we cover\n              in the courses Statistik I and Statistik II!\n              Im very much looking forward to hear both about what\n              you think about this website and about our seminar!\n              You can üìß me at any time (see button above)\n            \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2023-06-29T12:56:44+02:00"
    },
    {
      "path": "lm_ii_ii.html",
      "title": "Multiple Linear Regression Analysis - Part II",
      "description": "Linear Regression\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nIntroduction:\nWhile the traditional understanding of linear regression assumes continuous independent variables, there are scenarios where the independent variables (UV) may be discrete in nature.\nIn this blog post we will explore the concept of multiple linear regression with discrete variables and understand how it can be applied to real-world problems.\nWhat are discrete variables?\nA discrete variable is a type of variable that takes on specific, distinct values with no intermediate values possible between them. These values are typically counted or categorized rather than measured on a continuous scale\nCan you give me some examples?\nGender (e.g., either Male üßçüèº‚Äç‚ôÇÔ∏è vs Femaleüßçüèæ‚Äç‚ôÄÔ∏è)\nEthnicity (e.g., Caucasian, African American, and Asian)\nOccupation (e.g., ‚ÄúEngineer‚Äù üë©üèª‚Äçüíª vs.¬†‚ÄúTeacher‚Äù üë®üèª‚Äçüè´ vs.¬†‚ÄúSalesperson‚Äù üßëüèΩ‚Äçüíº)\nIn case we are looking at the effect of such variables, you will need to modify the multiple linear regression model to account for them.\nCool, but how do I code the variables expressions?\nWell, depending on how many levels/groups there are, this may change.\nA case of a variable with two categorical expressions:\nIn the case of a discrete predictor with two categorical expressions (e.g., male vs.¬†female), we first establish an arbitrary expression (e.g., male) as the reference category. This reference group will be coded with a 0. For example, if we decided that ‚Äúmale‚Äù is our reference group, we will insert 0 if the participant is a male and a 1 if they are identified as female.\nSo, we define a dummy variable \\(D_{i}\\) as follows:\n\\(D_{i} = 1\\), if the person is a female\nand\n\\(D_{i} = 0\\), if the person is a male\nA practical example:\nAnd here is an example of a data set, which includes information about a group of individuals, with each individual identified by a unique ID.\nID column: This variable represents the unique identifier assigned to each individual in the data set.\nGender Dummy column: This dummy coded variable indicates whether each person has the color red. It takes a value of 1 if the person has the color red and 0 if not.\nIQ column: This variable represents the IQ scores of individuals. It provides a measure of intellectual ability or cognitive capacity, with higher values indicating higher IQ scores.\n**Monthly Salary _Score column**: This variable represents the scores on Monthly Salary for each individual. It reflects the subjective assessment of Monthly Salary , with higher values indicating higher levels of self-reported Monthly Salary .\n\n   ID Gender_dummy  IQ Salary_month\n1   1            1 120         8000\n2   2            0 110         7000\n3   3            1 125         9000\n4   4            0 105         6000\n5   5            0  95         5000\n6   6            1 115         4000\n7   7            0 100         2500\n8   8            0 130         9000\n9   9            1 125         3000\n10 10            0 105         2000\n\nOK, but how does the linear model look like with a discrete variable with two expressions?\nThe general equation is:\n\\[Y_{i} = \\alpha + \\beta_{1} \\times D_{1i} +... + \\beta_{2} \\times D_{2i} +... +\\beta_{k-1} \\times D_{k-1}  + \\epsilon_{i} \\]\nFor our specific example, therefore, it will be:\n1) For the reference group (we decided on ‚Äúmale‚Äù), the model will be:\n\\[Y_{i} = \\alpha + \\beta \\times 0 + \\epsilon_{i}\\]\n\\[ = \\alpha + \\epsilon_{i}\\]\nIt follows that \\(\\alpha\\) is the predicted value for this group.\n2) For the second group (will be ‚Äúfemale‚Äù), the model will be:\n\\[Y_{i} = \\alpha + \\beta \\times 1 + \\epsilon_{i}\\]\n\\[ = \\alpha + \\beta +\\epsilon_{i}\\]\nIt follows that \\(\\alpha + \\beta\\) is the predicted value for this group.\nIt also implies that:\nIf \\(ùõΩ < 0\\), then the predicted value for individuals in the reference category is greater than in the other category.\nIf \\(ùõΩ> 0\\) , then the predicted value for individuals in the reference category is smaller than in the other category.\nIf \\(ùõΩ = 0\\), then the predicted values for both categories are equal.\nLets run a model to learn about it‚Äôs output:\n\n\nmodel_2 <- lm(Salary_month ~ Gender_dummy, data = data_2)\n  \nsummary(lm(Salary_month ~ Gender_dummy, data = data_2))\n\n\nCall:\nlm(formula = Salary_month ~ Gender_dummy, data = data_2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -3250  -2562    250   1938   3750 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)   \n(Intercept)      5250       1135   4.624   0.0017 **\nGender_dummy      750       1795   0.418   0.6871   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2781 on 8 degrees of freedom\nMultiple R-squared:  0.02135,   Adjusted R-squared:  -0.101 \nF-statistic: 0.1745 on 1 and 8 DF,  p-value: 0.6871\n\nconfint(model_2)\n\n                 2.5 %   97.5 %\n(Intercept)   2631.835 7868.165\nGender_dummy -3389.683 4889.683\n\nWe can see the results and learn that:\n(intercept) = 5250.\nThis is the predicted Monthly Salary value for males participants.\nGender_dummy = 750.\nThis is the predicted Monthly Salary value for females participants is 5250 + (1 * 750) = 6000.\nThe p-value of the slope (Gender_dummy) is non-significant.\nThis implies that there is no difference in Monthly Salary between men and women.\nThe confidence interval for the intercept is [2631.835, 7868.165].\nwhich implies that, for males, the possible Monthly Salary scores lie between 2631.835 and 7868.165.\nThe confidence interval for the slope is [-3389.683, 4889.683].\nwhich implies that the plausible Monthly Salary scores for females lie between -3389.683 and 4889.683.\nA case of a variable with more than two categorical expressions:\nLet‚Äôs consider an example of a discrete variable with three categories: ‚ÄúEducation Level‚Äù.\nWe‚Äôll define and code the categories as - ‚ÄúHigh School‚Äù\n- ‚ÄúBachelor‚Äôs Degree‚Äù\n- ‚ÄúMaster‚Äôs Degree.‚Äù\nWhen we create dummy variables to represent these categories, we assign a value of 1 if an individual belongs to that category and a value of 0 if they do not.\nIn the resulting data frame below, if a row has a 0 in both columns ‚ÄúDummy_HS‚Äù and ‚ÄúDummy_Bachelor‚Äù columns, it means that the person does not have a High School education nor a Bachelor‚Äôs Degree. Essentially, they are not in either of those categories. Instead, they may have a different education level, such as a Master‚Äôs Degree or some other qualification not represented in the dummy variables.\nTherefore, these rows with two zeros (as on rows 3, 6 and 8) indicate individuals who do not fall into the specified categories and should be understood as having a different education level or belonging to an unrepresented category (which is in this case the ‚Äúhigh schools degree‚Äù).\n\n   ID   Education_Level  IQ Salary_month Dummy_HS Dummy_Bachelor\n1   1 Bachelor's Degree 120         8000        1              0\n2   2       High School 110         7000        0              0\n3   3   Master's Degree 125         9000        0              1\n4   4       High School 105         6000        0              0\n5   5 Bachelor's Degree  95         5000        1              0\n6   6   Master's Degree 115         4000        0              1\n7   7       High School 100         2500        0              0\n8   8   Master's Degree 130         9000        0              1\n9   9 Bachelor's Degree 125         3000        1              0\n10 10       High School 105         2000        0              0\n\nLets discuss a dummy variable with more than two levels/expressions:\nFor the reference group (we decided on ‚Äúhigh school‚Äù), the model will be:\n\\[Y_{i} = \\alpha + \\beta_{bachelor} \\times 0 + \\beta_{masters} \\times 0 + \\epsilon_{i} \\]\n\\[ = \\alpha + \\epsilon_{i}\\]\nIt follows that \\(\\alpha\\) is the predicted value for this group.\nFor the second group (will be ‚ÄúBachelor‚Äôs Degree‚Äù), the model will be:\n\\[Y_{i} = \\alpha + \\beta_{bachelor} \\times 1 + \\beta_{masters} \\times 0 + \\epsilon_{i}\\]\n\\[ = \\alpha + \\beta_{bachelor} + \\epsilon_{i}\\]\nIt follows that \\(\\alpha + \\beta_{bachelor}\\) is the predicted value for this group.\nFor the third group (will be ‚ÄúMaster‚Äôs Degree‚Äù), the model will be:\n\\[Y_{i} = \\alpha + \\beta_{bachelor} \\times 0 + \\beta_{masters} \\times 1 + \\epsilon_{i}\\]\n\\[ = \\alpha + \\beta_{masters} + \\epsilon_{i}\\]\nIt follows that \\(\\alpha + \\beta_{masters}\\) is the predicted value for this group.\nLets run a model to learn about it‚Äôs output:\n\n[1] \"High School\"       \"Bachelor's Degree\" \"Master's Degree\"  \n\nCall:\nlm(formula = Salary_month ~ Education_Level, data = data_3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3333.3 -2218.8   645.8  1666.7  2666.7 \n\nCoefficients:\n                                 Estimate Std. Error t value Pr(>|t|)\n(Intercept)                        4375.0     1309.6   3.341   0.0124\nEducation_LevelBachelor's Degree    958.3     2000.4   0.479   0.6465\nEducation_LevelMaster's Degree     2958.3     2000.4   1.479   0.1827\n                                  \n(Intercept)                      *\nEducation_LevelBachelor's Degree  \nEducation_LevelMaster's Degree    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2619 on 7 degrees of freedom\nMultiple R-squared:  0.2405,    Adjusted R-squared:  0.02347 \nF-statistic: 1.108 on 2 and 7 DF,  p-value: 0.3819\n                                     2.5 %   97.5 %\n(Intercept)                       1278.308 7471.692\nEducation_LevelBachelor's Degree -3771.941 5688.608\nEducation_LevelMaster's Degree   -1771.941 7688.608\n\n(intercept) = 4,375.\nThis is the predicted Monthly Salary value for participants with a high school degree.\nEducation_LevelBachelor‚Äôs Degree = 958.3.\nThis is the predicted Monthly Salary value for participants with a BA degree\n\\[= 4,375 + (1 * 958.3) = 5,333.33\\]\nEducation_LevelMaster‚Äôs Degree = 2.417.\nThis is the predicted Monthly Salary value for participants with a MA degree\n\\[= 4,375.0 + (1 * 2958.3) = 7,333.3\\]\nWhat about a model that combines both discrete and continous variables?\nGreat question!\nThe following statistical model or analysis has two types of predictors:\none predictor is discrete, meaning it has two possible manifestations or categories.\na second predictor is continuous, meaning it takes on a range of numerical values.\nWe can use the dataset we have create before and add the IQ scores to predicts participant‚Äôs hapiness.\nThe general model in this case will be:\n\\[Y_{i} = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{education} \\times D_{i} + \\epsilon_{i} \\]\nThe specifications of all the other models are:\n1) For the reference group, which has a high school education, the regression model will be:\n\\[Y_{i} = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{education} \\times 0 + \\epsilon_{i} \\]\nWhich implies that the predicted value of this group‚Äôs Monthly Salary is\n\\[ = \\alpha + \\beta_{IQ} \\times X_{i}\\]\n2) For the second group, which has a becholar‚Äôs degree , the regression model will be:\n\\[Y_{i} = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{becholar} \\times 1 + \\epsilon_{i} \\]\nWhich implies that the predicted value of this group‚Äôs Monthly Salary is\n\\[ = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{becholar} \\times 1\\]\n3) For the third group, which has a masters‚Äôs degree , the regression model will be:\n\\[Y_{i} = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{masters} \\times 1 + \\epsilon_{i} \\]\nWhich implies that the predicted value of this group‚Äôs Monthly Salary is\n\\[ = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{masters} \\times 1\\]\nThe last question for today is: ü•Å\nWhat about a good-old interaction effect?\nLets revert back to our first above, including an interaction effect in a a linear regression model helps us to explore how the relationship between ‚Äúgender‚Äù (reminder: 2 levels - male vs.¬†female) and ‚ÄúMonthly Salary‚Äù changes, depending on the gender.\nMore specifically, in our data set we have gender and IQ as potential predictors. By including an interaction term, we can assess whether the relationship between gender and Monthly Salary differs for individuals with different IQ levels. It could show us, for example, that gender has a stronger positive effect on Monthly Salary for individuals with avergae IQ scores compared to those with lower/higher IQ scores.\nOverall, the interaction term helps us understand if the relationship between gender and Monthly Salary is dependent on an individual‚Äôs IQ level and if the two factors interact in influencing Monthly Salary .\nLet us define the general model equation:\n\\[Y_{i} = \\alpha + \\beta_{gender} \\times D_{i} + \\beta_{IQ} \\times X_{i} + \\beta_{interaction} \\times (X_{i} \\times D_{i})+ \\epsilon_{i} \\]\n1) For the reference group, where \\(D_{i} = 0\\), the model will be:\n\\[Y_{i} = \\alpha + \\beta_{gender} \\times D_{i} + \\beta_{IQ} \\times X_{i} + \\beta_{interaction} \\times (X_{i} \\times D_{i})+ \\epsilon_{i} \\]\n\\[ = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{gender} \\times 0 + \\beta_{interaction}(X_{i} \\times 0) \\]\n\\[ = \\alpha + \\beta_{IQ} \\times X_{i} + \\epsilon_{i}\\]\nWhich implies that the predicted Monthly Salary value for the reference group is:\n\\[= \\alpha + \\beta_{IQ} \\times X_{i}\\]\n2) For the second group, where \\(D_{i} = 1\\), the model will be:\n\\[Y_{i} = \\alpha + \\beta_{gender} \\times D_{i} + \\beta_{IQ} \\times X_{i} + \\beta_{interaction} \\times (X_{i} \\times D_{i})+ \\epsilon_{i} \\]\n\\[ = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{gender} \\times 1 + \\beta_{interaction}(X_{i} \\times 1) \\]\n\\[ = (\\alpha + \\beta_{gender}) + (\\beta_{IQ} + \\beta_{interaction}) \\times X_{i} + \\epsilon_{i}\\]\nWhich implies that the predicted Monthly Salary value for the second group is:\n\\[ = (\\alpha + \\beta_{gender}) + (\\beta_{IQ} + \\beta_{interaction}) \\times X_{i}\\]\nLets compute the model:\n\n\nCall:\nlm(formula = Salary_month ~ Gender_dummy + IQ + (Gender_dummy * \n    IQ), data = data_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3545.5 -1429.5   639.8  1658.9  2454.5 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)\n(Intercept)     -12059.32   10440.53  -1.155    0.292\nGender_dummy       422.96   39782.57   0.011    0.992\nIQ                 161.02      96.61   1.667    0.147\nGender_dummy:IQ    -15.56     330.84  -0.047    0.964\n\nResidual standard error: 2624 on 6 degrees of freedom\nMultiple R-squared:  0.3468,    Adjusted R-squared:  0.02017 \nF-statistic: 1.062 on 3 and 6 DF,  p-value: 0.4323\n                       2.5 %     97.5 %\n(Intercept)     -37606.38747 13487.7434\nGender_dummy    -96921.48242 97767.3992\nIQ                 -75.37631   397.4102\nGender_dummy:IQ   -825.09363   793.9688\n\nThe interpretation of the parameters is as follows:\n‚Ä¢ \\(\\alpha\\): This is the y-intercept in the reference category.\nIt represents the predicted salary for a participant without, who has an IQ of 0‚Ä¶ Not really interpretable, is it?\n‚Ä¢ \\(\\beta_{gender}\\): This is the slope parameter in the reference category.\nIt indicates the expected increase in Monthly Salary when a male participant has an increase of 1 point on his IQ test.\n‚Ä¢ \\(\\beta_{IQ}\\): This is the difference in intercepts between the two categories.\nIt represents the expected difference in Monthly Salary between male and female when their IQ = 0.\n‚Ä¢ \\(\\beta_{interaction}\\): This is the difference in slope parameters between the reference and other category.\nIt reflects the difference in the ‚Äúeffectiveness‚Äù of IQ for male vs.¬†female participants on Monthly Salary .\n‚Ä¢ \\(\\alpha + \\beta_{IQ}\\): This is the intercept in the female category.\nIt represents the predicted value for female participants, who have IQ = 0.\n‚Ä¢ \\(\\beta_{gender} + \\alpha_{IQ}\\): This is the slope parameter in the female category.\nIt indicates the expected increase in Monthly Salary when a female has an increase of 1 point on her IQ test.\n\n\n\n",
      "last_modified": "2023-07-03T21:23:30+02:00"
    },
    {
      "path": "lm_ii.html",
      "title": "Multiple Linear Regression Analysis",
      "description": "Linear Regression\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nLinear models can be extended to include more than one independent variable, and the equation becomes:\n\\(\\color{red}{Y{i}} = \\color{blue}{{\\alpha}} + \\color{green}{{\\beta_{1} \\times X_{1} + \\beta_{2} \\times X_{2} + ‚Ä¶ + \\beta_{n} \\times X_{n}} + \\color{orange}{{\\epsilon_{i}}}}\\)\nWhere \\(\\color{red}{Y_{i}}\\) is the dependent variable (what we aim to predict).\n\\(\\color{blue}\\alpha\\) is the intercept (the point at which the regression line crosses the y axis).\n\\(\\color{green}{{X_{1}, X_{2}, X_{n}}}\\) are the independent variables (what we measure).\n\\(\\color{green}{{\\beta_{1}, \\beta_{2}, ..., \\beta_{n}}}\\) are the slopes of the respective independent variables (tells us how important the respective variable is).\nSince we also are interested in the distribution of the residuals, we also estimate \\(\\color{orange}{{\\sigma^2}}\\), which represents the standard deviation of the error term \\(\\color{orange}\\epsilon\\).\nIn MLR (Multiple Linear Regression) analysis we ask the following questions:\n## 1) How much variance, relative to the total variability of the criterion, can all predictors explain together?\n## 2) Which predictor has the largest predictive contribution?\n## 3) What is the magnitude of the independent predictive contribution of a predictor?\n## 4) Does the strength, direction, and interpretation of the effect of a predictor change when considering another predictor compared to the multiple regression?\nWe are going to use a fun(ny) example for this topic.\nIn a parallel universe, in which Psychology students party during the semester and prior to their exams, a study was conducted‚Ä¶\n\nThe researchers wanted to know if the amount of alcohol üç∫ consumed consumed a day prior to an exam but also the number of hours spent on preparation üìöüìñ affected the student‚Äôs exam results ü•á.\nAnd so they asked students to confess about the amount of ml of alcohol (0 to ‚àûüòÇ) they had the day before the exam and the number hours (0-100) they spent studying and how much they scored in the exam (0-üíØ%).\nModel Equation:\nTherefore, the model equation they were about to test is:\n\\(\\color{red}{Test.Score_{i}} = \\color{blue}{{\\alpha}} + \\color{green}{{\\beta_{alcohol_ml} \\times X_{1} + \\beta_{Study.hours} \\times X_{2}} + \\color{orange}{{\\epsilon_{i}}}}\\)\nHere is the data they obtained:\n\n    Test_Score Drink_ml Hours_Studied\n1         99.9     65.1          66.2\n2        100.0     65.6          66.6\n3         99.9     63.8          64.1\n4        100.0     66.1          65.6\n5        100.0     65.1          64.6\n6        100.0     65.4          63.8\n7        100.0     64.5          64.1\n8         99.3     63.8          64.0\n9         99.2     63.5          65.1\n10       100.0     64.4          65.9\n11       100.0     65.1          65.9\n12       100.0     66.2          66.1\n13       100.0     65.0          64.9\n14        99.4     64.2          64.5\n15        99.8     67.0          66.4\n16        98.7     64.2          64.1\n17       100.0     65.2          65.4\n18       100.0     65.3          66.2\n19        99.8     65.9          63.9\n20       100.0     65.3          65.4\n21       100.0     64.6          64.7\n22        98.8     63.8          64.7\n23       100.0     65.4          66.0\n24       100.0     64.5          63.3\n25       100.0     64.4          64.5\n26       100.0     64.7          64.1\n27       100.0     64.9          65.0\n28       100.0     65.0          65.6\n29       100.0     65.6          66.0\n30       100.0     65.2          66.1\n31       100.0     65.9          65.7\n32        99.6     65.9          64.7\n33       100.0     67.0          65.8\n34        98.9     64.1          64.7\n35        99.4     64.3          64.0\n36        99.3     63.8          63.2\n37        99.7     65.6          64.6\n38       100.0     63.7          64.7\n39       100.0     65.5          65.3\n40        98.9     63.7          63.6\n41        99.7     64.0          64.3\n42       100.0     66.4          64.9\n43       100.0     64.9          64.2\n44       100.0     66.4          65.8\n45        99.3     64.0          63.0\n46       100.0     64.2          65.6\n47       100.0     64.4          65.8\n48        98.9     63.1          63.1\n49        98.0     63.6          63.1\n50       100.0     66.8          64.6\n51       100.0     66.0          65.7\n52        99.0     64.5          64.4\n53       100.0     65.1          65.9\n54        99.7     65.6          64.2\n55        99.6     67.5          65.2\n56       100.0     65.5          64.8\n57       100.0     65.4          65.1\n58       100.0     65.6          67.0\n59        99.0     63.8          64.5\n60       100.0     65.4          64.8\n61        99.3     65.8          64.7\n62        99.1     64.5          64.5\n63       100.0     65.6          66.0\n64        99.5     65.0          64.6\n65        99.5     63.8          63.6\n66       100.0     65.8          64.6\n67        99.7     64.4          66.5\n68       100.0     65.3          65.8\n69        99.3     64.2          64.0\n70        99.9     66.3          65.3\n71       100.0     65.6          66.2\n72        99.7     64.4          66.1\n73        99.0     63.9          63.5\n74        98.8     64.3          65.0\n75       100.0     65.9          65.2\n76        99.7     64.2          64.2\n77       100.0     64.9          66.8\n78        99.8     65.0          64.4\n79        99.0     63.6          64.3\n80       100.0     65.2          64.5\n81        99.6     64.7          66.0\n82        99.4     65.1          66.4\n83        99.3     63.5          64.0\n84       100.0     64.6          64.5\n85       100.0     65.4          66.4\n86       100.0     66.1          65.9\n87        99.3     63.4          64.1\n88        99.9     65.3          66.1\n89       100.0     67.0          66.0\n90        98.3     64.2          64.5\n91       100.0     66.3          66.1\n92        98.8     65.3          64.4\n93       100.0     65.2          65.5\n94        99.8     63.7          65.2\n95       100.0     64.9          65.1\n96       100.0     65.7          66.6\n97       100.0     65.4          66.1\n98       100.0     66.2          65.1\n99       100.0     66.2          67.3\n100       99.1     65.0          65.8\n\nIn this parallel universe the rules are often different to ours but plotting your data is just like in our universe, simply a must!\nAnd so they did!\n\n\n\n\nThey did not forget to test the MLR assumptions before making public statements about their results‚Ä¶\n1) They started off with the LINEARITY assumption. To examine this assumption they plotted the residuals of each UV (alcohol in ml and the number of hours spent on studying)\n1.1) First they ran the linear model to be able to tell something about the distribution of the residuals. \n\n\nmod <-  lm(Test_Score ~ Drink_ml + Hours_Studied, data = dat)\nsummary(mod)\n\n\nCall:\nlm(formula = Test_Score ~ Drink_ml + Hours_Studied, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.16667 -0.13925  0.04314  0.22749  0.64136 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   78.59565    2.81839  27.887  < 2e-16 ***\nDrink_ml       0.18755    0.04572   4.102 8.52e-05 ***\nHours_Studied  0.13691    0.04417   3.099  0.00254 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3593 on 97 degrees of freedom\nMultiple R-squared:  0.3667,    Adjusted R-squared:  0.3537 \nF-statistic: 28.09 on 2 and 97 DF,  p-value: 2.38e-10\n\n\n\nlibrary(car)\ncrPlots(mod, ylab = \"residuals\")\n\n\n\nThe blue line represents a linear trend.\nThe pink line represents a flexible function that describes the data as closely as possible.\nWhen both lines are close to each other, the linearity assumption can be considered fulfilled (in our example, none of the variables win this competition).\nAgain,\nThe blue dashed line shows the expected residuals if the relationship between the predictor and response variable was linear.\nThe pink line shows the actual residuals.\nIf the two lines are significantly different, then this is evidence of a nonlinear relationship.\n2) The next assumption they tested was the NORMALITY. They plotted a histogram of the residuals. Before plotting these, they remembered (of course) to standardize them.\n\n\nlibrary(ggplot2)\nstand_residuals <-  data.frame(\n                    \"stand_resid\" = c(rstandard(mod)))\n\n# Create a histogram with density line using ggplot2\nggplot(stand_residuals, aes(x = stand_resid)) +\n  geom_histogram(fill = \"lightblue\", color = \"black\", bins = 15) +\n  ylim(0, 10) +\n  labs(x = \"residuals (standardized)\", y = \"Frequency\", \n       title = \"Histogram to Examine Homoskedasticity\") +\n  theme_classic()\n\n\n\nThis plot did not make them super happy either‚Ä¶.\n3) lastly, they examined the presence of outliers:\n\n\ncooks_dist <-cooks.distance(model = mod)\n\nplot(cooks_dist, col=\"lightblue\", pch=19, cex=2)\n#add labels to every point\ntext(x = cooks_dist[1:100], labels=c(1:100))\nabline(h=4/sample_size)\n\n\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2023-07-05T11:23:15+02:00"
    },
    {
      "path": "lm.html",
      "title": "Simple Linear Regression Analysis",
      "description": "Linear Regression\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nWelcome to the post about linear models in statistics!\nLinear models are an essential tool in statistics used to model the relationship between a dependent variable and one or more independent variables.\nIn this post, we‚Äôll explore linear models in more detail and introduce an interactive Shiny app plot that will help you visualize the concepts.\nBefore we dive into the interactive part of this post (in which you get to explorer different regression lines using different variables), let‚Äôs first define what a linear model is.\nA linear model is a mathematical equation that represents a linear relationship between two or more variables.\nThe simplest form of a linear model is a straight line equation of the form:\n\\(Y_{i} = \\alpha_{0} + \\beta_{1} \\times X_{1}\\)\nWhere \\(Y_{i}\\) is the dependent variable and represent the expected value of all \\(y_{i}\\) (all single data points) given a specific value of \\(X_{i}\\)\nWe are going to use a very simple example. We will try to fit a model that aims to predict the relationship between individuals height and weight.\nhere is a fun illustration of the simple model:\nLet us look at a simple example with little number of data points (just so we can get the feeling of whats going on under the hood)\n\n  Height Weight\n1    170     60\n2    180     75\n3    167     59\n4    165     88\n5    191     91\n\nHere are the averages of both the weights and heights. We will need those to calculate the intercpet and the slope of the model.\n\n[1]  74.6 174.6\n\nRemember how I told you that I am a fan of plotting the data?\nWe will use a scatterplot this time.\n\n\nlibrary(ggplot2)\nggplot(data = dat, aes(y = Weight,x = Height)) +\n  geom_point(alpha = .7, color = \"red\") +\n  geom_hline(yintercept = mean_weight, color=\"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = mean_height, color = \"green\", linetype = \"dashed\") + \n  #geom_smooth(method = \"lm\") +\n  theme_classic() \n\n\n\nHow is the \\(\\beta\\) calculated?\nA quick reminder: Œ≤ represents how much the regression line will rise or fall.\nWe interpret the \\(\\beta\\) as the average increase in the Dependent variable (AV) when we increase the independent variable (UV) by one unit. For Example, if we increase the Height by 1cm, the average predicted increase of weight is ‚Ä¶..\n\\[\\frac{\\sum_{i=1}^{n}(x_{i} -\\hat{x})\\times(y_{i}-\\hat{y})}{\\sum_{i=1}^{n}(x_{1} - \\hat{x})^2}\\]\nWhich can be also written as:\n\\[\\frac{cov(X,Y)}{S_{x}^2}\\]\nLets start!\n\n  Height Weight xi - mean(x) yi - mean(y) sqr(xi - mean(x)\n1    170     60         -4.6        -14.6            21.16\n2    180     75          5.4          0.4            29.16\n3    167     59         -7.6        -15.6            57.76\n4    165     88         -9.6         13.4            92.16\n5    191     91         16.4         16.4           268.96\n  (xi - mean(x)) X (yi - mean(y))\n1                           67.16\n2                            2.16\n3                          118.56\n4                         -128.64\n5                          268.96\n\nNow we have everything to compute the \\(cov(X, Y)\\) and the \\(S_{X}^2\\)\nFor \\(cov(X, Y)\\) we just need to compute the sum of the 6th column\n\n\nsum(dat[,\"(xi - mean(x)) X (yi - mean(y))\"])\n\n[1] 328.2\n\nFor \\(S_{x}^2\\) we just need to compute the sum of the 5th column\n\n\nsum(dat[, \"sqr(xi - mean(x)\"])\n\n[1] 469.2\n\nThe slope, is therefore, 0.6994885\n\n[1] 0.6994885\n\n\\(\\beta = \\frac{328.2}{469.2} = .69\\)\nThis means that, for every 1cm increase in height we expect to see an increase of .69KG.\nHow does the \\(\\alpha\\) calculated?\n\\[\\overline{y} = \\alpha_{0} + .69\\times \\overline{x}\\]\nWe know both means of \\(x\\) and \\(y\\) (from the calculation above)\nIf we rearrange the equation to solve for \\(\\alpha_{0}\\), we get\n\\[-\\alpha_{0} = -\\overline{y} + .69\\times \\overline{x}\\]\nLets rearrange the equation such that it will look nicer‚Ä¶\n\\[\\alpha_{0} = \\overline{y} - .69\\times \\overline{x}\\]\n\n\nalpha_0 = mean_weight - .6994885 * (mean_height)\nalpha_0\n\n[1] -47.53069\n\nOK, enough with the hard, tiring work of calculating everything by hand‚Ä¶. for exactly this reason we have R (üòÉ).\nWe will use the function lm(), which which for Linear Model. We will wrap it with the function summary(), which provides us with the result summary of our model‚Äôs results.\n\n\nsum_lm <-  summary(lm(dat$Weight ~ dat$Height))\nsum_lm\n\n\nCall:\nlm(formula = dat$Weight ~ dat$Height)\n\nResiduals:\n      1       2       3       4       5 \n-11.382  -3.377 -10.284  20.115   4.928 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -47.5307   121.1507  -0.392    0.721\ndat$Height    0.6995     0.6928   1.010    0.387\n\nResidual standard error: 15.01 on 3 degrees of freedom\nMultiple R-squared:  0.2536,    Adjusted R-squared:  0.00482 \nF-statistic: 1.019 on 1 and 3 DF,  p-value: 0.387\n\nAccording to the model output, the intercept and the slope are not significant.\nAccording to the hypotheses of this mode, the intercept is compared against 0. That is, does -47.5307 is significantly different to 0. Looking at the estimated value itself it may seem odd that this value is not significant differernt to 0. To resolve this mysotory lets take a llok at the confidence intervals of the two:\n\n\nconfint.lm(lm(dat$Weight ~ dat$Height))\n\n                  2.5 %     97.5 %\n(Intercept) -433.086166 338.024785\ndat$Height    -1.505342   2.904319\n\nHow is the cofidence interval calculated?\nFor example: 95% C.I. for \\(\\beta_{1}\\): \\[b_{1} ¬± t_{1-Œ±/2, n-2} * se(b_{1})\\]\nFirst we need to find the t value that ‚Äúsits‚Äù at the lower and upper 2.5% of the t-distribution. We will use the function qt() and will provide us with the t value at 2.5% from a t-ditribution with 3 df.\n\n\nt_value <-  qt(p = .975, df =3, lower.tail = TRUE)\n\n\nWe now have all the unknowns to arrive at the solution.\nLets plug the numbers in and compare it to the 95% confidence interval we obtained above.\n\n\nupper_CI <-  0.6995 + (t_value * 0.6928) \nlower_CI <-  0.6995 - (t_value * 0.6928) \n\nupper_CI\n\n[1] 2.904299\n\nlower_CI\n\n[1] -1.505299\n\nLooking good!\nLets add this line to our scatterplot from before.\n\n\nggplot(data = dat, aes(y = Weight, x = Height)) +\n  geom_point(alpha = .7, color = \"red\") +\n  geom_hline(yintercept = mean_weight, color=\"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = mean_height, color = \"green\", linetype = \"dashed\") + \n  geom_smooth(method = \"lm\") +\n  theme_classic() \n\n\n\nLastly, here is how the standard error term is calculated. This term tells us the distance between the data points with respect to their y values and the best fitting line.\n\\[\\sigma^2 = S^2 = \\frac{\\sum_{i=1}^n(Y_{i} - \\hat{Y})}{n-2}\\]\nWe need the predicted value for each person in our dataset. We will use the predict.lm() function that will output the predicted values fro every person based on the model lm(weight ~ Height.\n\n\nlm <-  lm(dat$Weight ~ dat$Height)\ndat[,\"predicted_y\"] <-  predict.lm(lm)\n\ndat[,\"predicted_y\"] \n\n[1] 71.38235 78.37724 69.28389 67.88491 86.07161\n\ndat[, \"sqr(y-perd(y))\"] <- (dat$Weight - dat$predicted_y)**2\n\ndat[, \"sqr(y-perd(y))\"]\n\n[1] 129.55796  11.40574 105.75834 404.61683  24.28902\n\nsum <- sum(dat[, \"sqr(y-perd(y))\"])\n\nsqrt(sum/3)\n\n[1] 15.00697\n\nHere is how the t-distribution under the \\(H_{0}\\) looks like:\n\n\nlibrary(latex2exp)\nset.seed(123)\nx <- rt(1000, df = 3)\nx <-  round(x, digit = 3)\ncuts <-  quantile(x , c(0.000, .05, .95, .99999)) \n\n# Create data\nmy_variable = x\n \n# Calculate histogram, but do not draw it\nmy_hist = hist(x , breaks = 190  , plot = F)\n\n \n# Color vector\nmy_color= ifelse(my_hist$breaks <= 2.273526, \"lightgrey\",\n          ifelse(my_hist$breaks >= -2.273526, \"red\", rgb(0.2,0.2,0.2,0.2)))\n \n# Final plot\nplot(my_hist, \n     col = my_color, \n     border = F,\n     freq = FALSE, \n     main = (TeX('Histogram for a $\\\\t$-distribution with 3 degrees of freedom (df)')),\n     xlab = \"possible t values\", \n     xlim = c(-6,6), \n     ylim = c(0,.7), \n    cex.main=0.9)\ncurve(dt(x, df = 3), from = -5, to = 5, n = 500, col = 'red', lwd = 1, add = T)\nabline(v = -0.392, col = \"darkgreen\", lwd = 2,  lty = 'dashed')\nabline(v = 1.010, col = \"blue\", lwd = 2,  lty = 'dashed')\nabline(v = 2.273526, col = \"red\", lwd = 3)\ntext(x = 2.2, y = .7, TeX('t value for $\\\\alpha$'), col = \"blue\")\ntext(x = -1.6, y = .35,  TeX('t value for $\\\\beta$'),  col = \"darkgreen\")\ntext(x = 3.2, y = .15, substitute(paste(bold(\"t critical\"))),  col = \"red\")\n\n\n\nA Note on t-values:\n\\(t-value = \\frac{Estimated Parameter}{SF}\\)\nThe estimate represents the effect or impact that the independent variable has on the dependent variable. The standard error, on the other hand, quantifies the uncertainty or variability associated with that estimate.\nBy dividing the estimate by the standard error, we obtain the t-value. Essentially, the t-value tells us how many standard errors the estimate is away from zero. It helps us assess whether the estimate is statistically significant or just a result of random variation\nAnd a small toy-example for you to play around with üòâ\nLet‚Äôs move onto the interactive Shiny app plot. The plot allows you to visualize the relationship between two variables and fit a linear model to the data. To use the app, follow these steps:\nChoose the variables from the dropdown menu.\nObserve how the line changes to fit the data points better.\nObserve how the changes in:\nIntercept\nSlope(s)\np-values\nR-squared value as you adjust the model.\nThe app plot is an excellent way to see how the slope and intercept of a linear model can impact the fit of the model to the data. You can also see how the R-squared value changes as you adjust the model, which is a measure of how well the model fits the data. The closer the R-squared value is to 1, the better the model fits the data.\nThe data Im using here is from a built-in dataset in R called ‚Äúmtcar‚Äù.\n\n\nlibrary(knitr)\nkable(head(mtcars))\n\n\n\n\nmpg\n\n\ncyl\n\n\ndisp\n\n\nhp\n\n\ndrat\n\n\nwt\n\n\nqsec\n\n\nvs\n\n\nam\n\n\ngear\n\n\ncarb\n\n\nMazda RX4\n\n\n21.0\n\n\n6\n\n\n160\n\n\n110\n\n\n3.90\n\n\n2.620\n\n\n16.46\n\n\n0\n\n\n1\n\n\n4\n\n\n4\n\n\nMazda RX4 Wag\n\n\n21.0\n\n\n6\n\n\n160\n\n\n110\n\n\n3.90\n\n\n2.875\n\n\n17.02\n\n\n0\n\n\n1\n\n\n4\n\n\n4\n\n\nDatsun 710\n\n\n22.8\n\n\n4\n\n\n108\n\n\n93\n\n\n3.85\n\n\n2.320\n\n\n18.61\n\n\n1\n\n\n1\n\n\n4\n\n\n1\n\n\nHornet 4 Drive\n\n\n21.4\n\n\n6\n\n\n258\n\n\n110\n\n\n3.08\n\n\n3.215\n\n\n19.44\n\n\n1\n\n\n0\n\n\n3\n\n\n1\n\n\nHornet Sportabout\n\n\n18.7\n\n\n8\n\n\n360\n\n\n175\n\n\n3.15\n\n\n3.440\n\n\n17.02\n\n\n0\n\n\n0\n\n\n3\n\n\n2\n\n\nValiant\n\n\n18.1\n\n\n6\n\n\n225\n\n\n105\n\n\n2.76\n\n\n3.460\n\n\n20.22\n\n\n1\n\n\n0\n\n\n3\n\n\n1\n\n\nAfter looking at the different variables in this dataset, lets see how each of them can be modeled as a linear regression. The blue line represents the best-fitting line (i.e., the line that minimizes the distance between the data and the model ‚Äì the line with the lowest RSS)\n\n\n\n\nIn conclusion, linear models are a powerful tool in statistics, and the interactive Shiny app plot provides an excellent way to explore the concepts. With the app, you can experiment with different linear models and see how they fit the data. I hope you found this post informative and helpful, and please don‚Äôt hesitate to leave any comments or questions below!\nHomework solution:\nHere is the dataaset:\n\n\ndat_salary <-  data.frame(\"iq\" = c(120, 110,  100, 135, 140),\n                          \"monthly salary\" = c(2500, 2300, 2400, 3000, 2100))\n\ndat_salary\n\n   iq monthly.salary\n1 120           2500\n2 110           2300\n3 100           2400\n4 135           3000\n5 140           2100\n\nHere is a plot of the data, as per usual:\n\n\nggplot(dat_salary,  aes(x = iq, y = monthly.salary)) +\n  geom_point(col = \"blue\", alpha = .4) + \n  geom_smooth(method = \"lm\", col = \"red\") +\n  theme_minimal()\n\n\n\nAnd the model‚Äôs output:\n\n\nsummary(lm(dat_salary$monthly.salary ~ dat_salary$iq))\n\n\nCall:\nlm(formula = dat_salary$monthly.salary ~ dat_salary$iq)\n\nResiduals:\n       1        2        3        4        5 \n  43.304 -123.661    9.375  493.750 -422.768 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept)   2060.268   1394.855   1.477    0.236\ndat_salary$iq    3.304     11.441   0.289    0.792\n\nResidual standard error: 382.9 on 3 degrees of freedom\nMultiple R-squared:  0.02704,   Adjusted R-squared:  -0.2973 \nF-statistic: 0.08338 on 1 and 3 DF,  p-value: 0.7916\n\n\n\n\n",
      "last_modified": "2023-06-29T12:56:50+02:00"
    },
    {
      "path": "statistik_i.html",
      "title": "Statistik 1",
      "description": "This tab will contain the topics covered in the course Statistik I \n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\n\n\n",
      "last_modified": "2023-06-29T12:56:51+02:00"
    },
    {
      "path": "statistik_ii.html",
      "title": "Untitled",
      "description": "A new article created using the Distill format.\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\n\n\n",
      "last_modified": "2023-06-29T12:56:51+02:00"
    }
  ],
  "collections": []
}
