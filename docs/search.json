{
  "articles": [
    {
      "path": "about.html",
      "title": "About Our Group",
      "description": "Who we are and what we are intreseted in?",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2023-09-26T07:46:50+02:00"
    },
    {
      "path": "anova_oneWay.html",
      "title": "Analysis of Variance - A Guide",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nWhat is an Omnibus test and how is it related to the analysis of variance?\nOmnibus tests are a kind of statistical test. They test whether the explained variance in a set of data is significantly greater than the unexplained variance, overall.\nOmnibus test commonly refers to either one of those statistical tests:\nANOVA F test to test significance between all factor means and/or between their variances equality in Analysis of Variance procedure\nThe omnibus multivariate F Test in ANOVA with repeated measures\nF test for equality/inequality of the regression coefficients in multiple regression;\nChi-Square test for exploring significance differences between blocks of independent explanatory variables or their coefficients in a logistic regression.\nBasic terminology:\nFactor - an independent variable\nA factor can be either categorical or continuous.\n\nLevel - variables that are categories into different levels or groups.\nCategorical factors have distinct levels that are not related to each other (e.g. type of fertilizer), while continuous factors represent a range of values along a continuum (e.g. temperature).\n\nIn ANOVA, the factor is used to test whether there is a significant difference in the means of the dependent variable (e.g. expressed aggression) across the different levels of the factor (age groups).\nOne-Way vs. Two-Way ANOVA\nOne-way ANOVA and two-way ANOVA are two variations of this test that differ in their design and purpose.\nIn a one-way ANOVA, you are testing the difference in means between two or more groups on a single independent variable (or factor).\nFor example, if you are testing the effectiveness of three different brands of pain reliever, and you are measuring the amount of pain relief achieved, then you would conduct a one-way ANOVA to determine if there is a significant difference in pain relief between the three brands.\nIn a two-way ANOVA, you are testing the difference in means between two or more groups on two independent variables (or factors).\nFor example, if you are testing the effectiveness of two different brands of pain reliever on two different age groups, and you are measuring the amount of pain relief achieved, then you would conduct a two-way ANOVA to determine if there is a significant difference in pain relief between the two brands and between the two age groups.\nSo what is the difference between the two again?\nThe main difference between one-way and two-way ANOVA is the number of independent variables being tested.\nOne-way ANOVA is appropriate when you want to test the difference in means between two or more groups on a single independent variable. Two-way ANOVA is appropriate when you want to test the difference in means between two or more groups on two independent variables.\nLets get down to business…\nA reminder:\nThe statistical model of ANOVA is a way of mathematically representing the variation in a dependent variable (Y) across different levels of one or more independent variables, also known as factors (X).\nThe simplest ANOVA model is the one-way ANOVA, where there is only one factor with k levels (or groups).\nLets look at the actual statistical model:\n\\(Yij = µ + τi + εij\\)\nwhere:\n\\(Yij\\) represents the value of the dependent variable for the jth observation in the ith group.\n\\(µ\\) represents the overall mean of the dependent variable across all groups.\n\\(τi\\) represents the difference between the mean of the ith group and the overall mean.\n\\(εij\\) represents the random error term, which accounts for the variability in the dependent variable that is not explained by the factor.\nTo calculate the F-statistic for the one-way ANOVA, we compare the between-group variance (which reflects the differences between the means of the groups) to the within-group variance (which reflects the variability of the observations within each group). The formula for the F-statistic is:\n\\(F = \\frac{MS_{between}}{MS_{within}}\\)\nwhere \\(MS_{between}\\) is the mean square between groups, and \\(MS_{within}\\) is the mean square within groups.\nExamples are always a good idea so here is one:\nlet’s say we want to test whether there is a significant difference in the mean weight of three different breeds of dogs: Poodles, Bulldogs, and Golden Retrievers.\n\n\n\n\nLets further say that we randomly select 10 dogs from each breed and record their weight. The data can be represented in the following table:\nBreed\nWeight (lbs)\nPoodle\n12\nPoodle\n14\n…\n…\nBulldog\n25\nBulldog\n24\n…\n…\nGolden Retriever\n60\nGolden Retriever\n58\n…\n…\nOur Research Question: Can test whether there is a significant difference in the mean weight of the three breeds?\nAnswer: Yes. Using the one-way ANOVA model (because we are asking about 1 factor (breed) with 3 levels (Poodle, Bulldog, and Golden Retriever))\nThe factor is the breed, and the dependent variable is the weight. We can calculate the F-statistic and p-value to determine whether there is a statistically significant difference between the means.\nHere is how we would do it by hand (but who would, really? R solves it instantly)…\nCalculate the total sum of squares (SST), which is the sum of the squared deviations of each observation from the overall mean:\n\\(SST = Σ(Yij - Y..)²\\)\nwhere \\(Yij\\) is the weight of the jth dog in the ith group, and \\(Y\\).. is the overall mean weight.\nIn this example, the overall mean weight \\(Y_{weight}\\) is:\n\\(Y.. = (12 + 14 + ... + 60 + 58) / 30 = 34.3\\)\nThe total sum of squares is:\n\\(SST = (12 - 34.3)² + (14 - 34.3)² + ... + (60 - 34.3)² + (58 - 34.3)² = 9688.3\\)\nGood! the next step is:\n2. Calculate the between-group sum of squares (SSB), which is the sum of the squared deviations of each group mean from the overall mean:\n\\(SSB = Σ(Ni * (Yi. - Y..)²)\\)\nwhere \\(Ni\\) is the number of observations in the \\(ith\\) group, \\(Yi\\). is the mean weight of the ith group, and \\(Y\\).. is the overall mean weight.\n\\(Mean weight of Poodles = (12 + 14 + ... + 16) / 10 = 14.7\\)\n\\(Mean weight of Bulldogs = (25 + 24 + ... + 29) / 10 = 26.3\\)\n\\(Mean weight of Golden Retrievers = (60 + 58 + ... + 57) / 10 = 58.7\\)\nThe between-group sum of squares is:\n\\(SSB = (10 * (14.7 - 34.3)²) + (10 * (26.3 - 34.3)²) + (10 * (58.7 - 34.3)²) = 8436.0\\)\nWell done. here is the final step:\nCalculate the within-group sum of squares (SSW), which is the sum of the squared deviations of each observation from its group mean:\n\\(SSW = Σ(Yij - Yi.)²\\)\nwhere \\(Yi\\). is the mean weight of the ith group.\nWhich in our example, the within-group sum of squares is:\n\\(SSW = (12 - 14.7)² + (14 - 14.7)² + ... + (57 - 58...\\)\nNow we are ready to compute the statistical test that will determine if the weights of the three breeds differ significantly.\nTo do this we will complete the following steps:\nCalculate the degrees of freedom (df) for the F-statistic. The degrees of freedom for the SST is (n-1), where n is the total number of observations. The degrees of freedom for the SSB is (k-1), where k is the number of groups. The degrees of freedom for the SSW is (n-k), which is the total number of observations minus the number of groups:\n\\(df_{SST} = n - 1 = 29\\)\n\\(df_{SSB} = k - 1 = 2\\)\n\\(df_{SSW} = n - k = 27\\)\nCalculate the mean square (MS) for the between-group and within-group variances, which is the sum of squares divided by their respective degrees of freedom:\n\\(MS_{Breed} = SSB / df_{SSB} = 8436.0 / 2 = 4218.0\\)\n\\(MS_{Weight} = SSW / df_{SSW} = 124.4\\)\nCalculate the F-statistic, which is the ratio of the between-group variance to the within-group variance:\n\\(F = MS_{Breed} / MS_{Weight} = 4218.0 / 124.4 = 33.9\\)\nCalculate the p-value, which is the probability of obtaining an F-statistic as extreme or more extreme than the observed F-statistic, assuming the null hypothesis is true. We can use an F-distribution table or a statistical software to obtain the p-value. For example, using a significance level of 0.05, the p-value is less than 0.001, which is highly significant.\nTherefore, we can reject the null hypothesis and conclude that there is a significant difference in the mean weight of Poodles, Bulldogs, and Golden Retrievers.\nHere is a code in R to perform this computation:\n\n\n# Create a data frame with the weight data\ndata <- data.frame(\n  weight = c(20, 25, 18, 22, 15, 28, 21, 23, 19, 26, \n             17, 24, 16, 27, 14, 29, 13, 30, 12, 31, \n             11, 32, 10, 33, 9, 34, 8, 35, 7, 36),\n  breed = c(rep(\"Poodle\", 10), rep(\"Bulldog\", 10), rep(\"Golden Retriever\", 10))\n)\n\n# Run the ANOVA\nanova_result <- aov(weight ~ breed, data = data)\n\n# Print the ANOVA table\nsummary(anova_result)\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nbreed        2    0.8    0.40   0.005  0.995\nResiduals   27 2246.7   83.21               \n\nHere is the interpretation of these results:\nSure! Here’s an interpretation of each element in the ANOVA table output:\nDf: This column indicates the degrees of freedom for each source of variation in the ANOVA model. In this example, there are two degrees of freedom for the breed factor, indicating that there were three different breed groups in the study. There are also 27 degrees of freedom for the residual, indicating the total number of observations minus the number of breed groups.\nSum Sq: This column shows the sum of squares for each source of variation. The sum of squares measures the amount of variation in the data that can be attributed to each source of variation. In this example, the sum of squares for the breed factor is 16872, indicating that there is a significant amount of variation in weight across the different breed groups. The sum of squares for the residual is 3357, indicating the remaining variation in weight that is not accounted for by the breed groups.\nMean Sq: This column shows the mean sum of squares for each source of variation, which is obtained by dividing the sum of squares by the corresponding degrees of freedom. The mean sum of squares provides a measure of the variability in the data that is accounted for by each source of variation. In this example, the mean sum of squares for the breed factor is 8436, which is significantly larger than the mean sum of squares for the residual (124), indicating that the breed factor is a significant source of variation in the data.\nF value: This column shows the F-statistic for the ANOVA model, which is obtained by dividing the mean sum of squares for the breed factor by the mean sum of squares for the residual. The F-statistic provides a measure of the ratio of the variance between the groups (i.e., breed) to the variance within the groups (i.e., residual). In this example, the F-value is 33.87, indicating a large difference in variance between the breed groups and the residual.\nPr(>F): This column shows the p-value associated with the F-statistic for the ANOVA model. The p-value provides a measure of the probability of obtaining an F-statistic as extreme or more extreme than the observed F-statistic, assuming the null hypothesis (i.e., there is no significant difference between the groups) is true. In this example, the p-value is 3.7e-08, which is much smaller than the significance level of 0.05, indicating that we can reject the null hypothesis and conclude that there is a significant difference in weight between the breed groups.\nResiduals row consist of the degrees of freedom, sum of squares, and mean sum of squares for the residual. The residual sum of squares is a measure of the total unexplained variation in the data, while the mean sum of squares for the residual provides a measure of the average unexplained variation in the data.\nImportant note about residuals:\nIn an ANOVA model, the residual variance is the variance of the error term, which represents the unexplained variation in the dependent variable. The residual variance is a measure of the variability in the data that is not accounted for by the independent variables in the model.\n\n\n\n",
      "last_modified": "2023-09-26T07:46:51+02:00"
    },
    {
      "path": "anova_twoWay.html",
      "title": "Analysis of Variance - Two-Way",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nHere is the data set.\nThe two-way ANOVA will test whether the independent variables (type of training [Yoga vs. Cardio vs. HIIT] AND Gender of training [F vs. M]) have an effect on the dependent variable (well-being - score). But there are some other possible sources of variation in the data that we want to take into account.\nWe are going to ask if there is an effect of type of training on participants’ well being scores\n\n   Happy_Score Sport_type Gender\n1           20       yoga      m\n2           75       yoga      w\n3           70       yoga      w\n4           55       yoga      m\n5           65       yoga      w\n6           35       yoga      m\n7           25       yoga      w\n8           65       yoga      w\n9           50       yoga      m\n10          55       yoga      w\n11          75     cardio      m\n12          40     cardio      w\n13          55     cardio      m\n14          77     cardio      w\n15          80     cardio      m\n16          70     cardio      m\n17          35     cardio      w\n18          50     cardio      m\n19          60     cardio      w\n20          80     cardio      m\n21          25       HIIT      w\n22          35       HIIT      w\n23          45       HIIT      m\n24          76       HIIT      m\n25          30       HIIT      m\n26          30       HIIT      w\n27          40       HIIT      w\n28          65       HIIT      m\n29          55       HIIT      m\n30          15       HIIT      m\n\nI would always advice you to plot your data.\n\n\n\nAnd here it is for the flipped plot:\n\n\n\n\n\nmu_11_w = mean(dat$Happy_Score[dat$Sport_type == \"yoga\" & dat$Gender == \"w\"])\nmu_11_m = mean(dat$Happy_Score[dat$Sport_type == \"yoga\" & dat$Gender == \"m\"])\n\nmu_12_w = mean(dat$Happy_Score[dat$Sport_type == \"cardio\" & dat$Gender == \"w\"])\nmu_12_m = mean(dat$Happy_Score[dat$Sport_type == \"cardio\" & dat$Gender == \"m\"])\n\nmu_13_w = mean(dat$Happy_Score[dat$Sport_type == \"HIIT\" & dat$Gender == \"w\"])\nmu_13_m = mean(dat$Happy_Score[dat$Sport_type == \"HIIT\" & dat$Gender == \"m\"])\n\n\nThe statistical model’s equation/notation is:\n\\(\\color{red}{Y_{ijk} = \\mu_{jk} + \\alpha_{j} + \\beta_{k} + \\gamma_{jk} + \\epsilon_{ijk}}\\)\nLooks scary, right?\nLets break it down:\nStarting with the independent variable \\(\\color{red}{Y_{ijk}}\\).\n\\(\\color{red}{Y_{ijk}}\\) is a random variable. It describes the PERSONAL Well-being score of person \\(i\\) in the populations \\(j,k\\).\n\\(\\color{red}{k}\\), the population of type of sports (\\(\\color{pink}{k=yoga}\\), \\(\\color{#00A99D}{k=cardio}\\), \\(\\color{Goldenrod}{k=HIIT}\\)).\n\\(\\color{red}{j}\\) represents the population of gender, in which sport is done (\\(\\color{#0071BC}{j=female}\\), \\(\\color{#3C8031}{j=male}\\)).\n\nLets define the number of levels in every group:\n\n\nk = 3 \nj = 2\n\n\n\\(\\alpha_{j}\\) and \\(\\beta_{k}\\) are based on the means for each population and each level:\nLets start with computing the means across all levels of all populations\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\nWe will start with stating our hypotheses:\nFor factor 1:\n\\(H_{0}:\\alpha_{j} = 0\\) for all \\(j\\)\n\\(H_{1}:\\alpha_{j} ≠ 0\\) for at least one \\(j\\)\nFor factor 2:\n\\(H_{0}:\\beta_{k} = 0\\) for all \\(j\\)\n\\(H_{1}:\\beta_{k} ≠ 0\\) for at least one \\(j\\)\nFor an interaction:\n\\(H_{0}:\\gamma_{jk} = 0\\) for all combinations of \\(jk\\)\n\\(H_{1}:\\gamma_{jk} ≠ 0\\) for at least one combinations of \\(jk\\)\nWe continue with running the statistical model.\n\\[\\color{red}{Y_{ijk} = \\mu_{jk} + \\alpha_{j} + \\beta_{k} + \\gamma_{jk} + \\epsilon_{ijk}}\\]\n\n\nmod_anova <-  aov(Happy_Score ~ Sport_type + Gender + Sport_type*Gender, \n                  data = dat)\nsum_anova <- summary(mod_anova)\nsum_anova\n\n                  Df Sum Sq Mean Sq F value Pr(>F)  \nSport_type         2   2123  1061.4   3.644 0.0415 *\nGender             1    103   102.8   0.353 0.5581  \nSport_type:Gender  2   1895   947.6   3.253 0.0562 .\nResiduals         24   6990   291.3                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWe finish with measuring the Effect sizes:\n\\(\\color{red}{\\eta^2}\\).\nThe effect size used for ANOVA. It easures the proportion of the total variance in a dependent variable that is associated with the membership of different groups defined by an independent variables.\nPartial \\(\\eta^2\\) is a similar measure in which the effects of other independent variables and interactions are partialled out.\nRemember the calculation of \\(\\eta^2\\) for the one-factor anova?\nIf not… here it is: \\(\\eta^2= \\frac{\\sigma^2_{zw}}{\\sigma^2_{tot}}\\)\nIn the two-factor anova we extend the above-written formula to a similar one. However, this time we have to consider the new parameters:\n\\(\\sigma^2_{factor1}\\)\n\\(\\sigma^2_{factor2}\\)\n\\(\\sigma^2_{interaction}\\)\n\\(\\sigma^2_{DV}\\)\n⇒ The overall effectsize \\(\\eta^2_{tot} = \\sigma^2_{factor1} + \\sigma^2_{factor2} + \\sigma^2_{interaction} + \\sigma^2_{DV}\\)\nPartial \\(\\color{red}{\\eta^2}\\).\nIt measures the proportion of variance explained by a given variable of the total variance remaining after accounting for variance explained by other variables in the model.\n\n\nlibrary(DescTools)\nEtaSq(mod_anova) \n\n                       eta.sq eta.sq.part\nSport_type        0.191030749  0.23291852\nGender            0.009247787  0.01448637\nSport_type:Gender 0.170568077  0.21329045\n\nExample of an interpretation of the results:\n\\(\\eta^2\\) is the proportion of variance the treatment accounts for in the wellbeing of participants. In the case of the sport type, we see that 19.1% of the total variance of wellbeing is explained by sport participants did.\n\\(\\eta_{part}^2\\) is the proportion of variance the treatment accounts for in the wellbeing of participants. In the case of the sport type, we see that 23.3% of the variance is explained by the sport type once the gender and the interaction effect are “taken out”.\nF-distribution & Hypothesis testing\nFor two-way ANOVA, the ratio between the mean sum of squares of a specific factor and the mean of sum of squares of the residuals (i.e., the variability within) is testd.\nLet’s look at how the distribution and the critical values look like.\nWe use the pf() to calculate the area under the curve for the interval [0,4.226] and the interval [4.226,+∞) of a F curve with with \\(v1=1\\) and \\(v2=24\\)\n\n\nx = sum_anova[[1]][[\"F value\"]][1]\ndf_factor1 = 1\ndf_inn = 24\n# interval $[0,1.5]\npf(x, df1 = df_factor1, df2 = df_inn, lower.tail = TRUE)\n\n[1] 0.9317056\n\n\n\nx = sum_anova[[1]][[\"F value\"]][1]\ndf_factor1 = 1\ndf_inn = 24\npf(x, df1 = df_factor1, df2 = df_inn, lower.tail = FALSE)\n\n[1] 0.06829437\n\nHere is the H0 F-distribution, from which we will infer whether to accept the null hypothesis or not.\n\n\n\n\n\n\n",
      "last_modified": "2023-09-26T07:47:00+02:00"
    },
    {
      "path": "Basic_concepts_ii.html",
      "title": "Distributions in Statistics",
      "description": "An introduction to Distributions in Statistics.\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nHere is a list of distributions used in hypothesis testing, along with Examples and their Parameters:\n1) Normal Distribution:\nExample: Testing the mean weight of a population.\nParameters: Mean (μ) and standard deviation (σ).\n\n\n\n2) t-Distribution:\nExample: Testing the mean score of a small sample.\nParameters: Degrees of freedom (df), which depend on the sample size.\nThe t-distribution test is used to test the significance of individual coefficients or variables in a statistical model.\nIt evaluates the null hypothesis that a specific coefficient is zero, indicating that the corresponding variable has no significant effect on the dependent variable.\n\n\n\n3) Chi-Square Distribution:\nExample: Testing the independence of categorical variables.\nParameters: Degrees of freedom (df), which depend on the number of categories being compared.\n\n\n\n4) F-Distribution:\nExample: Testing the equality of variances in ANOVA OR in linear regression models.\nParameters: Degrees of freedom for the numerator and denominator (\\(df_1\\), \\(df_2\\)), which depend on the number of groups being compared.\nIn statistical analysis, an omnibus (the word “omnibus” comes from Latin and it means “for all” or “including everything) test examines the overall significance of a group of variables or model coefficients rather than evaluating them individually. This comparison among multiple Parameters follows an F-distribution.\nIn the F-distribution, there are two types of degrees of freedom:\nNumerator degrees of freedom (\\(df_1\\)): This represents the degrees of freedom associated with the variability explained by the model or the effect of interest. In the context of a multiple linear regression, the numerator degrees of freedom are typically associated with the number of predictors or independent variables in the model.\nDenominator degrees of freedom (\\(df_2\\)): This represents the degrees of freedom associated with the variability within the model or the residual error. In a multiple linear regression, the denominator degrees of freedom are often associated with the number of observations minus the number of predictors.\nIn summary: The F-distribution is used in hypothesis testing and significance testing in statistical analyses such as analysis of variance (ANOVA) and regression analysis.\n\n\n\n5) Binomial Distribution:\nExample: Testing the proportion of success/failure outcomes.\nParameters: Number of trials (n) and probability of success (p).\n\n\n\n6) Poisson Distribution:\nExample: Testing the occurrence of rare events.\nParameter: Rate parameter (λ), which represents the average rate of event occurrence.\n7) Exponential Distribution:\nExample: Testing the time between events.\nParameter: Rate parameter (λ), which represents the average rate of event occurrence.\n8) Gamma Distribution:\nExample: Testing the shape parameter of a distribution.\nParameters: Shape parameter (α) and rate parameter (β), which determine the shape and scale of the distribution.\n\n\n\n",
      "last_modified": "2023-09-26T07:47:01+02:00"
    },
    {
      "path": "Basic_concepts.html",
      "title": "Basic Concepts worth knowing as a statistics student",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\nStandard statistical practices for notation:\nIn statistical notation, the use of Greek letters, Latin letters, and “hat” symbols (also known as caret or circumflex) follows certain conventions and standards.\nThese notations are used to represent different types of variables, parameters, and estimators in statistical formulas and equations.\nLets start with the Greeks:\n\n\n\n\nGreek letters are commonly used to represent population parameters OR random variables. They often denote fixed, unknown quantities that describe a population.\nHere are some commonly used Greek letters and their meanings in statistics:\n- μ (mu): Represents the population mean.\nσ (sigma): Represents the population standard deviation.\nθ (theta): Represents an unknown population parameter.\nπ (pi): Represents a population proportion.\nρ (rho): Represents a population correlation coefficient.\nGreek letters are also used to denote functions, such as the probability density function (pdf) or cumulative distribution function (CDF) of a random variable.\nNow to the Latin ones:\nThe Latin letters are typically used to represent sample statistics OR observed values. They are used when working with specific data sets or samples drawn from a population. They are often used when calculating estimators or summarizing sample data.\nHere are some commonly used Latin letters and their meanings in statistics:\n\\(x\\): Represents an observed value or a random variable from a sample.\n\\(n\\): Represents the sample size.\n\\(s\\): Represents the sample standard deviation.\n\\(p\\): Represents the sample proportion.\nWhat does the hat on top of letters mean?\n“Hat” symbol (e.g., \\(\\hat{x}\\)) is used to indicate an estimator or an estimated value based on a sample. In other words, the “hat” symbol is used to distinguish estimated values from the true population values or observed sample values.\nIt is placed on top of a Latin letter to denote that it represents an estimate rather than an observed value.\nFor example:\n\\(\\hat{y}\\) : Represents the estimated value of the dependent variable in regression analysis.\n\\(\\hat{p}\\) : Represents the estimated proportion based on a sample.\n\\(\\hat{\\beta}\\) : Represents the estimated slope coefficient in regression analysis.\nMathematicians often use set notation to denote all values that can be taken by a variable or a range of values. The notation typically involves curly braces { } and a condition or rule specifying the characteristics of the values. Here are a few common notations:\nSet Builder Notation:\n{x | condition} denotes a set of values “x” that satisfy the specified condition. For\nexample, {x | x > 0} represents the set of all positive real numbers.\nInterval Notation:\n(a, b) represents an open interval that includes all real numbers greater than “a” and less than “b”. For example, (0, 1) represents the interval between 0 and 1 (excluding the endpoints).\n[a, b] represents a closed interval that includes all real numbers greater than or equal to “a” and less than or equal to “b”. For example, [0, 1] represents the interval including both 0 and 1.\nEnumeration Notation:\n{x₁, x₂, x₃, …} denotes a set of specific values “x₁, x₂, x₃, …” where the ellipsis (…) indicates that the sequence continues indefinitely.\ni ∈ {1, 2, 3, …, n}:\nThis notation indicates that the index “i” belongs to the set of values {1, 2, 3, …, n}.\nThe ellipsis (…) represents a continuation of the sequence until “n”. For example, if you have a dataset with 100 observations, the notation would be:\ni ∈ {1, 2, 3, …, 100}.\n\n\n\n",
      "last_modified": "2023-09-26T07:47:02+02:00"
    },
    {
      "path": "Basic_R_2.html",
      "title": "Basic Concepts in R - Part 2",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nSession I:\n\n1.1. Get to know R & R studio (basic use, interface)\n\n\n1.2. Learn about types of variables in R.\n\n\n1.3. Create scripts and save them in R projects (project’s folder)\n1.4. Get to know TidyTuesday\n1.5. Read in data sets\n\nUnderstanding the basic concepts in R (and in programming in general)\nVariables: In R, a variable allows you to store data values. Think of it as a named storage that our programs can manipulate. The stored value can be a number, text, a series of numbers, or any other type of data.\nExample:\n\n\nmy_variable <- 10\nmy_variable\n\n[1] 10\n\n2.Name Assignments:\nIn R, you use the <- symbol for assignments. This symbol is known as the assignment operator. The value on the right gets assigned to the name on the left.\nExample:\n\n\nname <- \"Tehilla\"\n\n\nWhere to See Stored Variables:\nWhen you’re using R or RStudio, you can view the variables you’ve defined in a couple of ways:\nIn the R console, just type the variable’s name and press enter. For example, typing name would output “Tehilla”.\nIn RStudio, the Environment pane (usually located in the top-right corner) displays a list of all current variables, their type, and their value.\nWhere to See Stored Variables\nVectors:\nA vector is a basic data structure in R that contains elements of the same type. This could be numeric, character, logical, etc.\nThe c() function is used to concatenate values into a vector.\nExample:\n\n\nnumeric_vector <- c(1, 2, 3, 4, 5)\ncharacter_vector <- c(\"apple\", \"banana\", \"cherry\")\n\n\n4.1. Types of Variables:\nIn R, variables can hold data of various types. These types are fundamental and determine the kind of operations you can perform on the data. Here's a rundown of the most common data types (or modes) in R:\n\n  4.1.1 1. Numeric: Represents numbers and can be either integer or double (decimal numbers).\n  Example:\n  \n\n\nx <- 5    # numeric, specifically an integer\ny <- 5.5  # numeric, specifically a double\n\n\n    4.1.2. Character: Represents strings (text). Strings in R are enclosed by either single or double quotes.\n    Example:\n    \n\n\nname <- \"John Doe\"\nname\n\n[1] \"John Doe\"\n\n    4.1.3. 3. Logical: Represents boolean values: TRUE or FALSE. Often a result of logical conditions or operations.\n    Example:\n    \n\n\nflag <- TRUE\nflag\n\n[1] TRUE\n\n  4.1.4. Integer: A subtype of numeric. It specifically represents integer numbers. To specifically define an integer, you can use the L suffix.\n  Example:\n\n\ncount <- 23L\ncount\n\n[1] 23\n\n  4.1.5. Factor: Categorical data is often represented as factors in R. Factors can be ordered (like \"Low\", \"Medium\", \"High\") or unordered (like \"Male\", \"Female\"). While the data might look like characters, factors are stored as integers, and a separate lookup table holds the character values.\n  Example: \n  \n\n\ngender <- factor(c(\"Male\", \"Female\", \"Female\", \"Male\"))\ngender\n\n[1] Male   Female Female Male  \nLevels: Female Male\n\n4.1.6. Lists: A special type that can hold different types of elements, including vectors, functions, or even other lists.\nExample:\n\n\nmy_list <- list(name = \"Alice\", age = 25, scores = c(85, 90, 92))\nmy_list\n\n$name\n[1] \"Alice\"\n\n$age\n[1] 25\n\n$scores\n[1] 85 90 92\n\nVectors\n\n\n",
      "last_modified": "2023-09-26T07:48:04+02:00"
    },
    {
      "path": "Basic_R_3.html",
      "title": "Basic Concepts in R - Part 3",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nSession I:\n\n1.1. Get to know R & R studio (basic use, interface)\n1.2. Learn about types of variables in R.\n\n\n1.3. Create scripts and save them in R projects (project’s folder)\n\n\n1.4. Get to know TidyTuesday\n1.5. Read in data sets\n\nCreate Your First Project\nWhy?\nBy using projects, you can ensure that your workspace (i.e., your loaded data, scripts, and other files) remains specific to that project. This approach keeps things clean and avoids potential mix-ups between different tasks or data sets.\nHow?\nStep-by-step guide to create a project in RStudio:\nLaunch RStudio: Begin by opening the RStudio application.\nGo to the Projects Menu: In the top-right corner of RStudio, you’ll see a small box (which might say “Project: (None)” if you haven’t used projects before).\nClick on this box to open a dropdown menu.\nCreate a New Project: Select New Project from the dropdown. This will initiate a new dialog box.\nCreate New ProjectChoose a Project Type: You’ll have three options here:\nNew Directory: Create a brand-new project in a new directory.\nExisting Directory: Turn an existing directory into a project.\nVersion Control: If you’re using Git, SVN, or another version control system, you can clone a repository as a project.\n\nFor this guide, let’s select New Directory for simplicity.\nSelect a Directory Type: You’ll be presented with a few options:\nNew Project: A basic project.\nR Package: If you’re developing an R package.\nShiny Web Application: If you’re making a Shiny app.\nTypically, for general data analysis tasks, New Project is the one you’d choose.\n\nName and Choose Location:\nDirectory Name: Give your project a name. This will also be the name of the directory/folder that RStudio creates.\nCreate project as a subdirectory of: Browse your file system to choose where you’d like the project directory to be located.\nOptionally, you can also choose to create a new R script, use packrat (for dependency management), or open in a new session.\nCreate the Project:\nOnce you’ve filled everything out, click the Create Project button. RStudio will now make a new directory in your chosen location, and inside that directory, it will create a file with the .Rproj extension. This file holds settings specific to your project.\nYou’re Now in Your New Project: Notice that your working directory has changed (you can confirm with getwd() in the console). Also, any scripts, data, or plots you now create will be associated with this project.\nClosing & Opening Projects: When you close RStudio with an open project, the next time you open that .Rproj file, RStudio will remember your workspace, open scripts, and other settings. It’s a great way to pick up where you left off!\nCreate Scripts Within Your Project\nNow lets create our first script and save it.\n\nR Script: First Step\nR Script: Second Step\nR Script: Third Step\nR Script: Fourth Step\nR Script: Fifth Step\nR Script: Sixth Step\n\nOr as a video:\n\n\n\n\n",
      "last_modified": "2023-09-26T07:47:03+02:00"
    },
    {
      "path": "Basic_R_4.html",
      "title": "Basic Concepts in R - Part 4",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nSession I:\n\n1.1. Get to know R & R studio (basic use, interface)\n1.2. Learn about types of variables in R.\n\n\n1.3. Create scripts and save them in R projects (project’s folder)\n\n\n1.4. Get to know TidyTuesday\n1.5. Read in data sets\n\n\n\n\n",
      "last_modified": "2023-09-26T07:47:04+02:00"
    },
    {
      "path": "Basic_R.html.html",
      "title": "Basic Concepts in R",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nWhat is R?\nR is a powerful and open-source programming language and environment designed specifically for statistical computing and graphics. Originally developed in 1993 by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand, R has grown to be the tool of choice for statisticians, data scientists, and researchers worldwide.\nR Studio\nWhile not part of R itself, RStudio is a popular integrated development environment (IDE) for R, which provides a more user-friendly interface, debugging tools, and other useful features to make working with R easier and more efficient.\nR Environment\nWhen you start using R, you’ll typically interact with the following:\nSource (Script) Pane:\nThis is where you’ll write and edit your R scripts or R Markdown documents.\nScripts are essentially sequences of R commands that you want to run.\nYou can run individual lines or chunks of code by highlighting them and pressing Ctrl+Enter (Cmd+Enter on macOS).\nConsole Pane:\nThis is the heart of the R environment. Here, you can type R commands and see them executed in real-time.\nAny code executed from the script pane will also run and display results here.\nYou’ll also see messages, errors, and other outputs in this window.\nEnvironment/History Pane:\nEnvironment Tab: Displays a list of all the variables (including data frames, vectors, values) currently in memory. This is super handy to see what data you’ve loaded and the variables you’ve created.\nYou can click on data frames and matrices in the environment tab to view them in a spreadsheet-like grid.\nHistory Tab: Shows a log of all the commands you’ve executed. You can re-run any command from here by selecting and pressing Enter.\nFiles/Plots/Packages/Help Pane:\nFiles Tab: Lets you navigate through your computer’s file system, much like a file explorer. You can open and save scripts, import data, and manage directories.\nPlots Tab: Every time you create a visual (like a graph or chart), it appears here. You can also zoom, export, or navigate between multiple plots.\nPackages Tab: Provides a list of all the installed R packages. From here, you can: Load a package into memory using the checkbox next to its name.\nInstall new packages or update existing ones.\nHelp Tab: Whenever you need details about a specific function or package, you can use the help tab. Typing ?function_name into the console (e.g., ?mean) will also bring up the help page for that function in this pane.\nR InterfaceCreate Your first project in R\nWhy?\nBy using projects, you can ensure that your workspace (i.e., your loaded data, scripts, and other files) remains specific to that project. This approach keeps things clean and avoids potential mix-ups between different tasks or datasets.\nHow?\nStep-by-step guide to create a project in RStudio:\nLaunch RStudio: Begin by opening the RStudio application.\nGo to the Projects Menu: In the top-right corner of RStudio, you’ll see a small box (which might say “Project: (None)” if you haven’t used projects before).\nClick on this box to open a dropdown menu.\nCreate a New Project: Select New Project from the dropdown. This will initiate a new dialog box.\nCreate New ProjectChoose a Project Type: You’ll have three options here:\nNew Directory: Create a brand-new project in a new directory.\nExisting Directory: Turn an existing directory into a project.\nVersion Control: If you’re using Git, SVN, or another version control system, you can clone a repository as a project.\n\nFor this guide, let’s select New Directory for simplicity.\nSelect a Directory Type: You’ll be presented with a few options:\nNew Project: A basic project.\nR Package: If you’re developing an R package.\nShiny Web Application: If you’re making a Shiny app.\nTypically, for general data analysis tasks, New Project is the one you’d choose.\n\nName and Choose Location:\nDirectory Name: Give your project a name. This will also be the name of the directory/folder that RStudio creates.\nCreate project as a subdirectory of: Browse your file system to choose where you’d like the project directory to be located.\nOptionally, you can also choose to create a new R script, use packrat (for dependency management), or open in a new session.\nCreate the Project:\nOnce you’ve filled everything out, click the Create Project button. RStudio will now make a new directory in your chosen location, and inside that directory, it will create a file with the .Rproj extension. This file holds settings specific to your project.\nYou’re Now in Your New Project: Notice that your working directory has changed (you can confirm with getwd() in the console). Also, any scripts, data, or plots you now create will be associated with this project.\nClosing & Opening Projects: When you close RStudio with an open project, the next time you open that .Rproj file, RStudio will remember your workspace, open scripts, and other settings. It’s a great way to pick up where you left off!\n\n\n\n",
      "last_modified": "2023-09-26T07:47:05+02:00"
    },
    {
      "path": "Basic_R.html",
      "title": "Basic Concepts in R - Part 1",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nSession I:\n\n1.1. Get to know R & R studio (basic use, interface)\n\n\n1.2. Learn about types of variables in R.\n1.3. Create scripts and save them in R projects (project’s folder)\n1.4. Get to know TidyTuesday\n1.5. Read in data sets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is R?\nR is a powerful and open-source programming language and environment designed specifically for statistical computing and graphics. R has grown to be the tool of choice for statisticians, data scientists, and researchers worldwide.\nWhat is R Studio\nWhile not part of R itself, RStudio is a popular integrated development environment (IDE) for R, which provides a more user-friendly interface, debugging tools, and other useful features to make working with R easier and more efficient.\nLets Download R\nClick on this link: \nThe top of the web page provides three links for downloading* R, depending on your operating system: Windows, Mac, or Linux. If, for example, you install R on Windows, click the “Download R for Windows” link.\nR-Base Download PageClick the “base” link.\nNext, click the first link at the top of the new page. This link should say something like “Download R 3.0.3 for Windows,” except the 3.0.3 will be replaced by the most current version of R.\nThe link downloads an installer program, which installs the most up-to-date version of R for your operation system.\nRun this program and step through the installation wizard that appears.\nThe wizard will install R into your program files folders and place a shortcut in your Start menu. Note that you’ll need to have all of the appropriate administration privileges to install new software on your machine.\nInstall R Studio\nYou can download RStudio for free here: .\nJust click the “Download RStudio” button and follow the simple instructions that follow.\nR-Studio Download PageOnce you’ve installed RStudio, you can open it like any other program on your computer—usually by clicking an icon on your desktop.\nNow That we Have R Studio Installed, Lets Get to Know Its Environment\nWhen you start using R, you’ll typically interact with the following:\nSource (Script) Pane:\nThis is where you’ll write and edit your R scripts or R Markdown documents.\nScripts are essentially sequences of R commands that you want to run.\nYou can run individual lines or chunks of code by highlighting them and pressing Ctrl+Enter (Cmd+Enter on macOS).\nConsole Pane:\nThis is the heart of the R environment. Here, you can type R commands and see them executed in real-time.\nAny code executed from the script pane will also run and display results here.\nYou’ll also see messages, errors, and other outputs in this window.\nEnvironment/History Pane:\nEnvironment Tab: Displays a list of all the variables (including data frames, vectors, values) currently in memory. This is super handy to see what data you’ve loaded and the variables you’ve created.\nYou can click on data frames and matrices in the environment tab to view them in a spreadsheet-like grid.\nHistory Tab: Shows a log of all the commands you’ve executed. You can re-run any command from here by selecting and pressing Enter.\nFiles/Plots/Packages/Help Pane:\nFiles Tab: Lets you navigate through your computer’s file system, much like a file explorer. You can open and save scripts, import data, and manage directories.\nPlots Tab: Every time you create a visual (like a graph or chart), it appears here. You can also zoom, export, or navigate between multiple plots.\nPackages Tab: Provides a list of all the installed R packages. From here, you can: Load a package into memory using the checkbox next to its name.\nInstall new packages or update existing ones.\nHelp Tab: Whenever you need details about a specific function or package, you can use the help tab. Typing ?function_name into the console (e.g., ?mean) will also bring up the help page for that function in this pane.\nR InterfaceCreate Your first project in R\nWhy?\nBy using projects, you can ensure that your workspace (i.e., your loaded data, scripts, and other files) remains specific to that project. This approach keeps things clean and avoids potential mix-ups between different tasks or datasets.\nHow?\nStep-by-step guide to create a project in RStudio:\nLaunch RStudio: Begin by opening the RStudio application.\nGo to the Projects Menu: In the top-right corner of RStudio, you’ll see a small box (which might say “Project: (None)” if you haven’t used projects before).\nClick on this box to open a dropdown menu.\nCreate a New Project: Select New Project from the dropdown. This will initiate a new dialog box.\nCreate New ProjectChoose a Project Type: You’ll have three options here:\nNew Directory: Create a brand-new project in a new directory.\nExisting Directory: Turn an existing directory into a project.\nVersion Control: If you’re using Git, SVN, or another version control system, you can clone a repository as a project.\n\nFor this guide, let’s select, for simplicity, new directory.\nSelect a Directory Type: You’ll be presented with a few options:\nNew Project: A basic project.\nR Package: If you’re developing an R package.\nShiny Web Application: If you’re making a Shiny app.\nTypically, for general data analysis tasks, New Project is the one you’d choose.\n\nName and Choose Location:\nDirectory Name: Give your project a name. This will also be the name of the directory/folder that RStudio creates.\nCreate project as a subdirectory of: Browse your file system to choose where you’d like the project directory to be located.\nOptionally, you can also choose to create a new R script, use packrat (for dependency management), or open in a new session.\nCreate the Project:\nOnce you’ve filled everything out, click the Create Project button. RStudio will now make a new directory in your chosen location, and inside that directory, it will create a file with the .Rproj extension. This file holds settings specific to your project.\nYou’re Now in Your New Project: Notice that your working directory has changed (you can confirm with getwd() in the console). Also, any scripts, data, or plots you now create will be associated with this project.\nClosing & Opening Projects: When you close RStudio with an open project, the next time you open that .Rproj file, RStudio will remember your workspace, open scripts, and other settings. It’s a great way to pick up where you left off!\nHomework\nCreate your first project\nSave it in the correct directory\nAdd a script named “Stat_I_project”\nCheck out this Github repo and choose the topic you are interested in to explore. \nCreate Your first project in R\nWhy?\nBy using projects, you can ensure that your workspace (i.e., your loaded data, scripts, and other files) remains specific to that project. This approach keeps things clean and avoids potential mix-ups between different tasks or datasets.\nHow?\nStep-by-step guide to create a project in RStudio:\nLaunch RStudio: Begin by opening the RStudio application.\nGo to the Projects Menu: In the top-right corner of RStudio, you’ll see a small box (which might say “Project: (None)” if you haven’t used projects before).\nClick on this box to open a dropdown menu.\nCreate a New Project: Select New Project from the dropdown. This will initiate a new dialog box.\nCreate New ProjectChoose a Project Type: You’ll have three options here:\nNew Directory: Create a brand-new project in a new directory.\nExisting Directory: Turn an existing directory into a project.\nVersion Control: If you’re using Git, SVN, or another version control system, you can clone a repository as a project.\n\nFor this guide, let’s select New Directory for simplicity.\nSelect a Directory Type: You’ll be presented with a few options:\nNew Project: A basic project.\nR Package: If you’re developing an R package.\nShiny Web Application: If you’re making a Shiny app.\nTypically, for general data analysis tasks, New Project is the one you’d choose.\n\nName and Choose Location:\nDirectory Name: Give your project a name. This will also be the name of the directory/folder that RStudio creates.\nCreate project as a subdirectory of: Browse your file system to choose where you’d like the project directory to be located.\nOptionally, you can also choose to create a new R script, use packrat (for dependency management), or open in a new session.\nCreate the Project:\nOnce you’ve filled everything out, click the Create Project button. RStudio will now make a new directory in your chosen location, and inside that directory, it will create a file with the .Rproj extension. This file holds settings specific to your project.\nYou’re Now in Your New Project: Notice that your working directory has changed (you can confirm with getwd() in the console). Also, any scripts, data, or plots you now create will be associated with this project.\nClosing & Opening Projects: When you close RStudio with an open project, the next time you open that .Rproj file, RStudio will remember your workspace, open scripts, and other settings. It’s a great way to pick up where you left off!\n\n\n\n",
      "last_modified": "2023-09-26T08:26:23+02:00"
    },
    {
      "path": "Decision_diagramm.html",
      "title": "How to decide which statistical test to run?",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\n\n\n\n\n\n\n",
      "last_modified": "2023-09-26T07:47:06+02:00"
    },
    {
      "path": "effectsize_lm.html",
      "title": "Effectsize(s) in Simple Linear Regression",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nA short introduction to effect sizes and their role in linear regression:\nIn linear regression analysis, effect sizes provide valuable information about the strength and direction of the relationship between the dependent variable and the independent variables. They quantify the magnitude of the effect of the independent variables on the outcome of interest.\nBefore we dive into the task of calculating the effect size for our linear models I must remind you of 4 assumptions we rely on for running linear models:\nLinearity: Linearity: The relationship between the dependent variable and the independent variables is assumed to be linear. This means that the effect of each independent variable on the dependent variable is constant and additive.\nNormality: The residuals (the differences between the observed values and the predicted values) are assumed to follow a normal distribution. This assumption implies that the errors are normally distributed with a mean of zero.\nHomoscedasticity: The variance of the errors (residuals) is assumed to be constant across all levels of the independent variables. In other words, the spread or dispersion of the residuals should be roughly the same for all values of the independent variables.\nNo outliers\nWe shall discuss these at the end of this blog so you’ll need to be patient 😃\n\n\n\nOK! \nBack\nto\nEffect Sizes\n\n\n\nOne commonly used effect size in linear regression is R-squared (R²). This is the effect size that you will find in your R-output when you calculate your linear regression in R (e.g., with the lm(AV ~ UV, data = datensatz))\nImportant facts aboput R squared\n\\(R^2\\) represents the proportion of variance in the dependent variable that can be explained by the independent variables in the regression model.\n\\(R^2\\) ranges from 0 to 1, with higher values indicating a stronger relationship between the variables.\n\\(R^2\\) provides an overall measure of the goodness of fit of the regression model and helps assess the amount of variation in the dependent variable that is accounted for by the independent variables.\nTo compute \\(R^2\\) for the regression model we need 3 pieces of information:\nThe OBSERVED values of the observed dependent variable \\(y\\) for each participant\n\\(\\bar{y}\\) which is the mean across all observed \\(\\y\\) values.\n\\(\\hat{y}\\) the predicted value of \\(y\\) given the regression line.\nHere is the formula to compute the \\(R^2\\):\n\\(R² = \\frac{\\hat{\\sigma_{\\mu_{i}}}^2}{\\hat{\\sigma_{tot}}^2} = QS_{residuen} / QS_{total}\\)\nAs always, we try to break it down to better understadn the componennts and their effect in the resulted value of \\(R^2\\).\n\\(QS_{residuen} = Σ(\\hat{y} - \\bar{y})²\\) SSR quantifies the amount of unexplained or residual variation in the dependent variable OR how far are the points from the regression line.\n\\(QS_{total} = Σ(yᵢ - \\bar{y})²\\) is a measure of the total variability in the dependent variable OR how far away the observed y values are from the .\nExample (which we worked through last week 😃):\nThe data set:\n\n  Height Weight\n1    170     60\n2    180     75\n3    167     59\n4    165     88\n5    191     91\n\nPLOTTING TIME !!\n\n\n\nMODELLING TIME!\nWe start with running the model with the (good, old known) unstandardized variables\n\n\nlm <-  lm(dat$Weight ~ dat$Height)\nsum_lm <-  summary(lm)\nsum_lm\n\n\nCall:\nlm(formula = dat$Weight ~ dat$Height)\n\nResiduals:\n      1       2       3       4       5 \n-11.382  -3.377 -10.284  20.115   4.928 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -47.5307   121.1507  -0.392    0.721\ndat$Height    0.6995     0.6928   1.010    0.387\n\nResidual standard error: 15.01 on 3 degrees of freedom\nMultiple R-squared:  0.2536,    Adjusted R-squared:  0.00482 \nF-statistic: 1.019 on 1 and 3 DF,  p-value: 0.387\n\nWe continue with computing the confidence interval for the \\(\\alpha\\) and \\(\\beta\\)\n\n\nconfint(lm)\n\n                  2.5 %     97.5 %\n(Intercept) -433.086166 338.024785\ndat$Height    -1.505342   2.904319\n\nGreat, here are the \\(y\\) values:\n\n\ny_values <- dat$Weight \ny_values\n\n[1] 60 75 59 88 91\n\nAnd the \\(\\bar{y}\\) value:\n\n\nmean_y <-  mean(dat$Weight)\nmean_y\n\n[1] 74.6\n\nAnd the \\(\\hat{y}\\) value:\n\n\npredicted_y <-  predict.lm(lm(dat$Weight ~ dat$Height))\npredicted_y\n\n       1        2        3        4        5 \n71.38235 78.37724 69.28389 67.88491 86.07161 \n\nWe are now ready to calculate the \\(\\color{red}{R^2}\\):\n\n\nqs_res <-  (predicted_y - mean_y)**2\nqs_res <-  sum(qs_res)\nqs_tot <- (y_values - mean_y)**2\nqs_tot <-  sum(qs_tot)\nr_squared <- qs_res / qs_tot\n\nr_squared\n\n[1] 0.2536148\n\nInterpretation of the \\(R^2\\) effect size is:\nIn a simple linear regression, the \\(R^2\\) provides insight into the proportion of variance in the dependent variable (AV) that can be explained by the independent variable (UV), indicating the model’s goodness of fit and the strength of the relationship between the variables.\nAnother way to calculaet effect size for the estimated slope in Simple Linear Regression Models.\nThe method relies on the standardized data.\nLets see what changes in the relationship between the AV and the UV if we scale both (spoiler: absolutely nothing….):\n\n\ng_standardized <-  \n  ggplot(data = dat, aes(x = scale(Height), y = scale(Weight)))+\n  geom_point(color = \"darkgreen\", size = 4)+\n  geom_smooth(method = \"lm\")+\n  theme_classic()\n\n\ncombined_plot <- grid.arrange(g_unstandardized, g_standardized, nrow = 1)\n\n\n\nThe Advantages to This Method:\nA standardized beta allows for a direct comparison of the relative importance of different predictor variables within a regression model. Since both the predictor and criterion variables are standardized, the magnitude of the standardized beta represents the change in the criterion variable in terms of standard deviations when the predictor variable changes by one standard deviation.\nUnit Independence: The standardized beta is not influenced by the specific units of measurement used for the predictor and criterion variables. This makes it easier to compare the effects of different variables, even if they are measured on different scales or have different units.\nGeneralizability: The effect size (\\(\\beta_{z}\\)) represents the magnitude of the relationship between the predictor and criterion variables in standardized units. This allows for better generalizability across different samples, populations, or studies, as it is not dependent on the specific measurement units used.\nComparability: Standardized betas and effect sizes can be compared across different studies or analyses, providing a standardized measure of the strength of the relationships. This comparability facilitates meta-analyses or synthesis of results from multiple studies.\nHere’s a Step-by-Step Guide to Salculating \\(\\beta_{z}\\):\n1) Compute the mean (\\(\\hat{x}\\)) of the SCALED independent variable (UV).\n\n\nscale_mean_x <-  mean(scale(dat$Height))\n\nscale_mean_x\n\n[1] 4.996004e-16\n\n2) Compute the mean (\\(\\hat{y}\\)) of the SCALED dependent variable (AV).\n\n\nscaled_mean_y <-  mean(scale(dat$Weight))\n\nscaled_mean_y\n\n[1] 4.066192e-16\n\n3) Calculate the covariance between scaled (standardized) AV and the scaled (standardized) UV using the formula:\n\\[cov(x,y) = \\frac{\\sum{(x_{i}-\\hat{x})\\times (y_{i}-\\hat{y})}}{n-1}\\]\n\n\ncov <-  sum((scale.default(dat$Height) - scale_mean_x) * (scale(dat$Weight) - scaled_mean_y))\ncov <-  cov/3\n\ncov\n\n[1] 0.6714691\n\n4) Calculate the variance of UV using the formula:\n\\[var(x) = \\frac{\\sum{(x_{i}-\\hat{x})^2}}{n-1}\\]\n\n\nvar_x <-  sum((scale(dat$Height) - scale_mean_x)^2)\n\nvar_x <-  var_x/3\n\nvar_x\n\n[1] 1.333333\n\n5) Calculate the standardized beta (\\(\\beta_{z}\\)):\n\n\nbeta_z <-  cov/var_x\nbeta_z\n\n[1] 0.5036018\n\n6) Compare your results with R-output:\n\n\nlm_z <-  lm(scale(dat$Weight) ~ scale(dat$Height))\n\nsummary(lm_z)\n\n\nCall:\nlm(formula = scale(dat$Weight) ~ scale(dat$Height))\n\nResiduals:\n      1       2       3       4       5 \n-0.7566 -0.2245 -0.6836  1.3371  0.3276 \nattr(,\"scaled:center\")\n[1] 74.6\nattr(,\"scaled:scale\")\n[1] 15.04\n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)\n(Intercept)       3.536e-17  4.461e-01    0.00    1.000\nscale(dat$Height) 5.036e-01  4.988e-01    1.01    0.387\n\nResidual standard error: 0.9976 on 3 degrees of freedom\nMultiple R-squared:  0.2536,    Adjusted R-squared:  0.00482 \nF-statistic: 1.019 on 1 and 3 DF,  p-value: 0.387\n\n7) Note that the \\(R^2\\) did not change either!\nWe continue with computing the confidence interval for the \\(\\alpha_{z}\\) and \\(\\beta_{z}\\)\n\n\nconfint(lm_z)\n\n                      2.5 %   97.5 %\n(Intercept)       -1.419799 1.419799\nscale(dat$Height) -1.083782 2.090986\n\n\n\n\nNow\n.\n.\n.\n\n\n\nWe know that since we have “collected” only 5 observations, this experiment might suffer from low power 🥵🦾… We even have a good reason to believe so because the relationship between height and weight may hold in reality…\nWe can determine the sample size we will need to achieve significant results\nBUT Careful! this method is no magic! 🪄 we will only be likely to obtain significant results only if there is, indeed, a relationship between height and weight)\nWe have to first calculate the\n\n\nrho_squared = coef(lm_z)[\"scale(dat$Height)\"]^2\n\nf_squared = rho_squared/(1-rho_squared)\n\nf_squared\n\nscale(dat$Height) \n        0.3397908 \n\nWe use the power calculation function pwr.f2.test(). It takes the following arguments: \\(u\\) which is the number of predictors we have in our model. In a simple linear regression we always ahve only 1, \\(f2\\) which is the effect size, f squared, which we calculated above. The other arguments are \\(sig.level\\) that determines the significance level (Type I error probability) and \\(power\\) represents the power we wish to achieve (1 minus Type II error probability)\n\n\nlibrary(pwr)\npwr.f2.test(u = 1, f2 = f_squared, sig.level = 0.005, power = 0.8)\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 41.23813\n             f2 = 0.3397908\n      sig.level = 0.005\n          power = 0.8\n\nInterpretation: Because 𝜈 = 𝑛 − 2 (we estimate 2 variables), we must add those 2 to know what the required sample (i.e., 𝑣 + 2). In our case, we will need 43 subjects (41. + ).\n\n\n\nThings\nto\nremember\nabout \\({R^2}\\)\n\n\n\nRanges between 0 and 1.\n1.1) A value closer to 1 indicates a stronger relationship between the independent (UV) and dependent (AV) variables, meaning that more of the variance in the dependent variable can be explained by the independent variable(s).\n1.2) Conversely, a value closer to 0 indicates a weaker relationship.\nThe estimated value \\(r^2\\) (which is the realised value of \\(R^2\\)) within the scope of the SLR (simple linear regression) is equivalent to the squared Pearson correlation.\n\\(r^2\\) is also referred to as the coefficient of determination.\nInterpretation of the standardized \\(\\beta_{z}\\): If the predictor variable (AV) increases by one standard deviation, the criterion variable (UV), on average, increases by \\(\\beta_{z}\\) standard deviations.\nBack to the summptions I mentioned at the beginning of this blog:\nLinearity. We will test this assumption with a simple scatter plot. To make the point here, I will create a new, richer data set.\n\n\n\nIn order to examine those assumptions, we will runa. linear regression (we will need it to exmain the distribution of the residuals).\n\n\nlibrary(gridExtra)\n\nlm_2 <-  lm(Weight ~ Height, data = dat_rich)\n \nsummary(lm_2)\n\n\nCall:\nlm(formula = Weight ~ Height, data = dat_rich)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.31926 -0.60302  0.02559  0.49829  2.40082 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  6.49342    7.58719   0.856    0.394    \nHeight       0.58495    0.07573   7.724 9.79e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8047 on 98 degrees of freedom\nMultiple R-squared:  0.3784,    Adjusted R-squared:  0.3721 \nF-statistic: 59.66 on 1 and 98 DF,  p-value: 9.793e-12\n\nLets plot all the figures we need to judge of the Linear regression assumptions are met.\n\n\nlibrary(ggplot2)\nlibrary(gridExtra)\nstand_res <-  as.data.frame(rstandard(lm_2))\n\n\np_1 <-  ggplot(data = dat_rich, aes(x = Height, y = Weight)) +\n  geom_point(col = \"lightblue\", size = 3) +\n  geom_smooth(method = \"lm\", ) +\n  ggtitle(\"Linearity Assumption\")+\n  theme_classic()\n\n\n\npredicted <-  as.data.frame(predict(lm_2, interval = \"prediction\", level = .95))\npredicted$stand_res <-  stand_res$`rstandard(lm_2)`\n\np_2 <-   ggplot(data = predicted, aes(fit, stand_res))+\n  geom_point(col = \"lightblue\")+\n  ggtitle(\"Homoskedasticity Assumption\") +\n  geom_hline(yintercept = 0) + \n  xlab(\"Predicted Value of DV\") +\n  ylab(\"Standardized Residuals\") +\n  theme_classic()\n\ncook_dat <-  as.data.frame(round(cooks.distance(lm_2), digits = 3))\ncook_dat$id <-  cbind(1:sample_size)\n\np_3 <-   ggplot(data = cook_dat, aes(x = id, y = `round(cooks.distance(lm_2), digits = 3)`))+\n  geom_point(col = \"lightblue\")+\n  ggtitle(\"Outliers Assumption\") +\n  geom_hline(yintercept = 0.04) + \n  xlab(\"Observation IDs\") +\n  ylab(\"Cooks Distances\") +\n  theme_classic()\n\n# Arrange the plots in a grid\ncombined_plots <- grid.arrange(p_1, p_2, p_3, layout_matrix = rbind(c(1, NA), c(2, 3)), \n                               heights = c(3,3), widths = c(5, 5))\n\n\n\nA quick reminder of the way Cooks Distance is computed:\nCook’s D(i) = \\(\\frac{{\\Delta \\hat{y_i}^2}}{{p}} \\times \\frac{{h_{ii}}}{{(1 - h_{ii})}}\\)\nAnd lastly, (for the statistics nurds among us….😉), here is the model with the standardized coefficients:\n\n\nscaled_dat_rich <-  as.data.frame(round(scale(dat_rich), digits = 2))\nlm_z_2 <-  lm(scaled_dat_rich$Weight ~scaled_dat_rich$Height)\n\nsummary(lm_z_2)\n\n\nCall:\nlm(formula = scaled_dat_rich$Weight ~ scaled_dat_rich$Height)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.28128 -0.59211  0.02353  0.48878  2.36279 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            0.0001846  0.0792326   0.002    0.998    \nscaled_dat_rich$Height 0.6152039  0.0796207   7.727 9.67e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7923 on 98 degrees of freedom\nMultiple R-squared:  0.3786,    Adjusted R-squared:  0.3722 \nF-statistic:  59.7 on 1 and 98 DF,  p-value: 9.669e-12\n\nconfint.lm(lm_z_2)\n\n                            2.5 %    97.5 %\n(Intercept)            -0.1570500 0.1574191\nscaled_dat_rich$Height  0.4571992 0.7732086\n\n\n\n\n",
      "last_modified": "2023-09-26T07:47:10+02:00"
    },
    {
      "path": "effectsize_mlr.html",
      "title": "Effectsize(s) and Collinearity in Multiple Linear Regression",
      "description": "Linear Regression\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nIn this blog we will examine the effect sizes in MLR and the correlation of predictors (a.k.a. multicollinearity)\nFirst, to set the stage, lets remind ourselves of the general model equation:\n\\[Y_{i} =𝛼+ \\beta_{1} \\times X_{i1} + \\beta_{2} \\times X_{i2} + ... + \\beta_{n} \\times X_{in} + \\epsilon_{i}  \\; \\;  \\; when \\; \\; \\;  \\epsilon \\sim 𝑁(0,\\sigma^2)\\]\n• \\(\\alpha\\): The predicted value, when all other predictors are qual 0.\n• \\(\\beta_{1}, \\beta_{2}, ..., \\beta_{k}\\): Slopes (Steigungsparameter, Regressionsgewichte), the expected change in the dependent variable (AV), when the predictor \\(X_{1}, X_{2} ... X_{k}\\) increase by one unit, while keeping all other variables contstant.\n• \\(\\epsilon_{i}\\) : Error term, describes the deviation of a randomly samples person \\(i\\) from their predicted value.\nIn multiple linear regression there are two types of effect sizes.\nThe effect sizes that describes the strength of the linear relationship between the all variables and the Dependent variable (AV). This effect size is called \\(\\rho^2\\) and its estimator is:\n\\[ \\hat{\\rho^2} = R^2 = \\frac{\\hat{\\sigma^2_{\\mu_{i}}}}{\\sigma^2_{total}} = \\frac{1/n \\times \\sum_{i=1}^{n} \\times (\\hat{Y_{i} - \\bar{Y}})^2}{1/n \\times \\sum_{i=1}^{n} \\times (Y_{i} - \\bar{Y})^2}\\]\nNote that the difference to simple linear regression, this effect size will refer to predicted value \\(\\hat{Y_{i}}\\) that are calculated based on MULTIPLE independent variables (mehrere UVs). That is the reason for its other known name multiple correlation, which is calculated as \\(\\sqrt{\\rho^2}\\).\nLets take an example:\nLet’s denote the dependent variable as \\(Y\\)(happiness with the statistics lecture) and the predictor variables as \\(X_1\\) (time spent learning) and \\(X_2\\) (level of subjective belief in math skills).\nOur model equation, is, therefore, \\[Y_{i} =𝛼+ \\beta_{time.prep} \\times X_{i1} + \\beta_{subj.belief} \\times X_{i2} + + \\epsilon_{i}  \\; \\;  \\; when \\; \\; \\;  \\epsilon \\sim 𝑁(0,\\sigma^2)\\]\nAnd here is this model’s R-Output:\n\n\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate sample data\nn <- 100  # Number of observations\ntime_spent <- runif(n, 0, 10)  # Time spent learning\nbelief_math <- runif(n, 1, 5)  # Subjective belief in math skills\nhappiness <- 2 + 0.5 * time_spent + 1.5 * belief_math + rnorm(n, 0, 1)  # Dependent variable (happiness)\n\n# Create a data frame\ndata <- data.frame(time_spent, belief_math, happiness)\n\n# Perform multiple linear regression\nmodel <- lm(happiness ~ time_spent + belief_math, data=data)\n\n# Print the regression summary\nsummary(model)\n\n\nCall:\nlm(formula = happiness ~ time_spent + belief_math, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8994 -0.6821 -0.1086  0.5749  3.3663 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.86662    0.35956   5.191 1.15e-06 ***\ntime_spent   0.50973    0.03457  14.746  < 2e-16 ***\nbelief_math  1.49259    0.09337  15.986  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9765 on 97 degrees of freedom\nMultiple R-squared:  0.8177,    Adjusted R-squared:  0.814 \nF-statistic: 217.6 on 2 and 97 DF,  p-value: < 2.2e-16\n\nNote the “Multiple R-squared” value, which is .8177.\nThis value is interpreted as the percentage of variance of “happiness” that can be explained by “time_spent” and by “belief_math”.\nAnd here is a plot of this data & model:\n\n\n\nThe effect sizes that describes the strength of the linear relationship between the every single variable and the Dependent variable (AV). A standardized effect size is called \\(\\beta_{zj}\\) and is similar to the \\(\\beta_{z}\\) discussed in the blog “Effectsize(s) in Simple Linear Regression”.\nTo calculate this effect size, one has to, first, z-Standardise all predictor variables (UVs) an the dependent variable (AV) and then calculate the model again.\n\n\nCall:\nlm(formula = scale.happiness. ~ scale.time_spent. + scale.belief_math., \n    data = data_scale)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.83899 -0.30131 -0.04796  0.25395  1.48692 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        1.465e-16  4.313e-02    0.00        1    \nscale.time_spent.  6.417e-01  4.351e-02   14.75   <2e-16 ***\nscale.belief_math. 6.956e-01  4.351e-02   15.99   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4313 on 97 degrees of freedom\nMultiple R-squared:  0.8177,    Adjusted R-squared:  0.814 \nF-statistic: 217.6 on 2 and 97 DF,  p-value: < 2.2e-16\n\nHow shoul I understand these values?\nIf a student will increase their time spent for studying by one standard deviation, holding their belief in math skills variable constant, their happiness with statistics class will increase on average by 0.6417 standard deviations\nMulticollinearity\nMulticollinearity refers to a situation in multiple linear regression where two or more predictor variables are highly correlated with each other.\nIt indicates a strong linear relationship between the predictor variables, which can cause issues in the regression analysis.\nIn the context of the example with the variables “time spent learning” and “belief in math skills” as predictor variables for happiness with the statistics lecture, multicollinearity would occur if these two variables are highly correlated. That is, for example, when those students, who have a strong belief in their math skills will also very likely to spend much time preparing for this lecture and vice versa for those students with less belief in their math skills.\nFor example, let’s consider a dataset with 100 observations. Here are the correlation coefficients between the predictor variables:\n\n            time_spent belief_math happiness\ntime_spent   1.0000000  -0.0872707 0.5809671\nbelief_math -0.0872707   1.0000000 0.6396235\nhappiness    0.5809671   0.6396235 1.0000000\n\nThe dangers in running a multiple (or any) linear regression without checking for multicoliniarity\nAffects the estimated standard errors for the \\(\\beta\\) (i.e., slopes, Steigungsparameter) negatively\nAs a result, multicollinearity also has a negative impact on the confidence intervals and hypothesis tests for the slope parameters:\n• Larger confidence intervals\n• Lower power of hypothesis tests\nThe parameter that are NOT affected by multicoliniarity are:\n• Confidence intervals for \\(\\rho^2\\)\n• Omnibustest for multiple linear regression\nWhat to do to examine if we have a multicoliniarity in our model?\nVariance Inflation Factor (VIF) - VIF helps us understand how much the variance (or uncertainty) of one predictor variable is increased due to its correlation with other predictor variables in the model.\nA high VIF value indicates a high degree of multicollinearity and suggests that the variable’s contribution to the regression model may be unreliable.\nThe formula to compute VIF is\n\\[VIF_{j} = \\frac{1}{1-\\color{red}{r_{j}^2}} \\; \\;  \\; when \\; \\; \\;  j =  time \\;spent, belief \\;math  \\; skills\\]\n\\[\\color{red}{r_{j}^2} = the \\; estimated \\; variance \\; explained \\; by \\; one \\; predictor \\; through \\; all \\; other \\; predictor \\; variables\\]\nIn other words, the variance inflation factor (VIF) of a predictor (e.g., time spent on studying) indicates **by what factor the variance of the estimated coefficient \\(\\hat{beta}_{j}\\) (e.g., \\(\\hat{beta}_{time \\; spent \\; studying}\\)) is larger relative to the other predictor .\n• The larger the VIF of a regression weight, the worse the estimator for that particular regression weight.\n\n\n\nImportant\nTo \nNote\n!!\n\n\n\nThe negative effects of collinearity can be mitigated by large sample sizes through decreasing the standard error (thereby increasing the precision of the estimated parameters) of the variables.\nHere is an example:\n\n\nlibrary(car)  # for the vif() function\n\n# Set the seed for reproducibility\nset.seed(123)\n\n# Generate a sample dataset with correlated predictor variables\nn_small <- 50  # Small sample size\nn_large <- 5000  # Large sample size\n\n# Generate correlated predictor variables\nx1 <- rnorm(n_large)\nx2 <- 0.8 * x1 + rnorm(n_large, sd = 0.2)\n\n# Generate the response variable\ny <- 2 * x1 + 3 * x2 + rnorm(n_large)\n\n# Function to calculate VIF for a given dataset and predictor variables\ncalculate_vif <- function(data, predictors) {\n  vif_values <- vif(lm(as.formula(paste(\"y ~\", paste(predictors, collapse = \"+\"))), data = data))\n  return(vif_values)\n}\n\n# Calculate VIF for the small sample size\npredictors <- c(\"x1\", \"x2\")\nvif_small <- calculate_vif(data.frame(x1[1:n_small], x2[1:n_small], y[1:n_small]), predictors)\ncat(\"VIF values (small sample size):\", vif_small, \"\\n\")\n\nVIF values (small sample size): 16.69219 16.69219 \n\n# Calculate VIF for the large sample size\nvif_large <- calculate_vif(data.frame(x1, x2, y), predictors)\ncat(\"VIF values (large sample size):\", vif_large, \"\\n\")\n\nVIF values (large sample size): 16.69219 16.69219 \n\n# Fit linear regression models with different sample sizes\nmodel_small <- lm(y[1:n_small] ~ x1[1:n_small] + x2[1:n_small])\nmodel_large <- lm(y ~ x1 + x2)\n\n# Calculate standard errors of the coefficients\nse_small <- summary(model_small)$coefficients[, \"Std. Error\"]\nse_large <- summary(model_large)$coefficients[, \"Std. Error\"]\n\n# Print the standard errors\ncat(\"Standard errors (small sample size):\", se_small, \"\\n\")\n\nStandard errors (small sample size): 0.1470152 0.6560189 0.7674668 \n\ncat(\"Standard errors (large sample size):\", se_large, \"\\n\")\n\nStandard errors (large sample size): 0.01415777 0.05816415 0.07060016 \n\nWith a larger sample size, the estimates of the regression coefficients tend to have lower standard errors, which means they become more precise. Consequently, the effects of multicollinearity on the significance and interpretation of the coefficients may become less pronounced.\n\n\n\n",
      "last_modified": "2023-09-26T07:47:15+02:00"
    },
    {
      "path": "index.html",
      "title": "Willkommen!",
      "author": [],
      "contents": "\n\n          \n          \n          LMU Statistik\n          \n          \n          Home\n          \n          \n          Statistik I\n           \n          ▾\n          \n          \n          Basic Concepts\n          Basics in R\n          Basics in R - Part 2\n          Basics in R - Part 3\n          Basics in R - Part 4\n          Statistical Tests - Distributions\n          Statistical Tests - Decision Diagramm\n          \n          \n          \n          \n          Statistik II\n           \n          ▾\n          \n          \n          Q & A - Summary\n          ANOVA - I\n          ANOVA - II\n          Simple Linear Models\n          Multiple Linear Models\n          Multiple Linear Models - Part II\n          Simple Linear Models: Effectsize\n          Multiple Linear Models: Effectsize\n          \n          \n          ☰\n          \n          \n      \n        \n          Willkommen!\n          \n            \n                \n                  \n                    Moodle\n                  \n                \n              \n                            \n                \n                  \n                    Tehilla’s Email\n                  \n                \n              \n                          \n        \n\n        \n          \n      \n      \n        \n          This website is dedicated ❤️ to all the AMAZING\n          Schulpsychologie students, who will sit/sat a\n          Statistik exam @LMU!\n          Enjoy your journey to understanding the topics we cover in\n          the courses Statistik I and Statistik II!\n          Im very much looking forward to hear both about what you\n          think about this website and about our seminar!\n          You can 📧 me at any time (see button above)\n        \n      \n    \n\n    \n      \n        \n          \n            \n              \n            \n              Willkommen!\n            \n            \n              \n                \n                                    \n                    \n                      Moodle\n                    \n                  \n                                    \n                    \n                      Tehilla’s Email\n                    \n                  \n                                  \n              \n            \n            \n              This website is dedicated ❤️ to all the AMAZING\n              Schulpsychologie students, who will sit/sat a\n              Statistik exam @LMU!\n              Enjoy your journey to understanding the topics we cover\n              in the courses Statistik I and Statistik II!\n              Im very much looking forward to hear both about what\n              you think about this website and about our seminar!\n              You can 📧 me at any time (see button above)\n            \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2023-09-26T07:47:19+02:00"
    },
    {
      "path": "lm_ii_ii.html",
      "title": "Multiple Linear Regression Analysis - Part II",
      "description": "Linear Regression\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nIntroduction:\nWhile the traditional understanding of linear regression assumes continuous independent variables, there are scenarios where the independent variables (UV) may be discrete in nature.\nIn this blog post we will explore the concept of multiple linear regression with discrete variables and understand how it can be applied to real-world problems.\nWhat are discrete variables?\nA discrete variable is a type of variable that takes on specific, distinct values with no intermediate values possible between them. These values are typically counted or categorized rather than measured on a continuous scale\nCan you give me some examples?\nGender (e.g., either Male 🧍🏼‍♂️ vs Female🧍🏾‍♀️)\nEthnicity (e.g., Caucasian, African American, and Asian)\nOccupation (e.g., “Engineer” 👩🏻‍💻 vs. “Teacher” 👨🏻‍🏫 vs. “Salesperson” 🧑🏽‍💼)\nIn case we are looking at the effect of such variables, you will need to modify the multiple linear regression model to account for them.\nCool, but how do I code the variables expressions?\nWell, depending on how many levels/groups there are, this may change.\nA case of a variable with two categorical expressions:\nIn the case of a discrete predictor with two categorical expressions (e.g., male vs. female), we first establish an arbitrary expression (e.g., male) as the reference category. This reference group will be coded with a 0. For example, if we decided that “male” is our reference group, we will insert 0 if the participant is a male and a 1 if they are identified as female.\nSo, we define a dummy variable \\(D_{i}\\) as follows:\n\\(D_{i} = 1\\), if the person is a female\nand\n\\(D_{i} = 0\\), if the person is a male\nA practical example:\nAnd here is an example of a data set, which includes information about a group of individuals, with each individual identified by a unique ID.\nID column: This variable represents the unique identifier assigned to each individual in the data set.\nGender Dummy column: This dummy coded variable indicates whether each person has the color red. It takes a value of 1 if the person has the color red and 0 if not.\nIQ column: This variable represents the IQ scores of individuals. It provides a measure of intellectual ability or cognitive capacity, with higher values indicating higher IQ scores.\n**Monthly Salary _Score column**: This variable represents the scores on Monthly Salary for each individual. It reflects the subjective assessment of Monthly Salary , with higher values indicating higher levels of self-reported Monthly Salary .\n\n   ID Gender_dummy  IQ Salary_month\n1   1            1 120         8000\n2   2            0 110         7000\n3   3            1 125         9000\n4   4            0 105         6000\n5   5            0  95         5000\n6   6            1 115         4000\n7   7            0 100         2500\n8   8            0 130         9000\n9   9            1 125         3000\n10 10            0 105         2000\n\nHow does the linear model look like with a discrete variable with two expressions?\nThe general equation is:\n\\[Y_{i} = \\alpha + \\beta_{1} \\times D_{1i} +... + \\beta_{2} \\times D_{2i} +... +\\beta_{k-1} \\times D_{k-1}  + \\epsilon_{i} \\]\nFor our specific example, therefore, it will be:\n1) For the reference group (we decided on “male”), the model will be:\n\\[Y_{i} = \\alpha + \\beta \\times 0 + \\epsilon_{i}\\]\n\\[ = \\alpha + \\epsilon_{i}\\]\nIt follows that \\(\\alpha\\) is the predicted value for this group.\n2) For the second group (will be “female”), the model will be:\n\\[Y_{i} = \\alpha + \\beta \\times 1 + \\epsilon_{i}\\]\n\\[ = \\alpha + \\beta +\\epsilon_{i}\\]\nIt follows that \\(\\alpha + \\beta\\) is the predicted value for this group.\nIt also implies that:\nIf \\(𝛽 < 0\\), then the predicted value for individuals in the reference category is greater than in the other category.\nIf \\(𝛽> 0\\) , then the predicted value for individuals in the reference category is smaller than in the other category.\nIf \\(𝛽 = 0\\), then the predicted values for both categories are equal.\nLets run a model to learn about the estimated varibales and their interpretation:\n\n\nmodel_2 <- lm(Salary_month ~ Gender_dummy, data = data_2)\n  \nsummary(lm(Salary_month ~ Gender_dummy, data = data_2))\n\n\nCall:\nlm(formula = Salary_month ~ Gender_dummy, data = data_2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -3250  -2562    250   1938   3750 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)   \n(Intercept)      5250       1135   4.624   0.0017 **\nGender_dummy      750       1795   0.418   0.6871   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2781 on 8 degrees of freedom\nMultiple R-squared:  0.02135,   Adjusted R-squared:  -0.101 \nF-statistic: 0.1745 on 1 and 8 DF,  p-value: 0.6871\n\nconfint(model_2)\n\n                 2.5 %   97.5 %\n(Intercept)   2631.835 7868.165\nGender_dummy -3389.683 4889.683\n\nWe can see the results and learn that:\n\\(\\alpha\\)(intercept) = 5,250.\n\\(\\alpha\\) represents the predicted Monthly Salary value for male participants.\nThe confidence interval for the intercept is [2631.835, 7868.165], which does not include the 0, indicating, alongside \\(p-value = .0017\\) that there our predicted value for male participants is non-zero and that the plausible monthly salary for males lies between 2631.835 and 7868.165.\nGender_dummy (\\(\\beta_{gender}\\)) = 750.\nThis means that the predicted Monthly Salary value for females participants is (\\(\\alpha + \\beta_{gender}\\)) 5250 + (1 * 750) = 6,000.\n\\(\\beta_{gender}\\) represents the difference in the predicted monthly salary fro females and males (the reference group).\nThe confidence interval for the slope is [-3389.683, 4889.683], which does include the 0, indicating, alongside \\(p-value = .687\\) that there our predicted value for female participants could potentially be zero (which is inline with \\(H_{0}\\)) and that the plausible monthly salary for males lies between -3389.683 and 4889.683.\nA case of a variable with MORE THAN two categorical expressions:\nA practical example:\nLet’s consider an example of a discrete variable with three categories: “Education Level”.\nWe’ll define and code the categories as - “High School”\n- “Bachelor’s Degree”\n- “Master’s Degree.”\nWhen we create dummy variables to represent these categories, we assign a value of 1 if an individual belongs to that category and a value of 0 if they do not.\nIn the resulting data frame below, if a row has a 0 in both columns “Dummy_HS” and “Dummy_Bachelor” columns, it means that the person does not have a High School education nor a Bachelor’s Degree. Essentially, they are not in either of those categories. Instead, they may have a different education level, such as a Master’s Degree or some other qualification not represented in the dummy variables.\nTherefore, these rows with two zeros (as on rows 3, 6 and 8) indicate individuals who do not fall into the specified categories and should be understood as having a different education level or belonging to an unrepresented category (which is in this case the “high schools degree”).\n\n   ID   Education_Level  IQ Salary_month Dummy_HS Dummy_Bachelor\n1   1 Bachelor's Degree 120         8000        1              0\n2   2       High School 110         7000        0              0\n3   3   Master's Degree 125         9000        0              1\n4   4       High School 105         6000        0              0\n5   5 Bachelor's Degree  95         5000        1              0\n6   6   Master's Degree 115         4000        0              1\n7   7       High School 100         2500        0              0\n8   8   Master's Degree 130         9000        0              1\n9   9 Bachelor's Degree 125         3000        1              0\n10 10       High School 105         2000        0              0\n\nFor our specific example, therefore, the equations will be:\nFor the reference group (we decided on “high school”), the model will be:\n\\[Y_{i} = \\alpha + \\beta_{bachelor} \\times 0 + \\beta_{masters} \\times 0 + \\epsilon_{i} \\]\n\\[ = \\alpha + \\epsilon_{i}\\]\nIt follows that \\(\\alpha\\) is the predicted value for this group.\nFor the second group (will be “Bachelor’s Degree”), the model will be:\n\\[Y_{i} = \\alpha + \\beta_{bachelor} \\times 1 + \\beta_{masters} \\times 0 + \\epsilon_{i}\\]\n\\[ = \\alpha + \\beta_{bachelor} + \\epsilon_{i}\\]\nIt follows that \\(\\alpha + \\beta_{bachelor}\\) is the predicted value for this group.\nFor the third group (will be “Master’s Degree”), the model will be:\n\\[Y_{i} = \\alpha + \\beta_{bachelor} \\times 0 + \\beta_{masters} \\times 1 + \\epsilon_{i}\\]\n\\[ = \\alpha + \\beta_{masters} + \\epsilon_{i}\\]\nIt follows that \\(\\alpha + \\beta_{masters}\\) is the predicted value for this group.\nLets run a model to learn about the estimated varibales and their interpretation:\n\n[1] \"High School\"       \"Bachelor's Degree\" \"Master's Degree\"  \n\nCall:\nlm(formula = Salary_month ~ Education_Level, data = data_3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3333.3 -2218.8   645.8  1666.7  2666.7 \n\nCoefficients:\n                                 Estimate Std. Error t value Pr(>|t|)\n(Intercept)                        4375.0     1309.6   3.341   0.0124\nEducation_LevelBachelor's Degree    958.3     2000.4   0.479   0.6465\nEducation_LevelMaster's Degree     2958.3     2000.4   1.479   0.1827\n                                  \n(Intercept)                      *\nEducation_LevelBachelor's Degree  \nEducation_LevelMaster's Degree    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2619 on 7 degrees of freedom\nMultiple R-squared:  0.2405,    Adjusted R-squared:  0.02347 \nF-statistic: 1.108 on 2 and 7 DF,  p-value: 0.3819\n                                     2.5 %   97.5 %\n(Intercept)                       1278.308 7471.692\nEducation_LevelBachelor's Degree -3771.941 5688.608\nEducation_LevelMaster's Degree   -1771.941 7688.608\n\n\\(\\alpha\\)(intercept) = 4,375.\n\\(\\alpha\\) represents the predicted Monthly Salary value for participants that have graduated Gymnasium.\nThe confidence interval for the intercept is [1278.308, 7471.692], which does not include the 0, indicating, alongside \\(p-value = .0124\\) that there our predicted value for participants, who graduated Gymnasium is non-zero and that the plausible monthly salary for those lies between 1278.308 and 7471.692.\nBachelor’s degree (\\(\\beta_{BA}\\)) = 958.3.\nThis means that the predicted Monthly Salary value for participants, who have a BA degree is (\\(\\alpha + \\beta_{BA}\\)) 4,375 + (1 * 958.3) = 5,333.3.\n\\(\\beta_{BA}\\) represents the difference in the predicted monthly salary for participants, who have BA degree and those, who have graduated Gymnasium (the reference group).\nThe confidence interval for this slope is [-3771.941, 5688.608], which does include the 0, indicating, alongside \\(p-value = .6465\\) that BA graduates do not differ in their salaries from those, who have a BA degree. Since this range includes a zero (which is inline with \\(H_{0}\\)) and the plausible monthly salary for BA graduates lies between -3389.683 and 4889.683, we can conclude that we will accept the \\(H_{0}\\).\nBachelor’s degree (\\(\\beta_{MA}\\)) = 2958.3.\nThis means that the predicted Monthly Salary value for participants, who have a MA degree is (\\(\\alpha + \\beta_{MA}\\)) 4,375 + (1 * 2958.3) = 7,333.3.\n\\(\\beta_{MA}\\) represents the difference in the predicted monthly salary for participants, who have MA degree and those, who have graduated Gymnasium (the reference group).\nThe confidence interval for this slope is [-1771.941, 7688.608], which does include the 0, indicating, alongside \\(p-value = .1827\\) that MA graduates do not differ in their salaries from those, who have a MA degree. Since this range includes a zero (which is inline with \\(H_{0}\\)) and the plausible monthly salary for MA graduates lies between -1771.941 and 7688.608, we can conclude that we will accept the \\(H_{0}\\).\nWhat about a model that combines both discrete and continous variables?\nGreat question!\nThe following statistical model or analysis has two types of predictors:\none predictor is discrete, meaning it has two possible manifestations or categories.\na second predictor is continuous, meaning it takes on a range of numerical values.\nWe can use the dataset we have create before and add the IQ scores to predicts participant’s hapinness.\nThe full model in this case will be:\n\\[Y_{i} = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{education} \\times D_{i} + \\epsilon_{i} \\]\nThe specifications of all the other models are:\n1) For the reference group, which has a high school education, the regression model will be:\n\\[Y_{i} = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{education} \\times 0 + \\epsilon_{i} \\]\nWhich implies that the predicted value of this group’s Monthly Salary is\n\\[ = \\alpha + \\beta_{IQ} \\times X_{i}\\]\n2) For the second group, which has a becholar’s degree , the regression model will be:\n\\[Y_{i} = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{becholar} \\times 1 + \\epsilon_{i} \\]\nWhich implies that the predicted value of this group’s Monthly Salary is\n\\[ = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{becholar} \\times 1\\]\n3) For the third group, which has a masters’s degree , the regression model will be:\n\\[Y_{i} = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{masters} \\times 1 + \\epsilon_{i} \\]\nWhich implies that the predicted value of this group’s Monthly Salary is\n\\[ = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{masters} \\times 1\\]\nThe last question for today is: 🥁\nWhat about a good-old interaction effect between two independent variable (UVs)?\nLets revert back to our first above, including an interaction effect in a a linear regression model helps us to explore how the relationship between “gender” (reminder: 2 levels - male vs. female) and “Monthly Salary” changes, depending on the gender.\nMore specifically, in our data set we have gender and IQ as potential predictors. By including an interaction term, we can assess whether the relationship between gender and Monthly Salary differs for individuals with different IQ levels. It could show us, for example, that gender has a stronger positive effect on Monthly Salary for individuals with avergae IQ scores compared to those with lower/higher IQ scores.\nOverall, the interaction term helps us understand if the relationship between gender and Monthly Salary is dependent on an individual’s IQ level and if the two factors interact in influencing Monthly Salary .\nLet us define the general model equation:\n\\[Y_{i} = \\alpha + \\beta_{gender} \\times D_{i} + \\beta_{IQ} \\times X_{i} + \\beta_{interaction} \\times (X_{i} \\times D_{i})+ \\epsilon_{i} \\]\n1) For the reference group, where \\(D_{i} = 0\\), the model will be:\n\\[Y_{i} = \\alpha + \\beta_{gender} \\times D_{i} + \\beta_{IQ} \\times X_{i} + \\beta_{interaction} \\times (X_{i} \\times D_{i})+ \\epsilon_{i} \\]\n\\[ = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{gender} \\times 0 + \\beta_{interaction}(X_{i} \\times 0) \\]\n\\[ = \\alpha + \\beta_{IQ} \\times X_{i} + \\epsilon_{i}\\]\nWhich implies that the predicted Monthly Salary value for the reference group is:\n\\[= \\alpha + \\beta_{IQ} \\times X_{i}\\]\n2) For the second group, where \\(D_{i} = 1\\), the model will be:\n\\[Y_{i} = \\alpha + \\beta_{gender} \\times D_{i} + \\beta_{IQ} \\times X_{i} + \\beta_{interaction} \\times (X_{i} \\times D_{i})+ \\epsilon_{i} \\]\n\\[ = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{gender} \\times 1 + \\beta_{interaction}(X_{i} \\times 1) \\]\n\\[ = (\\alpha + \\beta_{gender}) + (\\beta_{IQ} + \\beta_{interaction}) \\times X_{i} + \\epsilon_{i}\\]\nWhich implies that the predicted Monthly Salary value for the second group is:\n\\[ = (\\alpha + \\beta_{gender}) + (\\beta_{IQ} + \\beta_{interaction}) \\times X_{i}\\]\nLets run a model to learn about the estimated varibales and their interpretation:\n\n\nCall:\nlm(formula = Salary_month ~ Gender_dummy + IQ + (Gender_dummy * \n    IQ), data = data_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3545.5 -1429.5   639.8  1658.9  2454.5 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)\n(Intercept)     -12059.32   10440.53  -1.155    0.292\nGender_dummy       422.96   39782.57   0.011    0.992\nIQ                 161.02      96.61   1.667    0.147\nGender_dummy:IQ    -15.56     330.84  -0.047    0.964\n\nResidual standard error: 2624 on 6 degrees of freedom\nMultiple R-squared:  0.3468,    Adjusted R-squared:  0.02017 \nF-statistic: 1.062 on 3 and 6 DF,  p-value: 0.4323\n                       2.5 %     97.5 %\n(Intercept)     -37606.38747 13487.7434\nGender_dummy    -96921.48242 97767.3992\nIQ                 -75.37631   397.4102\nGender_dummy:IQ   -825.09363   793.9688\n\nThe interpretation of the parameters is as follows:\n• \\(\\alpha\\): This is the predicted value of \\(Y_i\\) for people in the reference category. In the current category, it represents the predicted salary for male participants, when IQ equal 0…\n• \\(\\alpha + \\beta_{gender}\\): This is the intercept in the female category and it represents the predicted value for female participants, when their IQ equals 0….\n• \\(\\beta_{gender}\\): This is the slope parameter for gender and it represents the expected increase in Monthly Salary for a male participant when IQ is increased by 1 point.\n• \\(\\beta_{IQ}\\): This is the slope parameter for IQ and it represents the expected difference in Monthly Salary between male and female participants when their IQ = 0.\n• \\(\\beta_{interaction}\\): This is the difference in slope parameters between the reference category (male) and other category (female). It reflects the difference in the “contribution” of IQ for male vs. female participants on their Monthly Salary.\n\n\n\n",
      "last_modified": "2023-09-26T07:47:20+02:00"
    },
    {
      "path": "lm_ii.html",
      "title": "Multiple Linear Regression Analysis",
      "description": "Linear Regression\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nLinear models can be extended to include more than one independent variable, and the equation becomes:\n\\(\\color{red}{Y{i}} = \\color{blue}{{\\alpha}} + \\color{green}{{\\beta_{1} \\times X_{1} + \\beta_{2} \\times X_{2} + … + \\beta_{n} \\times X_{n}} + \\color{orange}{{\\epsilon_{i}}}}\\)\nWhere \\(\\color{red}{Y_{i}}\\) is the dependent variable (what we aim to predict).\n\\(\\color{blue}\\alpha\\) is the intercept (the point at which the regression line crosses the y axis).\n\\(\\color{green}{{X_{1}, X_{2}, X_{n}}}\\) are the independent variables (what we measure).\n\\(\\color{green}{{\\beta_{1}, \\beta_{2}, ..., \\beta_{n}}}\\) are the slopes of the respective independent variables (tells us how important the respective variable is).\nSince we also are interested in the distribution of the residuals, we also estimate \\(\\color{orange}{{\\sigma^2}}\\), which represents the standard deviation of the error term \\(\\color{orange}\\epsilon\\).\nIn MLR (Multiple Linear Regression) analysis we ask the following questions:\n## 1) How much variance, relative to the total variability of the criterion, can all predictors explain together?\n## 2) Which predictor has the largest predictive contribution?\n## 3) What is the magnitude of the independent predictive contribution of a predictor?\n## 4) Does the strength, direction, and interpretation of the effect of a predictor change when considering another predictor compared to the multiple regression?\nWe are going to use a fun(ny) example for this topic.\nIn a parallel universe, in which Psychology students party during the semester and prior to their exams, a study was conducted…\n\nThe researchers wanted to know if the amount of alcohol 🍺 consumed consumed a day prior to an exam but also the number of hours spent on preparation 📚📖 affected the student’s exam results 🥇.\nAnd so they asked students to confess about the amount of ml of alcohol (0 to ∞😂) they had the day before the exam and the number hours (0-100) they spent studying and how much they scored in the exam (0-💯%).\nModel Equation:\nTherefore, the model equation they were about to test is:\n\\(\\color{red}{Test.Score_{i}} = \\color{blue}{{\\alpha}} + \\color{green}{{\\beta_{alcohol_ml} \\times X_{1} + \\beta_{Study.hours} \\times X_{2}} + \\color{orange}{{\\epsilon_{i}}}}\\)\nHere is the data they obtained:\n\n    Test_Score Drink_ml Hours_Studied\n1         99.9     65.1          66.2\n2        100.0     65.6          66.6\n3         99.9     63.8          64.1\n4        100.0     66.1          65.6\n5        100.0     65.1          64.6\n6        100.0     65.4          63.8\n7        100.0     64.5          64.1\n8         99.3     63.8          64.0\n9         99.2     63.5          65.1\n10       100.0     64.4          65.9\n11       100.0     65.1          65.9\n12       100.0     66.2          66.1\n13       100.0     65.0          64.9\n14        99.4     64.2          64.5\n15        99.8     67.0          66.4\n16        98.7     64.2          64.1\n17       100.0     65.2          65.4\n18       100.0     65.3          66.2\n19        99.8     65.9          63.9\n20       100.0     65.3          65.4\n21       100.0     64.6          64.7\n22        98.8     63.8          64.7\n23       100.0     65.4          66.0\n24       100.0     64.5          63.3\n25       100.0     64.4          64.5\n26       100.0     64.7          64.1\n27       100.0     64.9          65.0\n28       100.0     65.0          65.6\n29       100.0     65.6          66.0\n30       100.0     65.2          66.1\n31       100.0     65.9          65.7\n32        99.6     65.9          64.7\n33       100.0     67.0          65.8\n34        98.9     64.1          64.7\n35        99.4     64.3          64.0\n36        99.3     63.8          63.2\n37        99.7     65.6          64.6\n38       100.0     63.7          64.7\n39       100.0     65.5          65.3\n40        98.9     63.7          63.6\n41        99.7     64.0          64.3\n42       100.0     66.4          64.9\n43       100.0     64.9          64.2\n44       100.0     66.4          65.8\n45        99.3     64.0          63.0\n46       100.0     64.2          65.6\n47       100.0     64.4          65.8\n48        98.9     63.1          63.1\n49        98.0     63.6          63.1\n50       100.0     66.8          64.6\n51       100.0     66.0          65.7\n52        99.0     64.5          64.4\n53       100.0     65.1          65.9\n54        99.7     65.6          64.2\n55        99.6     67.5          65.2\n56       100.0     65.5          64.8\n57       100.0     65.4          65.1\n58       100.0     65.6          67.0\n59        99.0     63.8          64.5\n60       100.0     65.4          64.8\n61        99.3     65.8          64.7\n62        99.1     64.5          64.5\n63       100.0     65.6          66.0\n64        99.5     65.0          64.6\n65        99.5     63.8          63.6\n66       100.0     65.8          64.6\n67        99.7     64.4          66.5\n68       100.0     65.3          65.8\n69        99.3     64.2          64.0\n70        99.9     66.3          65.3\n71       100.0     65.6          66.2\n72        99.7     64.4          66.1\n73        99.0     63.9          63.5\n74        98.8     64.3          65.0\n75       100.0     65.9          65.2\n76        99.7     64.2          64.2\n77       100.0     64.9          66.8\n78        99.8     65.0          64.4\n79        99.0     63.6          64.3\n80       100.0     65.2          64.5\n81        99.6     64.7          66.0\n82        99.4     65.1          66.4\n83        99.3     63.5          64.0\n84       100.0     64.6          64.5\n85       100.0     65.4          66.4\n86       100.0     66.1          65.9\n87        99.3     63.4          64.1\n88        99.9     65.3          66.1\n89       100.0     67.0          66.0\n90        98.3     64.2          64.5\n91       100.0     66.3          66.1\n92        98.8     65.3          64.4\n93       100.0     65.2          65.5\n94        99.8     63.7          65.2\n95       100.0     64.9          65.1\n96       100.0     65.7          66.6\n97       100.0     65.4          66.1\n98       100.0     66.2          65.1\n99       100.0     66.2          67.3\n100       99.1     65.0          65.8\n\nIn this parallel universe the rules are often different to ours but plotting your data is just like in our universe, simply a must!\nAnd so they did!\n\n\n\n\nThey did not forget to test the MLR assumptions before making public statements about their results…\n1) They started off with the LINEARITY assumption. To examine this assumption they plotted the residuals of each UV (alcohol in ml and the number of hours spent on studying)\n1.1) First they ran the linear model to be able to tell something about the distribution of the residuals. \n\n\nmod <-  lm(Test_Score ~ Drink_ml + Hours_Studied, data = dat)\nsummary(mod)\n\n\nCall:\nlm(formula = Test_Score ~ Drink_ml + Hours_Studied, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.16667 -0.13925  0.04314  0.22749  0.64136 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   78.59565    2.81839  27.887  < 2e-16 ***\nDrink_ml       0.18755    0.04572   4.102 8.52e-05 ***\nHours_Studied  0.13691    0.04417   3.099  0.00254 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3593 on 97 degrees of freedom\nMultiple R-squared:  0.3667,    Adjusted R-squared:  0.3537 \nF-statistic: 28.09 on 2 and 97 DF,  p-value: 2.38e-10\n\n\n\nlibrary(car)\ncrPlots(mod, ylab = \"residuals\")\n\n\n\nThe blue line represents a linear trend.\nThe pink line represents a flexible function that describes the data as closely as possible.\nWhen both lines are close to each other, the linearity assumption can be considered fulfilled (in our example, none of the variables win this competition).\nAgain,\nThe blue dashed line shows the expected residuals if the relationship between the predictor and response variable was linear.\nThe pink line shows the actual residuals.\nIf the two lines are significantly different, then this is evidence of a nonlinear relationship.\n2) The next assumption they tested was the NORMALITY. They plotted a histogram of the residuals. Before plotting these, they remembered (of course) to standardize them.\n\n\nlibrary(ggplot2)\nstand_residuals <-  data.frame(\n                    \"stand_resid\" = c(rstandard(mod)))\n\n# Create a histogram with density line using ggplot2\nggplot(stand_residuals, aes(x = stand_resid)) +\n  geom_histogram(fill = \"lightblue\", color = \"black\", bins = 15) +\n  ylim(0, 10) +\n  labs(x = \"residuals (standardized)\", y = \"Frequency\", \n       title = \"Histogram to Examine Homoskedasticity\") +\n  theme_classic()\n\n\n\nThis plot did not make them super happy either….\n3) lastly, they examined the presence of outliers:\n\n\ncooks_dist <-cooks.distance(model = mod)\n\nplot(cooks_dist, col=\"lightblue\", pch=19, cex=2)\n#add labels to every point\ntext(x = cooks_dist[1:100], labels=c(1:100))\nabline(h=4/sample_size)\n\n\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2023-09-26T07:47:22+02:00"
    },
    {
      "path": "lm.html",
      "title": "Simple Linear Regression Analysis",
      "description": "Linear Regression\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nWelcome to the post about linear models in statistics!\nLinear models are an essential tool in statistics used to model the relationship between a dependent variable and one or more independent variables.\nIn this post, we’ll explore linear models in more detail and introduce an interactive Shiny app plot that will help you visualize the concepts.\nBefore we dive into the interactive part of this post (in which you get to explorer different regression lines using different variables), let’s first define what a linear model is.\nA linear model is a mathematical equation that represents a linear relationship between two or more variables.\nThe simplest form of a linear model is a straight line equation of the form:\n\\(Y_{i} = \\alpha_{0} + \\beta_{1} \\times X_{1}\\)\nWhere \\(Y_{i}\\) is the dependent variable and represent the expected value of all \\(y_{i}\\) (all single data points) given a specific value of \\(X_{i}\\)\nWe are going to use a very simple example. We will try to fit a model that aims to predict the relationship between individuals height and weight.\nhere is a fun illustration of the simple model:\nLet us look at a simple example with little number of data points (just so we can get the feeling of whats going on under the hood)\n\n  Height Weight\n1    170     60\n2    180     75\n3    167     59\n4    165     88\n5    191     91\n\nHere are the averages of both the weights and heights. We will need those to calculate the intercpet and the slope of the model.\n\n[1]  74.6 174.6\n\nRemember how I told you that I am a fan of plotting the data?\nWe will use a scatterplot this time.\n\n\nlibrary(ggplot2)\nggplot(data = dat, aes(y = Weight,x = Height)) +\n  geom_point(alpha = .7, color = \"red\") +\n  geom_hline(yintercept = mean_weight, color=\"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = mean_height, color = \"green\", linetype = \"dashed\") + \n  #geom_smooth(method = \"lm\") +\n  theme_classic() \n\n\n\nHow is the \\(\\beta\\) calculated?\nA quick reminder: β represents how much the regression line will rise or fall.\nWe interpret the \\(\\beta\\) as the average increase in the Dependent variable (AV) when we increase the independent variable (UV) by one unit. For Example, if we increase the Height by 1cm, the average predicted increase of weight is …..\n\\[\\frac{\\sum_{i=1}^{n}(x_{i} -\\hat{x})\\times(y_{i}-\\hat{y})}{\\sum_{i=1}^{n}(x_{1} - \\hat{x})^2}\\]\nWhich can be also written as:\n\\[\\frac{cov(X,Y)}{S_{x}^2}\\]\nLets start!\n\n  Height Weight xi - mean(x) yi - mean(y) sqr(xi - mean(x)\n1    170     60         -4.6        -14.6            21.16\n2    180     75          5.4          0.4            29.16\n3    167     59         -7.6        -15.6            57.76\n4    165     88         -9.6         13.4            92.16\n5    191     91         16.4         16.4           268.96\n  (xi - mean(x)) X (yi - mean(y))\n1                           67.16\n2                            2.16\n3                          118.56\n4                         -128.64\n5                          268.96\n\nNow we have everything to compute the \\(cov(X, Y)\\) and the \\(S_{X}^2\\)\nFor \\(cov(X, Y)\\) we just need to compute the sum of the 6th column\n\n\nsum(dat[,\"(xi - mean(x)) X (yi - mean(y))\"])\n\n[1] 328.2\n\nFor \\(S_{x}^2\\) we just need to compute the sum of the 5th column\n\n\nsum(dat[, \"sqr(xi - mean(x)\"])\n\n[1] 469.2\n\nThe slope, is therefore, 0.6994885\n\n[1] 0.6994885\n\n\\(\\beta = \\frac{328.2}{469.2} = .69\\)\nThis means that, for every 1cm increase in height we expect to see an increase of .69KG.\nHow does the \\(\\alpha\\) calculated?\n\\[\\overline{y} = \\alpha_{0} + .69\\times \\overline{x}\\]\nWe know both means of \\(x\\) and \\(y\\) (from the calculation above)\nIf we rearrange the equation to solve for \\(\\alpha_{0}\\), we get\n\\[-\\alpha_{0} = -\\overline{y} + .69\\times \\overline{x}\\]\nLets rearrange the equation such that it will look nicer…\n\\[\\alpha_{0} = \\overline{y} - .69\\times \\overline{x}\\]\n\n\nalpha_0 = mean_weight - .6994885 * (mean_height)\nalpha_0\n\n[1] -47.53069\n\nOK, enough with the hard, tiring work of calculating everything by hand…. for exactly this reason we have R (😃).\nWe will use the function lm(), which which for Linear Model. We will wrap it with the function summary(), which provides us with the result summary of our model’s results.\n\n\nsum_lm <-  summary(lm(dat$Weight ~ dat$Height))\nsum_lm\n\n\nCall:\nlm(formula = dat$Weight ~ dat$Height)\n\nResiduals:\n      1       2       3       4       5 \n-11.382  -3.377 -10.284  20.115   4.928 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -47.5307   121.1507  -0.392    0.721\ndat$Height    0.6995     0.6928   1.010    0.387\n\nResidual standard error: 15.01 on 3 degrees of freedom\nMultiple R-squared:  0.2536,    Adjusted R-squared:  0.00482 \nF-statistic: 1.019 on 1 and 3 DF,  p-value: 0.387\n\nAccording to the model output, the intercept and the slope are not significant.\nAccording to the hypotheses of this mode, the intercept is compared against 0. That is, does -47.5307 is significantly different to 0. Looking at the estimated value itself it may seem odd that this value is not significant differernt to 0. To resolve this mysotory lets take a llok at the confidence intervals of the two:\n\n\nconfint.lm(lm(dat$Weight ~ dat$Height))\n\n                  2.5 %     97.5 %\n(Intercept) -433.086166 338.024785\ndat$Height    -1.505342   2.904319\n\nHow is the cofidence interval calculated?\nFor example: 95% C.I. for \\(\\beta_{1}\\): \\[b_{1} ± t_{1-α/2, n-2} * se(b_{1})\\]\nFirst we need to find the t value that “sits” at the lower and upper 2.5% of the t-distribution. We will use the function qt() and will provide us with the t value at 2.5% from a t-ditribution with 3 df.\n\n\nt_value <-  qt(p = .975, df =3, lower.tail = TRUE)\n\n\nWe now have all the unknowns to arrive at the solution.\nLets plug the numbers in and compare it to the 95% confidence interval we obtained above.\n\n\nupper_CI <-  0.6995 + (t_value * 0.6928) \nlower_CI <-  0.6995 - (t_value * 0.6928) \n\nupper_CI\n\n[1] 2.904299\n\nlower_CI\n\n[1] -1.505299\n\nLooking good!\nLets add this line to our scatterplot from before.\n\n\nggplot(data = dat, aes(y = Weight, x = Height)) +\n  geom_point(alpha = .7, color = \"red\") +\n  geom_hline(yintercept = mean_weight, color=\"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = mean_height, color = \"green\", linetype = \"dashed\") + \n  geom_smooth(method = \"lm\") +\n  theme_classic() \n\n\n\nLastly, here is how the standard error term is calculated. This term tells us the distance between the data points with respect to their y values and the best fitting line.\n\\[\\sigma^2 = S^2 = \\frac{\\sum_{i=1}^n(Y_{i} - \\hat{Y})}{n-2}\\]\nWe need the predicted value for each person in our dataset. We will use the predict.lm() function that will output the predicted values fro every person based on the model lm(weight ~ Height.\n\n\nlm <-  lm(dat$Weight ~ dat$Height)\ndat[,\"predicted_y\"] <-  predict.lm(lm)\n\ndat[,\"predicted_y\"] \n\n[1] 71.38235 78.37724 69.28389 67.88491 86.07161\n\ndat[, \"sqr(y-perd(y))\"] <- (dat$Weight - dat$predicted_y)**2\n\ndat[, \"sqr(y-perd(y))\"]\n\n[1] 129.55796  11.40574 105.75834 404.61683  24.28902\n\nsum <- sum(dat[, \"sqr(y-perd(y))\"])\n\nsqrt(sum/3)\n\n[1] 15.00697\n\nHere is how the t-distribution under the \\(H_{0}\\) looks like:\n\n\nlibrary(latex2exp)\nset.seed(123)\nx <- rt(1000, df = 3)\nx <-  round(x, digit = 3)\ncuts <-  quantile(x , c(0.000, .05, .95, .99999)) \n\n# Create data\nmy_variable = x\n \n# Calculate histogram, but do not draw it\nmy_hist = hist(x , breaks = 190  , plot = F)\n\n \n# Color vector\nmy_color= ifelse(my_hist$breaks <= 2.273526, \"lightgrey\",\n          ifelse(my_hist$breaks >= -2.273526, \"red\", rgb(0.2,0.2,0.2,0.2)))\n \n# Final plot\nplot(my_hist, \n     col = my_color, \n     border = F,\n     freq = FALSE, \n     main = (TeX('Histogram for a $\\\\t$-distribution with 3 degrees of freedom (df)')),\n     xlab = \"possible t values\", \n     xlim = c(-6,6), \n     ylim = c(0,.7), \n    cex.main=0.9)\ncurve(dt(x, df = 3), from = -5, to = 5, n = 500, col = 'red', lwd = 1, add = T)\nabline(v = -0.392, col = \"darkgreen\", lwd = 2,  lty = 'dashed')\nabline(v = 1.010, col = \"blue\", lwd = 2,  lty = 'dashed')\nabline(v = 2.273526, col = \"red\", lwd = 3)\ntext(x = 2.2, y = .7, TeX('t value for $\\\\alpha$'), col = \"blue\")\ntext(x = -1.6, y = .35,  TeX('t value for $\\\\beta$'),  col = \"darkgreen\")\ntext(x = 3.2, y = .15, substitute(paste(bold(\"t critical\"))),  col = \"red\")\n\n\n\nA Note on t-values:\n\\(t-value = \\frac{Estimated Parameter}{SF}\\)\nThe estimate represents the effect or impact that the independent variable has on the dependent variable. The standard error, on the other hand, quantifies the uncertainty or variability associated with that estimate.\nBy dividing the estimate by the standard error, we obtain the t-value. Essentially, the t-value tells us how many standard errors the estimate is away from zero. It helps us assess whether the estimate is statistically significant or just a result of random variation\nAnd a small toy-example for you to play around with 😉\nLet’s move onto the interactive Shiny app plot. The plot allows you to visualize the relationship between two variables and fit a linear model to the data. To use the app, follow these steps:\nChoose the variables from the dropdown menu.\nObserve how the line changes to fit the data points better.\nObserve how the changes in:\nIntercept\nSlope(s)\np-values\nR-squared value as you adjust the model.\nThe app plot is an excellent way to see how the slope and intercept of a linear model can impact the fit of the model to the data. You can also see how the R-squared value changes as you adjust the model, which is a measure of how well the model fits the data. The closer the R-squared value is to 1, the better the model fits the data.\nThe data Im using here is from a built-in dataset in R called “mtcar”.\n\n\nlibrary(knitr)\nkable(head(mtcars))\n\n\n\n\nmpg\n\n\ncyl\n\n\ndisp\n\n\nhp\n\n\ndrat\n\n\nwt\n\n\nqsec\n\n\nvs\n\n\nam\n\n\ngear\n\n\ncarb\n\n\nMazda RX4\n\n\n21.0\n\n\n6\n\n\n160\n\n\n110\n\n\n3.90\n\n\n2.620\n\n\n16.46\n\n\n0\n\n\n1\n\n\n4\n\n\n4\n\n\nMazda RX4 Wag\n\n\n21.0\n\n\n6\n\n\n160\n\n\n110\n\n\n3.90\n\n\n2.875\n\n\n17.02\n\n\n0\n\n\n1\n\n\n4\n\n\n4\n\n\nDatsun 710\n\n\n22.8\n\n\n4\n\n\n108\n\n\n93\n\n\n3.85\n\n\n2.320\n\n\n18.61\n\n\n1\n\n\n1\n\n\n4\n\n\n1\n\n\nHornet 4 Drive\n\n\n21.4\n\n\n6\n\n\n258\n\n\n110\n\n\n3.08\n\n\n3.215\n\n\n19.44\n\n\n1\n\n\n0\n\n\n3\n\n\n1\n\n\nHornet Sportabout\n\n\n18.7\n\n\n8\n\n\n360\n\n\n175\n\n\n3.15\n\n\n3.440\n\n\n17.02\n\n\n0\n\n\n0\n\n\n3\n\n\n2\n\n\nValiant\n\n\n18.1\n\n\n6\n\n\n225\n\n\n105\n\n\n2.76\n\n\n3.460\n\n\n20.22\n\n\n1\n\n\n0\n\n\n3\n\n\n1\n\n\nAfter looking at the different variables in this dataset, lets see how each of them can be modeled as a linear regression. The blue line represents the best-fitting line (i.e., the line that minimizes the distance between the data and the model – the line with the lowest RSS)\n\n\n\n\nIn conclusion, linear models are a powerful tool in statistics, and the interactive Shiny app plot provides an excellent way to explore the concepts. With the app, you can experiment with different linear models and see how they fit the data. I hope you found this post informative and helpful, and please don’t hesitate to leave any comments or questions below!\nHomework solution:\nHere is the dataaset:\n\n\ndat_salary <-  data.frame(\"iq\" = c(120, 110,  100, 135, 140),\n                          \"monthly salary\" = c(2500, 2300, 2400, 3000, 2100))\n\ndat_salary\n\n   iq monthly.salary\n1 120           2500\n2 110           2300\n3 100           2400\n4 135           3000\n5 140           2100\n\nHere is a plot of the data, as per usual:\n\n\nggplot(dat_salary,  aes(x = iq, y = monthly.salary)) +\n  geom_point(col = \"blue\", alpha = .4) + \n  geom_smooth(method = \"lm\", col = \"red\") +\n  theme_minimal()\n\n\n\nAnd the model’s output:\n\n\nsummary(lm(dat_salary$monthly.salary ~ dat_salary$iq))\n\n\nCall:\nlm(formula = dat_salary$monthly.salary ~ dat_salary$iq)\n\nResiduals:\n       1        2        3        4        5 \n  43.304 -123.661    9.375  493.750 -422.768 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept)   2060.268   1394.855   1.477    0.236\ndat_salary$iq    3.304     11.441   0.289    0.792\n\nResidual standard error: 382.9 on 3 degrees of freedom\nMultiple R-squared:  0.02704,   Adjusted R-squared:  -0.2973 \nF-statistic: 0.08338 on 1 and 3 DF,  p-value: 0.7916\n\n\n\n\n",
      "last_modified": "2023-09-26T07:47:27+02:00"
    },
    {
      "path": "q_a_summary.html",
      "title": "Q & A  - Summary ",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nHere is the summary of your questions and answers\n\n\n\n\n\n\n\n",
      "last_modified": "2023-09-26T07:47:28+02:00"
    },
    {
      "path": "statistik_i.html",
      "title": "Statistik 1",
      "description": "This tab will contain the topics covered in the course Statistik I \n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\n\n\n",
      "last_modified": "2023-09-26T07:47:28+02:00"
    },
    {
      "path": "statistik_ii.html",
      "title": "Untitled",
      "description": "A new article created using the Distill format.\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\n\n\n",
      "last_modified": "2023-09-26T07:47:29+02:00"
    }
  ],
  "collections": []
}
