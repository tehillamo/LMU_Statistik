{
  "articles": [
    {
      "path": "about.html",
      "title": "About Our Group",
      "description": "Who we are and what we are intreseted in?",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2023-11-14T16:11:58+01:00"
    },
    {
      "path": "anova_oneWay.html",
      "title": "Analysis of Variance - A Guide",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nWhat is an Omnibus test and how is it related to the analysis of variance?\nOmnibus tests are a kind of statistical test. They test whether the explained variance in a set of data is significantly greater than the unexplained variance, overall.\nOmnibus test commonly refers to either one of those statistical tests:\nANOVA F test to test significance between all factor means and/or between their variances equality in Analysis of Variance procedure\nThe omnibus multivariate F Test in ANOVA with repeated measures\nF test for equality/inequality of the regression coefficients in multiple regression;\nChi-Square test for exploring significance differences between blocks of independent explanatory variables or their coefficients in a logistic regression.\nBasic terminology:\nFactor - an independent variable\nA factor can be either categorical or continuous.\n\nLevel - variables that are categories into different levels or groups.\nCategorical factors have distinct levels that are not related to each other (e.g. type of fertilizer), while continuous factors represent a range of values along a continuum (e.g. temperature).\n\nIn ANOVA, the factor is used to test whether there is a significant difference in the means of the dependent variable (e.g. expressed aggression) across the different levels of the factor (age groups).\nOne-Way vs. Two-Way ANOVA\nOne-way ANOVA and two-way ANOVA are two variations of this test that differ in their design and purpose.\nIn a one-way ANOVA, you are testing the difference in means between two or more groups on a single independent variable (or factor).\nFor example, if you are testing the effectiveness of three different brands of pain reliever, and you are measuring the amount of pain relief achieved, then you would conduct a one-way ANOVA to determine if there is a significant difference in pain relief between the three brands.\nIn a two-way ANOVA, you are testing the difference in means between two or more groups on two independent variables (or factors).\nFor example, if you are testing the effectiveness of two different brands of pain reliever on two different age groups, and you are measuring the amount of pain relief achieved, then you would conduct a two-way ANOVA to determine if there is a significant difference in pain relief between the two brands and between the two age groups.\nSo what is the difference between the two again?\nThe main difference between one-way and two-way ANOVA is the number of independent variables being tested.\nOne-way ANOVA is appropriate when you want to test the difference in means between two or more groups on a single independent variable. Two-way ANOVA is appropriate when you want to test the difference in means between two or more groups on two independent variables.\nLets get down to business…\nA reminder:\nThe statistical model of ANOVA is a way of mathematically representing the variation in a dependent variable (Y) across different levels of one or more independent variables, also known as factors (X).\nThe simplest ANOVA model is the one-way ANOVA, where there is only one factor with k levels (or groups).\nLets look at the actual statistical model:\n\\(Yij = µ + τi + εij\\)\nwhere:\n\\(Yij\\) represents the value of the dependent variable for the jth observation in the ith group.\n\\(µ\\) represents the overall mean of the dependent variable across all groups.\n\\(τi\\) represents the difference between the mean of the ith group and the overall mean.\n\\(εij\\) represents the random error term, which accounts for the variability in the dependent variable that is not explained by the factor.\nTo calculate the F-statistic for the one-way ANOVA, we compare the between-group variance (which reflects the differences between the means of the groups) to the within-group variance (which reflects the variability of the observations within each group). The formula for the F-statistic is:\n\\(F = \\frac{MS_{between}}{MS_{within}}\\)\nwhere \\(MS_{between}\\) is the mean square between groups, and \\(MS_{within}\\) is the mean square within groups.\nExamples are always a good idea so here is one:\nlet’s say we want to test whether there is a significant difference in the mean weight of three different breeds of dogs: Poodles, Bulldogs, and Golden Retrievers.\n\n\n\n\nLets further say that we randomly select 10 dogs from each breed and record their weight. The data can be represented in the following table:\nBreed\nWeight (lbs)\nPoodle\n12\nPoodle\n14\n…\n…\nBulldog\n25\nBulldog\n24\n…\n…\nGolden Retriever\n60\nGolden Retriever\n58\n…\n…\nOur Research Question: Can test whether there is a significant difference in the mean weight of the three breeds?\nAnswer: Yes. Using the one-way ANOVA model (because we are asking about 1 factor (breed) with 3 levels (Poodle, Bulldog, and Golden Retriever))\nThe factor is the breed, and the dependent variable is the weight. We can calculate the F-statistic and p-value to determine whether there is a statistically significant difference between the means.\nHere is how we would do it by hand (but who would, really? R solves it instantly)…\nCalculate the total sum of squares (SST), which is the sum of the squared deviations of each observation from the overall mean:\n\\(SST = Σ(Yij - Y..)²\\)\nwhere \\(Yij\\) is the weight of the jth dog in the ith group, and \\(Y\\).. is the overall mean weight.\nIn this example, the overall mean weight \\(Y_{weight}\\) is:\n\\(Y.. = (12 + 14 + ... + 60 + 58) / 30 = 34.3\\)\nThe total sum of squares is:\n\\(SST = (12 - 34.3)² + (14 - 34.3)² + ... + (60 - 34.3)² + (58 - 34.3)² = 9688.3\\)\nGood! the next step is:\n2. Calculate the between-group sum of squares (SSB), which is the sum of the squared deviations of each group mean from the overall mean:\n\\(SSB = Σ(Ni * (Yi. - Y..)²)\\)\nwhere \\(Ni\\) is the number of observations in the \\(ith\\) group, \\(Yi\\). is the mean weight of the ith group, and \\(Y\\).. is the overall mean weight.\n\\(Mean weight of Poodles = (12 + 14 + ... + 16) / 10 = 14.7\\)\n\\(Mean weight of Bulldogs = (25 + 24 + ... + 29) / 10 = 26.3\\)\n\\(Mean weight of Golden Retrievers = (60 + 58 + ... + 57) / 10 = 58.7\\)\nThe between-group sum of squares is:\n\\(SSB = (10 * (14.7 - 34.3)²) + (10 * (26.3 - 34.3)²) + (10 * (58.7 - 34.3)²) = 8436.0\\)\nWell done. here is the final step:\nCalculate the within-group sum of squares (SSW), which is the sum of the squared deviations of each observation from its group mean:\n\\(SSW = Σ(Yij - Yi.)²\\)\nwhere \\(Yi\\). is the mean weight of the ith group.\nWhich in our example, the within-group sum of squares is:\n\\(SSW = (12 - 14.7)² + (14 - 14.7)² + ... + (57 - 58...\\)\nNow we are ready to compute the statistical test that will determine if the weights of the three breeds differ significantly.\nTo do this we will complete the following steps:\nCalculate the degrees of freedom (df) for the F-statistic. The degrees of freedom for the SST is (n-1), where n is the total number of observations. The degrees of freedom for the SSB is (k-1), where k is the number of groups. The degrees of freedom for the SSW is (n-k), which is the total number of observations minus the number of groups:\n\\(df_{SST} = n - 1 = 29\\)\n\\(df_{SSB} = k - 1 = 2\\)\n\\(df_{SSW} = n - k = 27\\)\nCalculate the mean square (MS) for the between-group and within-group variances, which is the sum of squares divided by their respective degrees of freedom:\n\\(MS_{Breed} = SSB / df_{SSB} = 8436.0 / 2 = 4218.0\\)\n\\(MS_{Weight} = SSW / df_{SSW} = 124.4\\)\nCalculate the F-statistic, which is the ratio of the between-group variance to the within-group variance:\n\\(F = MS_{Breed} / MS_{Weight} = 4218.0 / 124.4 = 33.9\\)\nCalculate the p-value, which is the probability of obtaining an F-statistic as extreme or more extreme than the observed F-statistic, assuming the null hypothesis is true. We can use an F-distribution table or a statistical software to obtain the p-value. For example, using a significance level of 0.05, the p-value is less than 0.001, which is highly significant.\nTherefore, we can reject the null hypothesis and conclude that there is a significant difference in the mean weight of Poodles, Bulldogs, and Golden Retrievers.\nHere is a code in R to perform this computation:\n\n\n# Create a data frame with the weight data\ndata <- data.frame(\n  weight = c(20, 25, 18, 22, 15, 28, 21, 23, 19, 26, \n             17, 24, 16, 27, 14, 29, 13, 30, 12, 31, \n             11, 32, 10, 33, 9, 34, 8, 35, 7, 36),\n  breed = c(rep(\"Poodle\", 10), rep(\"Bulldog\", 10), rep(\"Golden Retriever\", 10))\n)\n\n# Run the ANOVA\nanova_result <- aov(weight ~ breed, data = data)\n\n# Print the ANOVA table\nsummary(anova_result)\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nbreed        2    0.8    0.40   0.005  0.995\nResiduals   27 2246.7   83.21               \n\nHere is the interpretation of these results:\nSure! Here’s an interpretation of each element in the ANOVA table output:\nDf: This column indicates the degrees of freedom for each source of variation in the ANOVA model. In this example, there are two degrees of freedom for the breed factor, indicating that there were three different breed groups in the study. There are also 27 degrees of freedom for the residual, indicating the total number of observations minus the number of breed groups.\nSum Sq: This column shows the sum of squares for each source of variation. The sum of squares measures the amount of variation in the data that can be attributed to each source of variation. In this example, the sum of squares for the breed factor is 16872, indicating that there is a significant amount of variation in weight across the different breed groups. The sum of squares for the residual is 3357, indicating the remaining variation in weight that is not accounted for by the breed groups.\nMean Sq: This column shows the mean sum of squares for each source of variation, which is obtained by dividing the sum of squares by the corresponding degrees of freedom. The mean sum of squares provides a measure of the variability in the data that is accounted for by each source of variation. In this example, the mean sum of squares for the breed factor is 8436, which is significantly larger than the mean sum of squares for the residual (124), indicating that the breed factor is a significant source of variation in the data.\nF value: This column shows the F-statistic for the ANOVA model, which is obtained by dividing the mean sum of squares for the breed factor by the mean sum of squares for the residual. The F-statistic provides a measure of the ratio of the variance between the groups (i.e., breed) to the variance within the groups (i.e., residual). In this example, the F-value is 33.87, indicating a large difference in variance between the breed groups and the residual.\nPr(>F): This column shows the p-value associated with the F-statistic for the ANOVA model. The p-value provides a measure of the probability of obtaining an F-statistic as extreme or more extreme than the observed F-statistic, assuming the null hypothesis (i.e., there is no significant difference between the groups) is true. In this example, the p-value is 3.7e-08, which is much smaller than the significance level of 0.05, indicating that we can reject the null hypothesis and conclude that there is a significant difference in weight between the breed groups.\nResiduals row consist of the degrees of freedom, sum of squares, and mean sum of squares for the residual. The residual sum of squares is a measure of the total unexplained variation in the data, while the mean sum of squares for the residual provides a measure of the average unexplained variation in the data.\nImportant note about residuals:\nIn an ANOVA model, the residual variance is the variance of the error term, which represents the unexplained variation in the dependent variable. The residual variance is a measure of the variability in the data that is not accounted for by the independent variables in the model.\n\n\n\n",
      "last_modified": "2023-11-14T16:11:59+01:00"
    },
    {
      "path": "anova_twoWay.html",
      "title": "Analysis of Variance - Two-Way",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nHere is the data set.\nThe two-way ANOVA will test whether the independent variables (type of training [Yoga vs. Cardio vs. HIIT] AND Gender of training [F vs. M]) have an effect on the dependent variable (well-being - score). But there are some other possible sources of variation in the data that we want to take into account.\nWe are going to ask if there is an effect of type of training on participants’ well being scores\n\n   Happy_Score Sport_type Gender\n1           20       yoga      m\n2           75       yoga      w\n3           70       yoga      w\n4           55       yoga      m\n5           65       yoga      w\n6           35       yoga      m\n7           25       yoga      w\n8           65       yoga      w\n9           50       yoga      m\n10          55       yoga      w\n11          75     cardio      m\n12          40     cardio      w\n13          55     cardio      m\n14          77     cardio      w\n15          80     cardio      m\n16          70     cardio      m\n17          35     cardio      w\n18          50     cardio      m\n19          60     cardio      w\n20          80     cardio      m\n21          25       HIIT      w\n22          35       HIIT      w\n23          45       HIIT      m\n24          76       HIIT      m\n25          30       HIIT      m\n26          30       HIIT      w\n27          40       HIIT      w\n28          65       HIIT      m\n29          55       HIIT      m\n30          15       HIIT      m\n\nI would always advice you to plot your data.\n\n\n\nAnd here it is for the flipped plot:\n\n\n\n\n\nmu_11_w = mean(dat$Happy_Score[dat$Sport_type == \"yoga\" & dat$Gender == \"w\"])\nmu_11_m = mean(dat$Happy_Score[dat$Sport_type == \"yoga\" & dat$Gender == \"m\"])\n\nmu_12_w = mean(dat$Happy_Score[dat$Sport_type == \"cardio\" & dat$Gender == \"w\"])\nmu_12_m = mean(dat$Happy_Score[dat$Sport_type == \"cardio\" & dat$Gender == \"m\"])\n\nmu_13_w = mean(dat$Happy_Score[dat$Sport_type == \"HIIT\" & dat$Gender == \"w\"])\nmu_13_m = mean(dat$Happy_Score[dat$Sport_type == \"HIIT\" & dat$Gender == \"m\"])\n\n\nThe statistical model’s equation/notation is:\n\\(\\color{red}{Y_{ijk} = \\mu_{jk} + \\alpha_{j} + \\beta_{k} + \\gamma_{jk} + \\epsilon_{ijk}}\\)\nLooks scary, right?\nLets break it down:\nStarting with the independent variable \\(\\color{red}{Y_{ijk}}\\).\n\\(\\color{red}{Y_{ijk}}\\) is a random variable. It describes the PERSONAL Well-being score of person \\(i\\) in the populations \\(j,k\\).\n\\(\\color{red}{k}\\), the population of type of sports (\\(\\color{pink}{k=yoga}\\), \\(\\color{#00A99D}{k=cardio}\\), \\(\\color{Goldenrod}{k=HIIT}\\)).\n\\(\\color{red}{j}\\) represents the population of gender, in which sport is done (\\(\\color{#0071BC}{j=female}\\), \\(\\color{#3C8031}{j=male}\\)).\n\nLets define the number of levels in every group:\n\n\nk = 3 \nj = 2\n\n\n\\(\\alpha_{j}\\) and \\(\\beta_{k}\\) are based on the means for each population and each level:\nLets start with computing the means across all levels of all populations\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\nWe will start with stating our hypotheses:\nFor factor 1:\n\\(H_{0}:\\alpha_{j} = 0\\) for all \\(j\\)\n\\(H_{1}:\\alpha_{j} ≠ 0\\) for at least one \\(j\\)\nFor factor 2:\n\\(H_{0}:\\beta_{k} = 0\\) for all \\(j\\)\n\\(H_{1}:\\beta_{k} ≠ 0\\) for at least one \\(j\\)\nFor an interaction:\n\\(H_{0}:\\gamma_{jk} = 0\\) for all combinations of \\(jk\\)\n\\(H_{1}:\\gamma_{jk} ≠ 0\\) for at least one combinations of \\(jk\\)\nWe continue with running the statistical model.\n\\[\\color{red}{Y_{ijk} = \\mu_{jk} + \\alpha_{j} + \\beta_{k} + \\gamma_{jk} + \\epsilon_{ijk}}\\]\n\n\nmod_anova <-  aov(Happy_Score ~ Sport_type + Gender + Sport_type*Gender, \n                  data = dat)\nsum_anova <- summary(mod_anova)\nsum_anova\n\n                  Df Sum Sq Mean Sq F value Pr(>F)  \nSport_type         2   2123  1061.4   3.644 0.0415 *\nGender             1    103   102.8   0.353 0.5581  \nSport_type:Gender  2   1895   947.6   3.253 0.0562 .\nResiduals         24   6990   291.3                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWe finish with measuring the Effect sizes:\n\\(\\color{red}{\\eta^2}\\).\nThe effect size used for ANOVA. It easures the proportion of the total variance in a dependent variable that is associated with the membership of different groups defined by an independent variables.\nPartial \\(\\eta^2\\) is a similar measure in which the effects of other independent variables and interactions are partialled out.\nRemember the calculation of \\(\\eta^2\\) for the one-factor anova?\nIf not… here it is: \\(\\eta^2= \\frac{\\sigma^2_{zw}}{\\sigma^2_{tot}}\\)\nIn the two-factor anova we extend the above-written formula to a similar one. However, this time we have to consider the new parameters:\n\\(\\sigma^2_{factor1}\\)\n\\(\\sigma^2_{factor2}\\)\n\\(\\sigma^2_{interaction}\\)\n\\(\\sigma^2_{DV}\\)\n⇒ The overall effectsize \\(\\eta^2_{tot} = \\sigma^2_{factor1} + \\sigma^2_{factor2} + \\sigma^2_{interaction} + \\sigma^2_{DV}\\)\nPartial \\(\\color{red}{\\eta^2}\\).\nIt measures the proportion of variance explained by a given variable of the total variance remaining after accounting for variance explained by other variables in the model.\n\n\nlibrary(DescTools)\nEtaSq(mod_anova) \n\n                       eta.sq eta.sq.part\nSport_type        0.191030749  0.23291852\nGender            0.009247787  0.01448637\nSport_type:Gender 0.170568077  0.21329045\n\nExample of an interpretation of the results:\n\\(\\eta^2\\) is the proportion of variance the treatment accounts for in the wellbeing of participants. In the case of the sport type, we see that 19.1% of the total variance of wellbeing is explained by sport participants did.\n\\(\\eta_{part}^2\\) is the proportion of variance the treatment accounts for in the wellbeing of participants. In the case of the sport type, we see that 23.3% of the variance is explained by the sport type once the gender and the interaction effect are “taken out”.\nF-distribution & Hypothesis testing\nFor two-way ANOVA, the ratio between the mean sum of squares of a specific factor and the mean of sum of squares of the residuals (i.e., the variability within) is testd.\nLet’s look at how the distribution and the critical values look like.\nWe use the pf() to calculate the area under the curve for the interval [0,4.226] and the interval [4.226,+∞) of a F curve with with \\(v1=1\\) and \\(v2=24\\)\n\n\nx = sum_anova[[1]][[\"F value\"]][1]\ndf_factor1 = 1\ndf_inn = 24\n# interval $[0,1.5]\npf(x, df1 = df_factor1, df2 = df_inn, lower.tail = TRUE)\n\n[1] 0.9317056\n\n\n\nx = sum_anova[[1]][[\"F value\"]][1]\ndf_factor1 = 1\ndf_inn = 24\npf(x, df1 = df_factor1, df2 = df_inn, lower.tail = FALSE)\n\n[1] 0.06829437\n\nHere is the H0 F-distribution, from which we will infer whether to accept the null hypothesis or not.\n\n\n\n\n\n\n",
      "last_modified": "2023-11-14T16:12:06+01:00"
    },
    {
      "path": "Basic_concepts_ii.html",
      "title": "Distributions in Statistics",
      "description": "An introduction to Distributions in Statistics.\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nHere is a list of distributions used in hypothesis testing, along with Examples and their Parameters:\n1) Normal Distribution:\nExample: Testing the mean weight of a population.\nParameters: Mean (μ) and standard deviation (σ).\n\n\n\n2) t-Distribution:\nExample: Testing the mean score of a small sample.\nParameters: Degrees of freedom (df), which depend on the sample size.\nThe t-distribution test is used to test the significance of individual coefficients or variables in a statistical model.\nIt evaluates the null hypothesis that a specific coefficient is zero, indicating that the corresponding variable has no significant effect on the dependent variable.\n\n\n\n3) Chi-Square Distribution:\nExample: Testing the independence of categorical variables.\nParameters: Degrees of freedom (df), which depend on the number of categories being compared.\n\n\n\n4) F-Distribution:\nExample: Testing the equality of variances in ANOVA OR in linear regression models.\nParameters: Degrees of freedom for the numerator and denominator (\\(df_1\\), \\(df_2\\)), which depend on the number of groups being compared.\nIn statistical analysis, an omnibus (the word “omnibus” comes from Latin and it means “for all” or “including everything) test examines the overall significance of a group of variables or model coefficients rather than evaluating them individually. This comparison among multiple Parameters follows an F-distribution.\nIn the F-distribution, there are two types of degrees of freedom:\nNumerator degrees of freedom (\\(df_1\\)): This represents the degrees of freedom associated with the variability explained by the model or the effect of interest. In the context of a multiple linear regression, the numerator degrees of freedom are typically associated with the number of predictors or independent variables in the model.\nDenominator degrees of freedom (\\(df_2\\)): This represents the degrees of freedom associated with the variability within the model or the residual error. In a multiple linear regression, the denominator degrees of freedom are often associated with the number of observations minus the number of predictors.\nIn summary: The F-distribution is used in hypothesis testing and significance testing in statistical analyses such as analysis of variance (ANOVA) and regression analysis.\n\n\n\n5) Binomial Distribution:\nExample: Testing the proportion of success/failure outcomes.\nParameters: Number of trials (n) and probability of success (p).\n\n\n\n6) Poisson Distribution:\nExample: Testing the occurrence of rare events.\nParameter: Rate parameter (λ), which represents the average rate of event occurrence.\n7) Exponential Distribution:\nExample: Testing the time between events.\nParameter: Rate parameter (λ), which represents the average rate of event occurrence.\n8) Gamma Distribution:\nExample: Testing the shape parameter of a distribution.\nParameters: Shape parameter (α) and rate parameter (β), which determine the shape and scale of the distribution.\n\n\n\n",
      "last_modified": "2023-11-14T16:12:07+01:00"
    },
    {
      "path": "Basic_concepts.html",
      "title": "Basic Concepts worth knowing as a statistics student",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\nStandard statistical practices for notation:\nIn statistical notation, the use of Greek letters, Latin letters, and “hat” symbols (also known as caret or circumflex) follows certain conventions and standards.\nThese notations are used to represent different types of variables, parameters, and estimators in statistical formulas and equations.\nLets start with the Greeks:\n\n\n\n\nGreek letters are commonly used to represent population parameters OR random variables. They often denote fixed, unknown quantities that describe a population.\nHere are some commonly used Greek letters and their meanings in statistics:\n- μ (mu): Represents the population mean.\nσ (sigma): Represents the population standard deviation.\nθ (theta): Represents an unknown population parameter.\nπ (pi): Represents a population proportion.\nρ (rho): Represents a population correlation coefficient.\nGreek letters are also used to denote functions, such as the probability density function (pdf) or cumulative distribution function (CDF) of a random variable.\nNow to the Latin ones:\nThe Latin letters are typically used to represent sample statistics OR observed values. They are used when working with specific data sets or samples drawn from a population. They are often used when calculating estimators or summarizing sample data.\nHere are some commonly used Latin letters and their meanings in statistics:\n\\(x\\): Represents an observed value or a random variable from a sample.\n\\(n\\): Represents the sample size.\n\\(s\\): Represents the sample standard deviation.\n\\(p\\): Represents the sample proportion.\nWhat does the hat on top of letters mean?\n“Hat” symbol (e.g., \\(\\hat{x}\\)) is used to indicate an estimator or an estimated value based on a sample. In other words, the “hat” symbol is used to distinguish estimated values from the true population values or observed sample values.\nIt is placed on top of a Latin letter to denote that it represents an estimate rather than an observed value.\nFor example:\n\\(\\hat{y}\\) : Represents the estimated value of the dependent variable in regression analysis.\n\\(\\hat{p}\\) : Represents the estimated proportion based on a sample.\n\\(\\hat{\\beta}\\) : Represents the estimated slope coefficient in regression analysis.\nMathematicians often use set notation to denote all values that can be taken by a variable or a range of values. The notation typically involves curly braces { } and a condition or rule specifying the characteristics of the values. Here are a few common notations:\nSet Builder Notation:\n{x | condition} denotes a set of values “x” that satisfy the specified condition. For\nexample, {x | x > 0} represents the set of all positive real numbers.\nInterval Notation:\n(a, b) represents an open interval that includes all real numbers greater than “a” and less than “b”. For example, (0, 1) represents the interval between 0 and 1 (excluding the endpoints).\n[a, b] represents a closed interval that includes all real numbers greater than or equal to “a” and less than or equal to “b”. For example, [0, 1] represents the interval including both 0 and 1.\nEnumeration Notation:\n{x₁, x₂, x₃, …} denotes a set of specific values “x₁, x₂, x₃, …” where the ellipsis (…) indicates that the sequence continues indefinitely.\ni ∈ {1, 2, 3, …, n}:\nThis notation indicates that the index “i” belongs to the set of values {1, 2, 3, …, n}.\nThe ellipsis (…) represents a continuation of the sequence until “n”. For example, if you have a dataset with 100 observations, the notation would be:\ni ∈ {1, 2, 3, …, 100}.\n\n\n\n",
      "last_modified": "2023-11-14T16:12:08+01:00"
    },
    {
      "path": "Basic_R_2.html",
      "title": "Basic Concepts in R - Types of Data",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nThis session, we’ll explore the evolution of data structures, from the simplest building block to the versatile and powerful lists.\nThis is going to be a useful guide for beginners looking to grasp the basics in programming, join us on this educational journey.\nIn this blog post, we will explore:\n\n1. Creating and Working with Vectors: How to declare and manipulate vectors to store data efficiently.\n\n2. Binding Data: Techniques for combining vectors into larger datasets, including cbind and rbind.\n\n3. Matrices in Action: Practical examples of using matrices for numerical operations and data analysis.\n\n4. Unlocking the Power of Lists: An in-depth exploration of lists, including creating, modifying, and navigating through nested lists.\n\n5. Real-World Use Cases: Demonstrations of how these data structures are applied in programming and data analysis.\nSingle Values: The Foundation\nIn our last blog post, we discussed objects in R that were assigned a single value, whether factor type (e.g., your name or gender) or a numerical value such as a = 1 or a logical type (e.g., rain_tomorrow = FALSE). These single values serve as the building blocks of more complex data structures. They represent individual pieces of information and can take on various data types, depending on the context.\nHere’s a brief recap:\nFactor Values: Factors are used to represent categorical data with distinct levels.\nFor example, you might have a factor variable for “gender” with levels “Male,” “Female,” and “Non-binary.”\nNumerical Values: Numerical values are used for storing numeric data, such as integers or floating-point numbers.\nThey allow you to perform mathematical operations and calculations.\nLogical Values: Logical values, often represented as TRUE or FALSE, are used to handle binary decisions or conditions.\nThey are essential for control flow and conditional statements in programming.\n\n\n# Single value examples\nage <- 30\nname <- \"John\"\ntemperature <- 25.5\n\n# Printing single values\ncat(\"Name:\", name, \"\\n\")\n\nName: John \n\ncat(\"Age:\", age, \"\\n\")\n\nAge: 30 \n\ncat(\"Temperature:\", temperature, \"\\n\")\n\nTemperature: 25.5 \n\n These single values are versatile and useful on their own, but as we venture further into data manipulation and analysis, we’ll discover how they can be combined and structured into more complex data structures like vectors, matrices, and lists.\nIndexing in R: Navigating Your Data\nBefore we dive into the world of vectors, it’s essential to understand the concept of indexing in R.\nIndexing allows us to access specific elements within data structures, such as vectors, matrices, and lists.\nThink of indexing as a way to pinpoint and retrieve exactly what you need from your data.\nIn R, indexing starts at 1, which means the first element in a data structure is accessed using index 1, the second element with index 2, and so on.\nAdditionally, negative indices can be used to exclude elements from a data structure, and logical vectors can be used to filter elements based on certain conditions.\nHere’s a brief overview of indexing in R:\nSingle Indexing: Access a single element using its position in the data structure.\n\n\nmy_vector <- c(\"apple\", \"banana\", \"cherry\")\nprint(my_vector[2])  # Accesses the second element, \"banana\"\n\n[1] \"banana\"\n\nMultiple Indexing: Access multiple elements by specifying a vector of indices.\n\n\nmy_vector <- c(\"apple\", \"banana\", \"cherry\")\nindices <- c(1, 3)\nprint(my_vector[indices])  # Accesses the first and third elements, \"apple\" and \"cherry\"\n\n[1] \"apple\"  \"cherry\"\n\nNegative Indexing: Exclude elements using negative indices.\n\n\nmy_vector <- c(\"apple\", \"banana\", \"cherry\")\nprint(my_vector[-2])  # Excludes the second element, \"banana\"\n\n[1] \"apple\"  \"cherry\"\n\nLogical Indexing: Filter elements based on logical conditions.\n\n\nmy_vector <- c(10, 20, 30, 40, 50)\ncondition <- my_vector > 25\nprint(my_vector[condition])  # Selects elements greater than 25, i.e., 30, 40, and 50\n\n[1] 30 40 50\n\nNow that we have a good grasp of indexing, let’s explore the world of vectors and see how indexing plays a crucial role in working with them effectively.\nVectors: Bridging the Gap\nEnter vectors, an array-like structure that allows us to store multiple single values of the same data type within a single variable.\nVectors provide a seamless way to work with lists of data, enabling us to perform operations on them collectively.\n\n\n# Creating vectors\nnumbers <- c(1, 2, 3, 4, 5)\nfruits <- c(\"apple\", \"banana\", \"cherry\")\n\n# Accessing elements of a vector\nprint(numbers[2])    # Prints the second element (2)\n\n[1] 2\n\nprint(fruits[3])     # Prints \"cherry\"\n\n[1] \"cherry\"\n\nMatrices: Two-Dimensional Arrays\nMatrices are the next step in our journey.\nThese two-dimensional data structures consist of rows and columns, providing a structured way to represent data.\nMatrices are especially useful for numerical computations, statistical analysis, and linear algebra.\n\n\n# Combining vectors using cbind and rbind\nvector1 <- c(1, 2, 3)\nvector2 <- c(\"A\", \"B\", \"C\")\n\n# Combining by columns (cbind)\ncombined_col <- cbind(vector1, vector2)\n\n# Combining by rows (rbind)\ncombined_row <- rbind(vector1, vector2)\n\nprint(combined_col)\n\n     vector1 vector2\n[1,] \"1\"     \"A\"    \n[2,] \"2\"     \"B\"    \n[3,] \"3\"     \"C\"    \n\nprint(combined_row)\n\n        [,1] [,2] [,3]\nvector1 \"1\"  \"2\"  \"3\" \nvector2 \"A\"  \"B\"  \"C\" \n\n\n\n# Creating matrices\nmatrix1 <- matrix(1:6, nrow = 2, ncol = 3)  # 2x3 matrix\nmatrix2 <- matrix(c(7, 8, 9), nrow = 3, ncol = 1)  # 3x1 matrix\n\n# Matrix multiplication\nresult_matrix <- matrix1 %*% matrix2\n\nprint(matrix1)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nprint(matrix2)\n\n     [,1]\n[1,]    7\n[2,]    8\n[3,]    9\n\nprint(result_matrix)\n\n     [,1]\n[1,]   76\n[2,]  100\n\n\n\n# Creating lists\nstudent1 <- list(name = \"Alice\", age = 22, grades = c(85, 90, 78))\nstudent2 <- list(name = \"Bob\", age = 24, grades = c(76, 88, 92))\n\n# Creating a list of students\nstudents <- list(student1, student2)\n\n# Accessing elements of a list\nprint(students[[1]]$name)  # Accessing the name of the first student\n\n[1] \"Alice\"\n\nprint(students[[2]]$grades)  # Accessing the grades of the second student\n\n[1] 76 88 92\n\nLists: The Ultimate Data Structure\nFinally, we arrive at the pinnacle of data structures: lists.\nLists are versatile collections that can store heterogeneous data types, including vectors, matrices, and even other lists.\nThey allow us to create complex, nested data structures, making them invaluable in data analysis, data manipulation, and managing large-scale projects.\n\n\n\n",
      "last_modified": "2023-11-14T16:12:09+01:00"
    },
    {
      "path": "Basic_R_3.html",
      "title": "Basic Concepts in R - Part 3",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nSession I:\n\n1.1. Get to know R & R studio (basic use, interface)\n1.2. Learn about types of variables in R.\n\n\n1.3. Create scripts and save them in R projects (project’s folder)\n\n\n1.4. Get to know TidyTuesday\n1.5. Read in data sets\n\nCreate Your First Project\nWhy?\nBy using projects, you can ensure that your workspace (i.e., your loaded data, scripts, and other files) remains specific to that project. This approach keeps things clean and avoids potential mix-ups between different tasks or data sets.\nHow?\nStep-by-step guide to create a project in RStudio:\nLaunch RStudio: Begin by opening the RStudio application.\nGo to the Projects Menu: In the top-right corner of RStudio, you’ll see a small box (which might say “Project: (None)” if you haven’t used projects before).\nClick on this box to open a dropdown menu.\nCreate a New Project: Select New Project from the dropdown. This will initiate a new dialog box.\nCreate New ProjectChoose a Project Type: You’ll have three options here:\nNew Directory: Create a brand-new project in a new directory.\nExisting Directory: Turn an existing directory into a project.\nVersion Control: If you’re using Git, SVN, or another version control system, you can clone a repository as a project.\n\nFor this guide, let’s select New Directory for simplicity.\nSelect a Directory Type: You’ll be presented with a few options:\nNew Project: A basic project.\nR Package: If you’re developing an R package.\nShiny Web Application: If you’re making a Shiny app.\nTypically, for general data analysis tasks, New Project is the one you’d choose.\n\nName and Choose Location:\nDirectory Name: Give your project a name. This will also be the name of the directory/folder that RStudio creates.\nCreate project as a subdirectory of: Browse your file system to choose where you’d like the project directory to be located.\nOptionally, you can also choose to create a new R script, use packrat (for dependency management), or open in a new session.\nCreate the Project:\nOnce you’ve filled everything out, click the Create Project button. RStudio will now make a new directory in your chosen location, and inside that directory, it will create a file with the .Rproj extension. This file holds settings specific to your project.\nYou’re Now in Your New Project: Notice that your working directory has changed (you can confirm with getwd() in the console). Also, any scripts, data, or plots you now create will be associated with this project.\nClosing & Opening Projects: When you close RStudio with an open project, the next time you open that .Rproj file, RStudio will remember your workspace, open scripts, and other settings. It’s a great way to pick up where you left off!\nCreate Scripts Within Your Project\nNow lets create our first script and save it.\n\nR Script: First Step\nR Script: Second Step\nR Script: Third Step\nR Script: Fourth Step\nR Script: Fifth Step\nR Script: Sixth Step\n\nOr as a video:\n\n\n\n\n",
      "last_modified": "2023-11-14T16:12:10+01:00"
    },
    {
      "path": "Basic_R_4.html",
      "title": "Basic Concepts in R - Part 4",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nSession I:\n\n1.1. Get to know R & R studio (basic use, interface)\n1.2. Learn about types of variables in R.\n\n\n1.3. Create scripts and save them in R projects (project’s folder)\n\n\n1.4. Get to know TidyTuesday\n1.5. Read in data sets\n\n\n\n\n",
      "last_modified": "2023-11-14T16:12:11+01:00"
    },
    {
      "path": "Basic_R_Functions_Repetitions.html",
      "title": "Basic Concepts in R: Recapping Objects",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nObjekte - a Quick Recap\n\nWie wir in unserer letzten Sitzung gesehen und geübt haben, sind Objekte wichtige Bausteine der Datenstrukturen in R.\nA short recap:\nR ObjekteSkalar: Ein einfacher Wert.\n\n\nmein_skalar = 42\nprint(mein_skalar)\n\n[1] 42\n\nVektor: Eine eindimensionale Datenstruktur, die mehrere Werte desselben Typs speichern kann.\n\n\nmein_vektor = c(1, 2, 3, 4, 5)\nprint(mein_vektor)\n\n[1] 1 2 3 4 5\n\nMatrix: Eine zweidimensionale Datenstruktur, bei der jedes Element denselben Datentyp hat.\nIn R wird die Funktion matrix() hauptsächlich verwendet, um Matrizen zu erstellen.\nMatrix-Erstellung mit der matrix() Funktion:\nMan kann eine Matrix erstellen, indem man die Daten und die Anzahl der Zeilen oder Spalten angibt:\n\n\nmeine_matrix = matrix(1:9, nrow=3, ncol=3)\nprint(meine_matrix)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nEin Data Frame ist eine besondere Art von Liste, bei der jede Spalte ein Vektor sein kann. Im Beispiel unten haben wir einen Data Frame mit drei Personen und ihren jeweiligen Berufen und Altersangaben erstellt.\n\nData Frame-Erstellung direkt innerhalb der data.frame() Funktion:\nMan kann einen Data Frame direkt innerhalb der Klammern der data.frame() Funktion erstellen, ohne zuerst Vektoren zu definieren. Hierbei werden die Spalten “Name”, “Alter” und “Beruf” direkt innerhalb der Funktion definiert, ohne separate Vektoren zu verwenden.\n\n\n# Daten für den DataFrame erstellen\npersonen = data.frame(\n  Name = c(\"Anna\", \"Ben\", \"Clara\"),\n  Alter = c(25, 30, 29),\n  Beruf = c(\"Ingenieur\", \"Arzt\", \"Lehrer\")\n)\n\n# DataFrame anzeigen\nprint(personen)\n\n   Name Alter     Beruf\n1  Anna    25 Ingenieur\n2   Ben    30      Arzt\n3 Clara    29    Lehrer\n\nEinen Moment… was ist der Unterschied zwischen einer Matrix und einem Data Frame?\nDas ist eine berechtigte Frage…\nData Frame:\nEin Data Frame in R ähnelt einer Matrix, ist aber flexibler, da verschiedene Spalten unterschiedliche Datentypen haben können.\nEs ist vergleichbar mit einer Tabelle in einer Datenbank oder einer Excel-Tabelle.\nEin Data Frame ist eine der am häufigsten verwendeten Datenstrukturen in R, insbesondere für Datenanalysen.\n\n\ndf <- data.frame(Name = c(\"Anna\", \"Ben\"), \n                 Alter = c(25, 30))\nprint(df)\n\n  Name Alter\n1 Anna    25\n2  Ben    30\n\n\n\n\n",
      "last_modified": "2023-11-14T16:12:12+01:00"
    },
    {
      "path": "Basic_R_Introduction.html",
      "title": "Basic Concepts in R: Objects & Functions",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nEnglish\n\nHow R Works?\nIn R, there are (essentially) only two things: objects and functions.\nObjects\nObjects are data. These can be datasets, for example, but also individual numbers, entire models, or even lists of different objects. Objects always have a name and are called by that name.”\n\n\ntehilla <- 54\ntehilla\n\n[1] 54\n\nObjects also have properties that determine which functions (see more below) can be applied to them. For example, if an object is of type ‘numeric,’ you can add it to other ‘numeric’ objects. However, if an object is a ‘string,’ which is a character string, you cannot add it. Many error messages result from attempting to apply functions to an incompatible data type.\n\n\nage_obj <- 35\nage_obj_2 <- 1\nI_will_be_X_next_year <-  age_obj + age_obj_2\nI_will_be_X_next_year\n\n[1] 36\n\nVariables: In R, a variable allows you to store data values. Think of it as a named storage that our programs can manipulate. The stored value can be a number, text, a series of numbers, or any other type of data.\nExample:\n\n\nmy_variable <- 10\nmy_variable\n\n[1] 10\n\nAssignments:\nIn R, you use the <- symbol for assignments. This symbol is known as the assignment operator. The value on the right gets assigned to the name on the left.\nExample:\n\n\nname <- \"Tehilla\"\n\n\nWhere to See Stored Variables:\nWhen you’re using R or RStudio, you can view the variables you’ve defined in a couple of ways:\nIn the R console, just type the variable’s name and press enter. For example, typing name would output “Tehilla”.\nIn RStudio, the Environment pane (usually located in the top-right corner) displays a list of all current variables, their type, and their value.\nStored Variables in RTypes of R Objects\nVectors:\nA vector is a basic data structure in R that contains elements of the same type. This could be numeric, character, logical, etc.\nThe c() function is used to concatenate values into a vector.\nExample:\n\n\nnumeric_vector <- c(1, 2, 3, 4, 5)\ncharacter_vector <- c(\"apple\", \"banana\", \"cherry\")\n\n\nIn R, variables can hold data of various types. These types are fundamental and determine the kind of operations you can perform on the data. Here’s a rundown of the most common data types (or modes) in R:\n  1.1 Numeric: Represents numbers and can be either integer or double (decimal numbers).\n  Example:\n  \n\n\nx <- 5    # numeric, specifically an integer\ny <- 5.5  # numeric, specifically a double\n\n\n    1.2. Character: Represents strings (text). Strings in R are enclosed by either single or double quotes.\n    Example:\n    \n\n\nname <- \"John Doe\"\nname\n\n[1] \"John Doe\"\n\n    1.3. Logical: Represents boolean values: TRUE or FALSE. Often a result of logical conditions or operations.\n    Example:\n    \n\n\nflag <- TRUE\nflag\n\n[1] TRUE\n\n    1.4. Integer: A subtype of numeric. It specifically represents integer numbers. To specifically define an integer, you can use the L suffix.\n  Example:\n\n\ncount <- 23L\ncount\n\n[1] 23\n\n  1.5. Factor: Categorical data is often represented as factors in R. Factors can be ordered (like \"Low\", \"Medium\", \"High\") or unordered (like \"Male\", \"Female\"). While the data might look like characters, factors are stored as integers, and a separate lookup table holds the character values.\n  Example: \n  \n\n\ngender <- factor(c(\"Male\", \"Female\", \"Female\", \"Male\"))\ngender\n\n[1] Male   Female Female Male  \nLevels: Female Male\n\n 1.6. Lists: A special type that can hold different types of elements, including vectors, functions, or even other lists.\nExample:\n\n\nmy_list <- list(name = \"Alice\", age = 25, scores = c(85, 90, 92))\nmy_list\n\n$name\n[1] \"Alice\"\n\n$age\n[1] 25\n\n$scores\n[1] 85 90 92\n\nVectorsFunctions\nFunctions alter objects in specific ways. A function can be almost anything. For statistics, we mainly need functions that transform data (e.g., mapping strings to specific values, creating new variables, loading datasets, saving datasets, etc.) and functions that perform statistical calculations (e.g., calculating means, performing regressions, etc.).\nfunctionsExample:\nHere is an innocent vector… It’s about to change its form by being passed into a function.\n\n\n# Create a numeric vector\nnumbers <- c(5, 10, 15, 20, 25) \n\n# The function c() is used very often. It combines elements into a vector.\nnumbers\n\n[1]  5 10 15 20 25\n\nIn fact, we will use the “mean” function in R to calculate the average or arithmetic mean of a set of numeric values. It adds up all the numbers in a dataset and then divides that sum by the total count of numbers. This provides you with a single value that represents the central tendency or typical value of the data.\n\n\n# Calculate the mean\naverage <- mean(numbers)\n\n# Print the result\nprint(average)\n\n[1] 15\n\nIn this example, the mean function will add up the numbers (5 + 10 + 15 + 20 + 25) and divide the sum by the total count (5 in this case), resulting in an average of 15.\nFunctions can also generate data and objects. For example, there are functions that create random numbers.\n\n\nrnorm(n = 20, mean = 0, sd = 1) # randomly draws 20 values from the normal distribution with a mean of 0 and SD of 1\n\n [1] -2.61364347  0.78045190  0.04510507 -0.10848916 -0.39122431\n [6] -1.68699950 -0.95270446  0.50741411  0.01881610 -1.41891037\n[11]  0.59817193  0.78637962 -0.13463305 -0.01638652  1.40914041\n[16]  0.39483870  1.21987513 -0.04507721  1.47948784 -0.07568379\n\nFunctions can also generate graphics from objects.\n\n\nnormally_distributed <- rnorm(n = 20, mean = 0, sd = 1)\nhist(normally_distributed) # creates a histogram from the vector of normally distributed variables\n\n\n\n\n# Wie R funktioniert\nIn R gibt es im Wesentlichen nur zwei Dinge: Objekte und Funktionen.\n## Objekte\nObjekte sind Daten. Dies können beispielsweise Datensätze sein, aber auch einzelne Zahlen, ganze Modelle oder sogar Listen von verschiedenen Objekten. Objekte haben immer einen Namen und werden mit diesem Namen aufgerufen.\n\n[1] 54\n\nObjekte haben auch Eigenschaften, die bestimmen, welche Funktionen (siehe unten) auf sie angewendet werden können. Wenn ein Objekt beispielsweise vom Typ ‘numeric’ ist, können Sie es zu anderen ‘numeric’-Objekten hinzufügen. Wenn ein Objekt jedoch ein ‘string’ ist, eine Zeichenkette, können Sie es nicht hinzufügen. Viele Fehlermeldungen resultieren aus dem Versuch, Funktionen auf einen unpassenden Datentyp anzuwenden.\n\n[1] 36\n\nVariablen: In R ermöglicht es eine Variable, Datenwerte zu speichern. Denken Sie an sie als an einen benannten Speicher, den unsere Programme manipulieren können. Der gespeicherte Wert kann eine Zahl, Text, eine Reihe von Zahlen oder jede andere Art von Daten sein.\nBeispiel:\n[1] 10\n\nZuweisungen: In R verwenden Sie das <- Symbol für Zuweisungen. Dieses Symbol wird als Zuweisungsoperator bezeichnet. Der Wert auf der rechten Seite wird dem Namen auf der linken Seite zugewiesen.\nBeispiel:\n\n\nWo Sie gespeicherte Variablen sehen können:\nWenn Sie R oder RStudio verwenden, können Sie die von Ihnen definierten Variablen auf verschiedene Arten anzeigen:\nIn der R-Konsole geben Sie einfach den Namen der Variablen ein und drücken Sie Enter. Wenn Sie beispielsweise ‘name’ eingeben, wird “Tehilla” ausgegeben.\nIn RStudio zeigt das Environment-Fenster (normalerweise oben rechts) eine Liste aller aktuellen Variablen, ihres Typs und ihres Werts an.\n\n#### Arten von R-Objekten\n1. Vektoren:\nEin Vektor ist eine grundlegende Datenstruktur in R, die Elemente des gleichen Typs enthält. Dies kann numerisch, character, logisch usw. sein.\nDie Funktion c() wird verwendet, um Werte zu einem Vektor zusammenzuführen.\nBeispiel:\n\n\nIn R können Variablen Daten verschiedener Typen speichern. Diese Typen sind grundlegend und bestimmen die Art der Operationen, die auf die Daten angewendet werden können. Hier ist eine Übersicht über die häufigsten Datentypen (oder modes) in R:\n1.1 Numerisch: Stellt Zahlen dar und kann entweder Ganzzahlen oder Dezimalzahlen sein.\nBeispiel:\n\n1.2 Character: Stellt Zeichenketten (Text) dar. Zeichenketten in R sind entweder von einfachen oder doppelten Anführungszeichen umschlossen.\nBeispiel:\n\n[1] \"John Doe\"\n\n1.3 Logisch: Stellt boolesche Werte dar: TRUE oder FALSE. Oft das Ergebnis logischer Bedingungen oder Operationen.\nBeispiel:\n\n[1] TRUE\n\n1.4 Ganzzahl: Ein Untertyp von numerisch. Es stellt speziell ganze Zahlen dar. Um eine Ganzzahl explizit zu definieren, können Sie das L-Suffix verwenden.\nBeispiel:\n[1] 23\n\n1.5 Faktor: Kategoriale Daten werden oft als Faktoren in R dargestellt. Faktoren können geordnet sein (wie “Niedrig”, “Mittel”, “Hoch”) oder ungeordnet (wie “Männlich”, “Weiblich”). Obwohl die Daten wie Zeichenketten aussehen können, werden Faktoren als Ganzzahlen gespeichert, und eine separate Lookup-Tabelle enthält die Zeichenwerte.\nBeispiel:\n\n[1] Männlich Weiblich Weiblich Männlich Levels: Männlich Weiblich\n\n1.6 Listen: Ein spezieller Typ, der verschiedene Arten von Elementen wie Vektoren, Funktionen oder sogar andere Listen enthalten kann.\nBeispiel:\n\n```\n$name\n[1] “Alice”\n$alter\n[1] 25\n$punkte\n[1] 85 90 92\n```\n\n\nFunctions:\nThe way to maniupulate objects.\n\n\nvector <- c(1,4,6,8) # die Funktion c() braucht man sehr oft. Sie fügt Elemente␣ ↪zu einem Vektor zusammen\nvector\n\n[1] 1 4 6 8\n\nFunctions\nFunktionen verändern Objekte auf bestimmte Art und Weisen. Eine Funktion kann fast alles sein. Für die Statistik benötigen wir vor allem Funktionen, die Daten transformieren (z.B., Zuordnen von Zeichenketten zu bestimmten Werten, Erstellen neuer Variablen, Laden von Datensätzen, Speichern von Datensätzen usw.) und Funktionen, die statistische Berechnungen durchführen (z.B., Berechnung von Mittelwerten, Durchführung von Regressionen usw.).\nfunctionsBeispiel:\nHier ist ein unschuldiger Vektor… Er wird gleich seine Form ändern, indem er an eine Funktion übergeben wird.\n\n\n# Einen numerischen Vektor erstellen\nzahlen <- c(5, 10, 15, 20, 25)\n\n# Die Funktion c() wird sehr oft verwendet. Sie fügt Elemente zu einem Vektor zusammen.\nzahlen\n\n[1]  5 10 15 20 25\n\nTatsächlich werden wir in R die Funktion “mean” verwenden, um den Durchschnitt oder arithmetischen Mittelwert einer Menge numerischer Werte zu berechnen. Sie summiert alle Zahlen in einem Datensatz auf und teilt dann diese Summe durch die Gesamtanzahl der Zahlen. Dadurch erhalten Sie einen einzigen Wert, der die zentrale Tendenz oder den typischen Wert der Daten darstellt.\n\n\n# Den Mittelwert berechnen\ndurchschnitt <- mean(zahlen)\n\n# Das Ergebnis ausgeben\nprint(durchschnitt)\n\n[1] 15\n\nIn diesem Beispiel wird die Mittelwert-Funktion die Zahlen (5 + 10 + 15 + 20 + 25) addieren und die Summe durch die Gesamtanzahl (in diesem Fall 5) teilen, was zu einem Durchschnitt von 15 führt.\nFunktionen können auch Daten und Objekte generieren. Zum Beispiel gibt es Funktionen, die Zufallszahlen erstellen.\n\n\nrnorm(n = 20, mean = 0, sd = 1) # zieht zufällig 20 Werte aus der Normalverteilung mit einem Mittelwert von 0 und einer Standardabweichung von 1\n\n [1]  0.71222821  0.44295423  0.75958731 -0.46887767 -0.17270998\n [6]  0.68204990 -0.28157733 -0.96162257 -1.37765014  2.69758664\n[11] -1.69971506  0.02931665  1.93431434  0.83751088 -0.18534633\n[16]  0.53332918  1.78495624  0.18962782 -0.56908751  1.45189294\n\nFunktionen können auch Grafiken aus Objekten generieren.\n\n\nnormalverteilt <- rnorm(n = 20, mean = 0, sd = 1)\nhist(normalverteilt) # erstellt ein Histogramm aus dem Vektor der normalverteilten Variablen\n\n\n\n\n\n\n",
      "last_modified": "2023-11-14T16:12:14+01:00"
    },
    {
      "path": "Basic_R_Project.html",
      "title": "Basic Concepts in R: Create Your First R Project",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nEnglish\n\nCreate Your first project in R\nWhy?\nBy using projects, you can ensure that your workspace (i.e., your loaded data, scripts, and other files) remains specific to that project. This approach keeps things clean and avoids potential mix-ups between different tasks or datasets.\nHow?\nStep-by-step guide to create a project in RStudio:\nLaunch RStudio: Begin by opening the RStudio application.\nGo to the Projects Menu: In the top-right corner of RStudio, you’ll see a small box (which might say “Project: (None)” if you haven’t used projects before).\nClick on this box to open a dropdown menu.\nCreate a New Project: Select New Project from the dropdown. This will initiate a new dialog box.\nCreate New ProjectChoose a Project Type: You’ll have three options here:\nNew Directory: Create a brand-new project in a new directory.\nExisting Directory: Turn an existing directory into a project.\nVersion Control: If you’re using Git, SVN, or another version control system, you can clone a repository as a project.\n\nFor this guide, let’s select, for simplicity, new directory.\nSelect a Directory Type: You’ll be presented with a few options:\nNew Project: A basic project.\nR Package: If you’re developing an R package.\nShiny Web Application: If you’re making a Shiny app.\nTypically, for general data analysis tasks, New Project is the one you’d choose.\n\nName and Choose Location:\nDirectory Name: Give your project a name. This will also be the name of the directory/folder that RStudio creates.\nCreate project as a subdirectory of: Browse your file system to choose where you’d like the project directory to be located.\nOptionally, you can also choose to create a new R script, use packrat (for dependency management), or open in a new session.\nCreate the Project:\nOnce you’ve filled everything out, click the Create Project button. RStudio will now make a new directory in your chosen location, and inside that directory, it will create a file with the .Rproj extension. This file holds settings specific to your project.\nYou’re Now in Your New Project: Notice that your working directory has changed (you can confirm with getwd() in the console). Also, any scripts, data, or plots you now create will be associated with this project.\nClosing & Opening Projects: When you close RStudio with an open project, the next time you open that .Rproj file, RStudio will remember your workspace, open scripts, and other settings. It’s a great way to pick up where you left off!\nCreate Scripts Within Your Project\nNow lets create our first script and save it.\n\nR Script: First Step\nR Script: Second Step\nR Script: Third Step\nR Script: Fourth Step\nR Script: Fifth Step\nR Script: Sixth Step\nR Script: Seventh StepOr as a video:\n\n\nDeutsch\n\nErstelle dein erstes Projekt in R\nWarum?\nDurch die Verwendung von Projekten kannst du sicherstellen, dass dein Arbeitsbereich (d.h. deine geladenen Daten, Skripte und andere Dateien) spezifisch für dieses Projekt bleibt. Dieser Ansatz hält alles übersichtlich und verhindert mögliche Verwechslungen zwischen verschiedenen Aufgaben oder Datensätzen.\nWie?\nSchritt-für-Schritt Anleitung zur Erstellung eines Projekts in RStudio:\nRStudio starten: Beginne, indem du die RStudio-Anwendung öffnest.\nGehe zum Projekte-Menü: In der oberen rechten Ecke von RStudio siehst du ein kleines Feld (welches “Projekt: (Keines)” anzeigen könnte, wenn du zuvor noch keine Projekte verwendet hast).\nKlicke auf dieses Feld, um ein Dropdown-Menü zu öffnen.\nEin neues Projekt erstellen: Wähle Neues Projekt aus dem Dropdown aus. Dadurch wird ein neues Dialogfeld geöffnet.\nNeues Projekt erstellen\nCreate New ProjectWähle einen Projekttyp: Du hast hier drei Optionen:\nNeues Verzeichnis: Erstelle ein brandneues Projekt in einem neuen Verzeichnis.\nVorhandenes Verzeichnis: Verwandle ein bestehendes Verzeichnis in ein Projekt.\nVersionskontrolle: Wenn du Git, SVN oder ein anderes Versionskontrollsystem verwendest, kannst du ein Repository als Projekt klonen.\nFür diesen Leitfaden wählen wir aus Gründen der Einfachheit Neues Verzeichnis.\nWähle einen Verzeichnistyp: Dir werden einige Optionen angezeigt:\nNeues Projekt: Ein Basisprojekt.\nR Paket: Wenn du ein R-Paket entwickelst.\n-Shiny Webanwendung: Wenn du eine Shiny-App erstellst.Normalerweise würdest du für allgemeine Datenanalyseaufgaben Neues Projekt wählen.\nName und Standort wählen:\nVerzeichnisname: Gib deinem Projekt einen Namen. Dies wird auch der Name des von RStudio erstellten Verzeichnisses/Ordners sein.\nErstelle das Projekt als Unterordner von: Durchsuche dein Dateisystem, um den gewünschten Standort für das Projektverzeichnis auszuwählen.\nOptional kannst du auch ein neues R-Skript erstellen, Packrat verwenden (zur Abhängigkeitsverwaltung) oder in einer neuen Sitzung öffnen.\nDas Projekt erstellen:\nNachdem du alles ausgefüllt hast, klicke auf den Button Projekt erstellen. RStudio wird nun in deinem gewählten Ort ein neues Verzeichnis erstellen und in diesem Verzeichnis eine Datei mit der Erweiterung .Rproj erstellen. Diese Datei enthält projektspezifische Einstellungen.\nDu bist jetzt in deinem neuen Projekt: Beachte, dass sich dein Arbeitsverzeichnis geändert hat (du kannst dies mit getwd() in der Konsole bestätigen). Auch alle Skripte, Daten oder Plots, die du jetzt erstellst, werden diesem Projekt zugeordnet.\nProjekte schließen & öffnen: Wenn du RStudio mit einem geöffneten Projekt schließt, wird RStudio bei der nächsten Öffnung dieser .Rproj-Datei deinen Arbeitsbereich, offene Skripte und andere Einstellungen wiederherstellen. Es ist eine großartige Möglichkeit, dort weiterzumachen, wo du aufgehört hast!\nCreate Scripts Within Your Project\nJetzt erstellen wir unser erstes Skript und speichern es.\n\nR Script: First Step\nR Script: Second Step\nR Script: Third Step\nR Script: Fourth Step\nR Script: Fifth Step\nR Script: Sixth Step\nR Script: Seventh StepOr as a video:\n\nTerminology:\nGithub repo\nEnglish - A GitHub repo (repository) is like a folder for your project on the website GitHub. It holds all the files, history, and changes related to that project so others can see, use, or help improve it.\nDeutsch - A GitHub repo (Repository) ist wie ein Ordner für dein Projekt auf der Webseite GitHub. Er beinhaltet alle Dateien, den Verlauf und Änderungen des Projekts, sodass andere es sehen, verwenden oder helfen können, es zu verbessern.\nTidyTuesday\nEnglish\nTidy Tuesday is a weekly data project aimed at the R community, primarily on Twitter. It provides data enthusiasts with a weekly dataset which they can explore, visualize, and analyze using the R programming language. The initiative encourages participants to share their findings and visualizations on Twitter using the hashtag #TidyTuesday.\n: The idea is to promote both learning in data visualization and R programming, as well as community engagement. Each week’s dataset is diverse, ranging from topics like sports, science, economics, and more. The datasets are often “tidy,” meaning they adhere to a specific structure that’s optimized for analysis with the tidyverse set of packages in R.\nThe datasets and challenges are shared via the “rfordatascience” GitHub repository, which is why you often see references to their GitHub repo when discussing Tidy Tuesday.\nDeutsch:\nTidy Tuesday ist ein wöchentliches Datenprojekt, das sich an die R-Community richtet, hauptsächlich auf Twitter. Es bietet Datenbegeisterten wöchentlich einen Datensatz, den sie mithilfe der R-Programmiersprache erkunden, visualisieren und analysieren können. Die Initiative ermutigt die Teilnehmer dazu, ihre Erkenntnisse und Visualisierungen auf Twitter unter Verwendung des Hashtags #TidyTuesday zu teilen.\nDie Idee besteht darin, sowohl das Lernen in der Datenvisualisierung als auch in der R-Programmierung zu fördern als auch die Gemeinschaftsbeteiligung zu unterstützen. Der Datensatz der Woche ist vielfältig und reicht von Themen wie Sport, Wissenschaft, Wirtschaft und mehr. Die Datensätze sind oft “ordentlich”, was bedeutet, dass sie einer spezifischen Struktur folgen, die für die Analyse mit dem Set von tidyverse-Paketen in R optimiert ist.\nDie Datensätze und Herausforderungen werden über das GitHub-Repository “rfordatascience” geteilt, weshalb Sie häufig Verweise auf ihr GitHub-Repo sehen, wenn Sie über Tidy Tuesday sprechen.\n\n\n\n",
      "last_modified": "2023-11-14T16:12:15+01:00"
    },
    {
      "path": "Basic_R_Variabels_i.html",
      "title": "Basic Concepts in R: Objects",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeutsch\n\nIn diesem Blog werden wir uns mit zwei grundlegenden Konzepten befassen, die die Grundlage jeder Programmiersprache bilden: Objekte und Funktionen.\nObjekte sind die Behälter, die Informationen speichern und es uns ermöglichen, Daten zu speichern und zu manipulieren.\nWährend Funktionen die Werkzeuge sind, mit denen wir spezifische Aufgaben effizient ausführen können, sind sie auch für Dinge wie Datengenerierung und Datenmanipulation nützlich\n\n Wie Funktioniert R?\n\nIn R gibt es im Wesentlichen zwei Dinge:\nObjekte\nFunktionen\n\nObjekte\n\nObjekte sind Behälter, die von einfachen eindimensionalen Daten bis zu mehrdimensionalen Datensätzen alles speichern können. Sie können auch ganze Modelle oder Listen von verschiedenen Objekten aufnehmen.\nObjekte haben auch Eigenschaften, die bestimmen, welche Funktionen (siehe unten) auf sie angewendet werden können.\nObjekte haben immer einen Namen (z.B., my_matrix = matrix(…)) und werden mit diesem Namen aufgerufen (Zum Beispiel, um den Output von ‘my_matrix’ in der Konsole anzuzeigen/display/print).\nR Objekte\nEin Skalar\n\nEin Skalar ist in einfachen Worten eine einzelne Zahl oder ein einzelner Wert. In der Mathematik und Programmierung ist ein Skalar ein einzelnes Element, das keine Richtung oder Dimension hat.\nZum Beispiel ist die Zahl 5 ein Skalar, weil sie lediglich eine einzelne Zahl darstellt und keine mehrdimensionale Datenstruktur besitzt.\nSkalare werden oft in Berechnungen und Programmierungen verwendet, um einzelne Werte oder Datenpunkte darzustellen.\n\n*p.s.\nIn der Programmierung sind Zeichenketten (Strings) Abfolgen von Zeichen, wie Wörter oder Sätze. Um sie von anderen Datentypen wie Variablen oder Zahlen zu unterscheiden, müssen Zeichenketten in Anführungszeichen eingeschlossen sein (i.e., doppelte (” “)), um eine Zeichenkette zu definieren.\n\n\n\ntehilla = 54\ntehilla\n\n[1] 54\n\nage_obj = 35\nage_obj_2 = 1\nIch_werde_nächstes_Jahr_X_sein =  age_obj + age_obj_2\n# print(c(\"Ich werde, nächstes Jahr,\", Ich_werde_nächstes_Jahr_X_sein, \"Jahre alt\"))\n\n\n\nAufgabe:\nSet a variable with your name and assign a value. For example: your_name = “Lena”.\nCreate a variable for your current age and assign a value, e.g., your_age = 30\nCreate another variable that represents your age next year. You can achieve this by adding 1 to your current age. For example: your_age_next_year = your_age + 1\nOutput the current and future age using the print() command and the respective variables.\n\n\nVektoren\n\nEin Vektor ist eine grundlegende Datenstruktur in R, die Elemente des gleichen Typs enthält.\nDies kann numerisch, character, logisch usw. sein (siehe unten).\nIn der Programmiersprache R wird die Funktion c() verwendet, um Vektoren zu erstellen und zu kombinieren. Das “c” steht für “concatenate” (zusammenfügen) oder “combine” (kombinieren).\n\n\n# Adding two numeric vectors element-wise\nvector1 = c(1, 2, 3, 4, 5)\nvector2 = c(6, 7, 8, 9, 10)\nresult_vector = vector1 + vector2\nprint(result_vector)\n\n[1]  7  9 11 13 15\n\n\nAufgabe:\nCreate a vector ‘your_age_and_friends’ that contains the current age values. For example: your_age_and_friends = c(25, 30, 35, 40).\nCreate another vector ‘ones_vector’ consisting solely of ones and having the same length as the vector ‘your_age_and_friends’.\nAdd the two vectors ‘your_age_and_friends’ and ‘ones_vector’ element-wise to calculate the age values of all friends for next year and save it as ‘age_of_friends_next_year’ to represent the age values of all friends for next year.\nprint your resulting vector.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Arten in R\n\nData Types in RBeispiel:\n\n\nnumerischer_vektor = c(1, 2, 3, 4, 5)\ncharacter_vektor = c(\"Apfel\", \"Banane\", \"Kirsche\")\n\n\nIn R können Variablen Daten verschiedener Typen speichern.\nDiese Typen sind grundlegend und bestimmen die Art der Operationen, die auf die Daten angewendet werden können.\nHier ist eine Übersicht über die häufigsten Datentypen (oder modes) in R:\n\nNumerisch\n\nStellt Zahlen dar und kann entweder Ganzzahlen oder Dezimalzahlen sein.\nBeispiel:\n\n\nx = 5    # numerisch, speziell eine Ganzzahl\ny = 5.5  # numerisch, speziell eine Dezimalzahl\nz = x*y  # numeric, specifically a double. a multiplication of x and y\ntypeof(z)\n\n[1] \"double\"\n\nDie Funktion typeof() in R gibt den Datentyp eines Objekts zurück. Es zeigt an, welche Art von Daten (zum Beispiel Zahl: “numeric/double”, Zeichenkette: “character”) das Objekt enthält.\n\nAufgabe:\nCreate a variable ‘your_age’ and assign a numerical value that represents your current age, including decimal places to account for months. For example: your_age = 25.5 (25 years and 6 months).\nCreate another variable ‘friend_age’ and assign a numerical value that represents the age of one of your friends, also including decimal places for months. For example: friend_age = 30.8 (30 years and 9 months).\nCalculate the product of your age and your friend’s age, and assign the result to a variable ‘product_of_ages’\nUse the typeof() function (as shown in the provided code) to check the data type of the variable ‘product_of_ages’ and determine whether it is a ‘double’ (which is used in R to represent both floating-point numbers and integers).\nDouble = Numeric!\n\n\nCharacter\n\nStellt Zeichenketten (Text) dar.\nZeichenketten in R sind entweder von doppelten Anführungszeichen umschlossen.\nBeispiel:\n\n\nname = \"John\"\nname\n\n[1] \"John\"\n\nname_empty = \"John Doe is a great, unknown, singer from the 90's\"\nname_empty\n\n[1] \"John Doe is a great, unknown, singer from the 90's\"\n\nIn der Programmiersprache R kann eine Variable des Typs “character” (Zeichen) ein einzelnes Wort, einen Buchstaben, einen ganzen Satz oder sogar einen gesamten Absatz speichern. Für R ist jede Zeichenfolge, egal wie lang oder kurz, die in Anführungszeichen eingeschlossen ist, als ein einzelnes Zeichenobjekt zu betrachten\n\nAufgabe:\nCreate a variable ‘your_name’ and assign a value that represents your name. For example: your_name = “Max Mustermann”.\nCreate another variable ‘name_empty’ and assign a value that indicates a name should be entered, similar to the provided code. For example: name_empty = “Please Insert Your Name”\nPrint the value of the variable ‘your_name’ to display your name.\nask R to provide you with the type of both variables. Are they the same?\n\n\nLogisch\n\nStellt boolesche Werte dar: TRUE oder FALSE.\nSie gibt uns als Ergebnis, ob logische Bedingungen oder Operationen wahr (‘true’) oder falsch (‘false’) sind.\nBeispiel:\n\n\nflag = TRUE\nflag\n\n[1] TRUE\n\nis_dog_a_cat =  FALSE\nis_dog_a_cat\n\n[1] FALSE\n\n3>4\n\n[1] FALSE\n\n3==4\n\n[1] FALSE\n\n3<4\n\n[1] TRUE\n\n\nAufgabe:\nCreate a variable ‘zahl1’ and assign it the value 8.\nCreate a variable ‘zahl2’ and assign it the value 4.\nCompare the two numbers using logical expressions and report the output for each comparison. For example, check:\n3.1. Whether ‘zahl1’ is greater than ‘zahl2’ (zahl1 > zahl2) and report the result.\n3.2. Whether ‘zahl1’ is equal to ‘zahl2’ (zahl1 == zahl2) and report the result.\n3.3. Whether ‘zahl1’ is less than ‘zahl2’ (zahl1 < zahl2) and report the result.”\nsave the conditions (3.1-3.3) as new variables. try to give them meaningfull names.\n\n\nFaktor\n\nKategoriale Daten werden oft als Faktoren in R dargestellt.\nIn der Programmiersprache R bezeichnet der Datentyp “factor” (Faktor) eine Variable, die kategorische Daten repraesentiert. Ein Faktor besitzt eine feste Anzahl von unterschiedlichen Werten, auch als “Levels” (Ebenen) bezeichnet.\nEin großer Vorteil von Faktoren ist, dass sie uns schnell erkennen lassen, welche Kategorien in unseren Daten vorhanden sind.\nStellen Sie sich beispielsweise eine Variable vor, die die Farben von Autos in einem Datensatz repraesentiert. Selbst wenn die Farbe “Blau” tausend Mal in diesem Datensatz vorkommt, wird sie durch den Faktor nur einmal als Level repraesentiert.\nBeispiel:\nNehmen wir an, wir haben folgende Daten über Auto-Farben: c(“Blau”, “Rot”, “Blau”, “Grün”, “Rot”, “Rot”). Wenn wir diese Daten als Faktor speichern:\n\n\nauto_farben = factor(c(\"Blau\", \"Rot\", \"Blau\", \"Grün\", \"Rot\", \"Rot\"))\n\n\nDann können wir die Funktion levels() verwenden, um die unterschiedlichen Farben (Kategorien) zu identifizieren, die in unseren Daten vorkommen:\n\n\nlevels(auto_farben)\n\n[1] \"Blau\" \"Grün\" \"Rot\" \n\n\nUnd noch ein Beispiel:\n\n\ngeschlecht = factor(c(\"Männlich\", \"Weiblich\", \"Weiblich\", \"Männlich\"))\n\ngeschlecht\n\n[1] Männlich Weiblich Weiblich Männlich\nLevels: Männlich Weiblich\n\nlevels(geschlecht)\n\n[1] \"Männlich\" \"Weiblich\"\n\n\nAufgabe:\nCreate a vector called ‘fruit_colors’ that represents the ONLY the colors of these fruits. Note: You will need the following structure for your ‘fruit_colors’ vector (Factor(c(fruit_colors))):\nApple: “Red”\nBanana: “Yellow”\nStrawberry: “Red”\nGrapes: “Green”\nOrange: “Orange”\nCherry: “Red”\nRaspberry: “Red”\nPlum: “Purple”\nMango: “Orange”\nLemon: “Yellow”\nPrint the created vector ‘fruit_colors’.\nUse the levels() function on your ‘fruit_colors’ vector to display the unique color categories or factors in the vector. Name it unique_colour.\n\n\nMatrix\n\nDefinition:\nEine Matrix ist eine geordnete Anordnung von Zahlen, Werten oder Elementen in Zeilen und Spalten. Sie wird oft als rechteckige Tabelle dargestellt.\nEine Matrix hat zwei Hauptmerkmale:\nZeilen: Die horizontalen Reihen in einer Matrix, die normalerweise numerisch oder alphanumerisch nummeriert oder beschriftet sind.\nSpalten: Die vertikalen Spalten in einer Matrix, die normalerweise numerisch oder alphanumerisch nummeriert oder beschriftet sind.\nDie Elemente in einer Matrix können Zahlen, Buchstaben, logische Werte oder andere Datentypen sein.\nMatrizen werden in der linearen Algebra, Statistik und Datenanalyse häufig verwendet, um Daten zu organisieren, zu strukturieren und mathematische Operationen durchzuführen, wie zum Beispiel Multiplikation, Addition und Subtraktion.\nMatrizen bieten eine effiziente Möglichkeit, Daten in einer tabellarischen Form zu verarbeiten und zu analysieren.\nBeispiel: Erstellen einer Matrix aus Vektoren.\n\n\n# Erstellen Sie zwei numerische Vektoren\nvektor1 = c(1, 2, 3)\nvektor2 = c(4, 5, 6)\n\n# Kombinieren Sie die Vektoren, um eine Matrix zu erstellen\nmatrix1 = cbind(vektor1, vektor2)\nprint(matrix1)\n\n     vektor1 vektor2\n[1,]       1       4\n[2,]       2       5\n[3,]       3       6\n\nFunktion cbind stehts für “columns bind” oder “Spalten zusammenfügen”. Sie wird verwendet, um mehrere Vektoren, Matrizen oder Datenrahmen nach Spalten zu kombinieren oder zu konkatenieren.\n\nListen\n\nEin spezieller Typ, der verschiedene Arten von Elementen wie Vektoren, Funktionen oder sogar andere Listen enthalten kann.\nBeispiel:\n\n\nmeine_liste = list(name = \"Alice\", alter = 25, punkte = c(85, 90, 92))\nmeine_liste\n\n$name\n[1] \"Alice\"\n\n$alter\n[1] 25\n\n$punkte\n[1] 85 90 92\n\nVectors\n\n\n",
      "last_modified": "2023-11-14T16:12:16+01:00"
    },
    {
      "path": "Basic_R.html.html",
      "title": "Basic Concepts in R",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nWhat is R?\nR is a powerful and open-source programming language and environment designed specifically for statistical computing and graphics. Originally developed in 1993 by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand, R has grown to be the tool of choice for statisticians, data scientists, and researchers worldwide.\nR Studio\nWhile not part of R itself, RStudio is a popular integrated development environment (IDE) for R, which provides a more user-friendly interface, debugging tools, and other useful features to make working with R easier and more efficient.\nR Environment\nWhen you start using R, you’ll typically interact with the following:\nSource (Script) Pane:\nThis is where you’ll write and edit your R scripts or R Markdown documents.\nScripts are essentially sequences of R commands that you want to run.\nYou can run individual lines or chunks of code by highlighting them and pressing Ctrl+Enter (Cmd+Enter on macOS).\nConsole Pane:\nThis is the heart of the R environment. Here, you can type R commands and see them executed in real-time.\nAny code executed from the script pane will also run and display results here.\nYou’ll also see messages, errors, and other outputs in this window.\nEnvironment/History Pane:\nEnvironment Tab: Displays a list of all the variables (including data frames, vectors, values) currently in memory. This is super handy to see what data you’ve loaded and the variables you’ve created.\nYou can click on data frames and matrices in the environment tab to view them in a spreadsheet-like grid.\nHistory Tab: Shows a log of all the commands you’ve executed. You can re-run any command from here by selecting and pressing Enter.\nFiles/Plots/Packages/Help Pane:\nFiles Tab: Lets you navigate through your computer’s file system, much like a file explorer. You can open and save scripts, import data, and manage directories.\nPlots Tab: Every time you create a visual (like a graph or chart), it appears here. You can also zoom, export, or navigate between multiple plots.\nPackages Tab: Provides a list of all the installed R packages. From here, you can: Load a package into memory using the checkbox next to its name.\nInstall new packages or update existing ones.\nHelp Tab: Whenever you need details about a specific function or package, you can use the help tab. Typing ?function_name into the console (e.g., ?mean) will also bring up the help page for that function in this pane.\nR InterfaceCreate Your first project in R\nWhy?\nBy using projects, you can ensure that your workspace (i.e., your loaded data, scripts, and other files) remains specific to that project. This approach keeps things clean and avoids potential mix-ups between different tasks or datasets.\nHow?\nStep-by-step guide to create a project in RStudio:\nLaunch RStudio: Begin by opening the RStudio application.\nGo to the Projects Menu: In the top-right corner of RStudio, you’ll see a small box (which might say “Project: (None)” if you haven’t used projects before).\nClick on this box to open a dropdown menu.\nCreate a New Project: Select New Project from the dropdown. This will initiate a new dialog box.\nCreate New ProjectChoose a Project Type: You’ll have three options here:\nNew Directory: Create a brand-new project in a new directory.\nExisting Directory: Turn an existing directory into a project.\nVersion Control: If you’re using Git, SVN, or another version control system, you can clone a repository as a project.\n\nFor this guide, let’s select New Directory for simplicity.\nSelect a Directory Type: You’ll be presented with a few options:\nNew Project: A basic project.\nR Package: If you’re developing an R package.\nShiny Web Application: If you’re making a Shiny app.\nTypically, for general data analysis tasks, New Project is the one you’d choose.\n\nName and Choose Location:\nDirectory Name: Give your project a name. This will also be the name of the directory/folder that RStudio creates.\nCreate project as a subdirectory of: Browse your file system to choose where you’d like the project directory to be located.\nOptionally, you can also choose to create a new R script, use packrat (for dependency management), or open in a new session.\nCreate the Project:\nOnce you’ve filled everything out, click the Create Project button. RStudio will now make a new directory in your chosen location, and inside that directory, it will create a file with the .Rproj extension. This file holds settings specific to your project.\nYou’re Now in Your New Project: Notice that your working directory has changed (you can confirm with getwd() in the console). Also, any scripts, data, or plots you now create will be associated with this project.\nClosing & Opening Projects: When you close RStudio with an open project, the next time you open that .Rproj file, RStudio will remember your workspace, open scripts, and other settings. It’s a great way to pick up where you left off!\n\n\n\n",
      "last_modified": "2023-11-14T16:12:17+01:00"
    },
    {
      "path": "Basic_R.html",
      "title": "Basic Concepts in R: Download R & R-Studio",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnglish\n\nWhat is R?\nR is a powerful and open-source programming language and environment designed specifically for statistical computing and graphics. R has grown to be the tool of choice for statisticians, data scientists, and researchers worldwide.\nWhy use R?\nR is a powerful open-source software developed specifically for statistical calculations and data visualization. For example, to examine the relationship between the number of hours students spend on their studies per week and their semester-end grades.\nWhat is R Studio?\nWhile not part of R itself, RStudio is a popular integrated development environment (IDE) for R, which provides a more user-friendly interface, debugging tools, and other useful features to make working with R eadur and more efficient.\nR base: The Ugly Friend of R Studio?Lets Download R\nClick on this link\nThe top of the web page provides three links for downloading R, depending on your operating system: Windows, Mac, or Linux. If, for example, you install R on Windows, click the “Download R for Windows” link.\nOben auf der Webseite sind drei Links zum Herunterladen von R , abhängig von Ihrem Betriebssystem: Windows, Mac oder Linux. Wenn du zum Beispiel R auf Windows installieren möchten, klicken du auf den Link „Download R for Windows“.\nR-Base Download PageClick the “base” link.\nNext, click the first link at the top of the new page.\nThis link should say something like “Download R 3.0.3 for Windows” except the 3.0.3 will be replaced by the most current version of R.\nThe link downloads an installer program, which installs the most up-to-date version of R for your operation system.\nRun this program and step through the installation wizard that appears.\nThe wizard will install R into your program files folders and place a shortcut in your Start menu.\nNote that you’ll need to have all of the appropriate administration privileges to install new software on your machine.\nInstall R Studio\nYou can download RStudio for free here.\nJust click the “Download RStudio” button and follow the simple instructions that follow.\nR-Studio Download PageOnce you’ve installed RStudio, you can open it like any other program on your computer—usually by clicking an icon on your desktop.\nNow That we Have R Studio Installed, Lets Get to Know Its Environment\nR InterfaceWhen you start using R, you’ll typically interact with the following:\nSource (Script) Pane:\nThis is where you’ll write and edit your R scripts or R Markdown documents.\nScripts are essentially sequences of R commands that you want to run.\nYou can run individual lines or chunks of code by highlighting them and pressing Ctrl+Enter (Cmd+Enter on macOS).\nANALOGY\nIn cooking, a recipe book is your source of instructions for preparing a dish.\nSimilarly, an R script is like your recipe book for data analysis. It contains a set of instructions (code) that you follow to perform various tasks and analyses.\nR script = Recipe BookConsole Pane:\nThis is the heart of the R environment. Here, you can type R commands and see them executed in real-time.\nAny code executed from the script pane will also run and display results here.\nYou’ll also see messages, errors, and other outputs in this window.\nANALOGY\nIn a kitchen, the chef’s workspace is where all the actual cooking and experimentation take place.\nIn RStudio, the console is your workspace where you can interact with R directly.\nIt’s where you can run commands, experiment with code, and see immediate results, just like a chef working with ingredients and cooking utensils.\nConsole = Chef’s Workspace:Environment/History Pane:\nEnvironment Tab: Displays a list of all the variables (including data frames, vectors, values) currently in memory. This is super handy to see what data you’ve loaded and the variables you’ve created.\nYou can click on data frames and matrices in the environment tab to view them in a spreadsheet-like grid.\nHistory Tab: Shows a log of all the commands you’ve executed. You can re-run any command from here by selecting and pressing Enter.\nANALOGY\nWhen cooking, you gather ingredients and use various cooking tools such as pots, pans, and utensils.\nIn R, the environment consists of variables, data, and functions that you’ve loaded or created.\nIt’s where your “ingredients” (data) and “cooking tools” (functions) are stored for use in your analysis.\nEnvironment = Ingredients and Cooking ToolsFiles/Plots/Packages/Help Pane:\nFiles Tab: Lets you navigate through your computer’s file system, much like a file explorer. You can open and save scripts, import data, and manage directories.\nPlots Tab: Every time you create a visual (like a graph or chart), it appears here. You can also zoom, export, or navigate between multiple plots.\nPackages Tab: Provides a list of all the installed R packages. From here, you can: Load a package into memory using the checkbox next to its name.\nInstall new packages or update existing ones.\nHelp Tab: Whenever you need details about a specific function or package, you can use the help tab. Typing ?function_name into the console (e.g., ?mean) will also bring up the help page for that function in this pane.\nANALOGY\nIn cooking, you periodically check the dish’s progress to see if it’s turning out as expected.\nIn RStudio, the view tabs (e.g., data viewer, plot viewer) allow you to inspect the intermediate results of your analysis, much like checking on your dish while it’s cooking.\nYou can visualize data, explore plots, and ensure everything is on track.\n\n\nDeutsch\n\nWhat is R?\nR ist eine kostenfreie Software für statistische Datenanalyse und Graphiken. Es beruht auf einer Implementation der Sprache S. Anfänglich wurde R von Ross Ihaka und Robert Gentleman (Univ. Auckland) entwickelt und wird seit Mitte der 90er Jahre von einem Entwickler-Kollektiv (R-Core) betreut.\nWhy use R?\nR ist eine leistungsstarke Open-Source-Software, die speziell für statistische Berechnungen und Datenvisualisierung entwickelt wurde. Zum Beispiel, um den Zusammenhang zwischen der Anzahl der Stunden, die Studenten pro Woche für ihr Studium aufgewendet haben, und ihren Semesterendnoten zu untersuchen.\nWhat is R Studio?\nObwohl nicht direkt ein Teil von R selbst, ist RStudio eine beliebte integrierte Entwicklungsumgebung (IDE) für R. Sie bietet eine benutzerfreundlichere Oberfläche, Debugging-Tools und andere nützliche Funktionen, um die Arbeit mit R einfacher und effizienter zu gestalten\nR base: The Ugly Friend of R Studio?Lets Download R\nOben auf der Webseite sind drei Links zum Herunterladen von R , abhängig von Ihrem Betriebssystem: Windows, Mac oder Linux. Wenn du zum Beispiel R auf Windows installieren möchten, klicken du auf den Link „Download R for Windows“.\nR-Base Download PageKlicke zuerst auf den ersten Link ganz oben auf der neuen Seite.\nDer Link wird so etwas wie „R X.0.3 für Windows herunterladen“ anzeigen, aber die X.0.3 wird durch die neueste Version von R ersetzt sein.\nWenn du auf den Link klickst, wird ein Installationsprogramm heruntergeladen. Dieses Programm installiert die aktuellste R-Version für dein Betriebssystem.\nStarte dieses Programm und folge dem Installationsassistenten.\nR wird in deinem Programmdateien-Ordner installiert werden und ein Verknüpfung im Startmenü erstellen.\nBeachte, dass du alle notwendigen Administrationsrechte benötigst, um neue Software auf deinem Computer zu installieren.\nInstall R Studio\nDu kannst RStudio hier kostenlos herunterladen: here.\nKlicke einfach auf den „RStudio herunterladen“-Button und folge den einfachen Anweisungen, die danach kommen.\nR-Studio Download PageNachdem du RStudio installiert hast, kannst du es wie jedes andere Programm auf deinem Computer öffnen – normalerweise durch Klicken auf ein Symbol auf deinem Desktop.\nJetzt, da wir R Studio installiert haben, lass uns seine Umgebung kennenlernen.\nR InterfaceWenn du anfängst, R zu nutzen, wirst du typischerweise mit den folgenden Elementen interagieren:\nQuellcode / Skript Fenster:\nHier schreibst und bearbeitest du deine R-Skripte oder R Markdown-Dokumente.\nSkripte sind im Wesentlichen Abfolgen von R-Befehlen, die du ausführen möchtest (analog zum Rezept).\nDu kannst einzelne Zeilen oder Code-Blöcke ausführen, indem du du markierst und Ctrl+Enter drückst (Cmd+Enter bei macOS).\nANALOGIE\nBeim Kochen ist ein Kochbuch Ihre Informationsquelle für die Zubereitung eines Gerichts.\nÄhnlich verhält es sich mit einem R-Skript, das wie Ihr Kochbuch für die Datenanalyse ist. Es enthält eine Reihe von Anweisungen (Code), denen du folgen, um verschiedene Aufgaben und Analysen durchzuführen.\nR-Skript = KochbuchKonsole Fenster:\nDies ist das Herz der R-Umgebung. Hier kannst du R-Befehle eingeben und du in Echtzeit ausführen sehen.\nJeder im Skriptfenster ausgeführte Code wird auch hier ausgeführt und die Ergebnisse werden hier angezeigt.\nDu wirst auch Nachrichten, Fehler und andere Ausgaben in diesem Fenster sehen.\nANALOGIE\nIn einer Küche ist der Arbeitsbereich des Kochs der Ort, an dem das eigentliche Kochen und Experimentieren stattfindet.\nIn RStudio ist die Konsole Ihr Arbeitsbereich, in dem du direkt mit R interagieren können.\nHier können du Befehle ausführen, mit Code experimentieren und sofortige Ergebnisse sehen, genauso wie ein Koch, der mit Zutaten und Kochutensilien arbeitet.\nConsole = Arbeitsbereich des Kochs:Umgebung/Verlauf Fenster:\nUmgebung Tab: Zeigt eine Liste aller Variablen (einschließlich Datenrahmen, Vektoren, Werte), die gerade im Speicher sind.\nDas ist super praktisch, um zu sehen, welche Daten du geladen hast und welche Variablen du erstellt hast.\nDu kannst auf Datenrahmen und Matrizen im Umgebung-Tab klicken, um du in einem rasterähnlichen Gitter anzusehen.\nANALOGIE\nBeim Kochen sammeln du Zutaten und verwenden verschiedene Küchenwerkzeuge wie Töpfe, Pfannen und Besteck.\nIn R besteht die Umgebung aus Variablen, Daten und Funktionen, die du geladen oder erstellt haben.\nHier werden Ihre “Zutaten” (Daten) und “Küchenwerkzeuge” (Funktionen) gespeichert, die du für Ihre Analyse verwenden.\nUmgebung = Zutaten und KüchenwerkzeugeDateien/Grafiken/Pakete/Hilfe Fenster:\nDateien Tab: Ermöglicht dir die Navigation durch dein Computersystem, ähnlich wie ein Datei-Explorer. Du kannst Skripte öffnen und speichern, Daten importieren und Verzeichnisse verwalten.\nGrafiken Tab: Jedes Mal, wenn du ein Visualidurungselement (wie eine Grafik oder ein Diagramm) erstellst, erscheint es hier. Du kannst auch zwischen mehreren Grafiken zoomen, exportieren oder navigieren.\nPackages Tab: Zeigt eine Liste aller installierten R-Pakete an. Von hier aus kannst du: Ein Paket in den Speicher laden, indem du das Kästchen neben seinem Namen ankreuzt. Neue Pakete installieren oder bestehende aktualiduren.\nHilfe Tab: Immer wenn du Details über eine bestimmte Funktion oder ein Paket benötigst, kannst du den Hilfe-Tab verwenden. Wenn du ?Funktionsname in die Konsole eingibst (z. B. ?mean), wird auch die Hilfeseite für diese Funktion in diesem Fenster aufgerufen.\nANALOGIE\nBeim Kochen überprüfen du regelmäßig den Fortschritt des Gerichts, um zu sehen, ob es sich wie erwartet entwickelt.\nIn RStudio ermöglichen Ihnen die Ansichtsregisterkarten (z. B. Datenanzeige, Plot-Anzeige), die Zwischenergebnisse Ihrer Analyse zu inspizieren, ähnlich wie beim Überprüfen Ihres Gerichts während des Kochens.\ndu können Daten visualiduren, Diagramme erkunden und sicherstellen, dass alles wie geplant verläuft.\nAnsichtsregisterkarten = Überprüfung des Gerichts\n\n\n",
      "last_modified": "2023-11-14T16:12:20+01:00"
    },
    {
      "path": "data_analysis.html",
      "title": "Basic Concepts in R: Data Loading To Data Visualization",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nENGLISH\nIn this blog, we will focus on analyzing a dataset of the best-selling movies between years XX and XX.\nStep 1: Loading the Dataset\nTo start, we need to load our dataset into R.\nWe will use a real dataset HollywoodMovies.csv (.csv is …), which includes movie names, budgets, earnings, and country of origin for movies released between the specified years 2007-2013.\nhow to call in a dataset in R ?\nlink to the Github Repo\n1. click on the download button to download this dataset\n\n2. Whats in this dataset? Lets call it in…. we use the function read.csv(), which “Reads a file in table format and creates a data frame from it, with cases corresponding to lines and variables to fields in the file.”\n\n\nmovies <- read.csv(\"/Users/tmechera/Dropbox/PostDoc_LMU/LMU_Statistik_Website/HollywoodMovies.csv\")\n\n\n3. Indexing and Creating Histograms\nNow, let’s create histograms to visually analyze the budget and net earnings of these movies. We will use the hist() function, which is a straightforward way to create histograms in R.\n\n\ncondition_budget = movies$Budget != \"NA\" \n\nhist_budget = hist(movies[condition_budget, 13], \n     main = \"Histogram of Movie Budgets\", \n     xlab = \"Budget in USD\", \n     col = \"lightblue\", border = \"darkblue\")\n\n\nhist_budget\n\n$breaks\n [1]   0  20  40  60  80 100 120 140 160 180 200 220 240 260 280 300\n\n$counts\n [1] 267 245 111  82  50  23  27  35  14  25   5   6   6   0   1\n\n$density\n [1] 1.488294e-02 1.365663e-02 6.187291e-03 4.570792e-03 2.787068e-03\n [6] 1.282051e-03 1.505017e-03 1.950948e-03 7.803790e-04 1.393534e-03\n[11] 2.787068e-04 3.344482e-04 3.344482e-04 0.000000e+00 5.574136e-05\n\n$mids\n [1]  10  30  50  70  90 110 130 150 170 190 210 230 250 270 290\n\n$xname\n[1] \"movies[condition_budget, 13]\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\n\n\ncondition_profit = movies$Profitability != \"NA\"\n\nhist_prof = hist(movies[condition_profit, 14], \n     main = \"Histogram of Movie Profitability\", \n     xlab = \"Profitability in USD\", \n     col = \"darkgreen\", border = \"green\")\n\n\nhist_prof\n\n$breaks\n [1]     0  1000  2000  3000  4000  5000  6000  7000  8000  9000 10000\n[12] 11000\n\n$counts\n [1] 858  20   9   4   1   1   2   0   0   0   1\n\n$density\n [1] 9.575893e-04 2.232143e-05 1.004464e-05 4.464286e-06 1.116071e-06\n [6] 1.116071e-06 2.232143e-06 0.000000e+00 0.000000e+00 0.000000e+00\n[11] 1.116071e-06\n\n$mids\n [1]   500  1500  2500  3500  4500  5500  6500  7500  8500  9500 10500\n\n$xname\n[1] \"movies[condition_profit, 14]\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\nCorrelation\nLets calculate the correlation between the budget of the movie and its profitability.\nWe will use the function cor.test(). which tests & measures the correlation between two variables.\nCorrelation is a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate)\ncorrelation\n\nlibrary(scales)\n\nmovies = na.omit(movies)\n\nmy_colors <- rainbow(length(unique(movies$Genre)))\n\nmy_colors_transparent <- sapply(my_colors, adjustcolor, alpha = 0.5)\n\n\ngenre_colors_transparent <- my_colors_transparent[as.numeric(factor(movies$Genre))]\n\n\n# Scatter plot\nplot(jitter(movies$Budget), jitter(movies$Profitability),\n     pch = 19, xlim = c(0, 450), \n     ylim = c(0, 10000), cex = 1, \n     xlab = \"Budget\", ylab = \"Profit\",\n     col = genre_colors_transparent)\n\n# Legend\nlegend(\"topright\", cex = 0.6, pch = 19,\n       legend = levels(factor(movies$Genre)),\n       col = factor(levels(factor(movies$Genre))))\n\n\n\n\n\ncor_budget_profit = cor.test(movies$Budget, movies$Profitability)\n\n\nWe can extract the correlation coefficient, which describes the strength of the relationship between two variables.\n\n\ncor_budget_profit$estimate\n\n       cor \n-0.1415343 \n\nIn (very) non-technical terms, p.value (supposably) is the indication of whether our results (in this case it is the correlation) is of any meaning.\n\n\ncor_budget_profit$p.value\n\n[1] 4.054042e-05\n\nA bonus task.\nwe will use the lm() function to create a linear line over our plot to represents the relationship between the budget and the profitability of the movies.\n\n\n# getting only values of the \ncondition_budget_Animation = movies$Genre == \"Action\"\n\nmovies_animation = movies[condition_budget_Animation,]\n\n# building the model\nmodel_budget_animation = lm(movies_animation$Profitability ~ movies_animation$Budget)\n\n# lets exclude \n\n# Scatter plot\nplot(jitter(movies_animation$Budget), jitter(movies_animation$Profitability),\n     pch = 19, \n     #xlim = c(0, 450), \n     #ylim = c(0, 10000), cex = 1, \n     xlab = \"Budget\", ylab = \"Profit\",\n     col = genre_colors_transparent)\n\nabline(model_budget_animation, col = 4, lwd = 3)\n\n\n\n\n\n\n",
      "last_modified": "2023-11-14T16:12:21+01:00"
    },
    {
      "path": "Decision_diagramm.html",
      "title": "How to decide which statistical test to run?",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\n\n\n\n\n\n\n",
      "last_modified": "2023-11-14T16:12:22+01:00"
    },
    {
      "path": "descriptive_stat_EN.html",
      "title": "Basic Concepts in R: Descriptive Statistics",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nIn this blog we will be focusing on two fundamental concepts that form the backbone of statistical analysis: Descriptive Statistics and Inferential Statistics.\nThese two serve distinct purposes in the world of data.\nDescriptive statistics\nDescriptive statistics involves summarizing and organizing data so it can be easily understood.\nEssentially, it’s about describing what’s observed in the dataset.\nThese statistics describe the basic features of a dataset, providing simple summaries about the sample and the measures.\nInferential Statistics\nInferential statistics involves using a sample of data to make inferences or predictions about a larger population.\nIt goes beyond merely describing the sample data to make generalizations about the population from which the sample was drawn.\n\nIn summary, while descriptive statistics are about summarizing and describing the characteristics of a dataset, inferential statistics uses that data to make predictions, decisions, and inferences about a larger population or future trends\n\nlets get our hands dirty….\nIn our upcoming analysis, we will focus on a meticulously curated dataset of the best-selling movies released between years 2007 and 2013.\nGet to know your dataset…\nStep 1: Loading the Dataset\nTo start, we need to load our dataset into R.\nWe will use a real dataset HollywoodMovies.csv (.csv is …), which includes movie names, budgets, earnings, and country of origin for movies released between the specified years 2007-2013.\nHow to call in a dataset in R ?\nlink to the Github Repo\nclick on the download button to download this dataset\n\nWhats in this dataset? Lets call it in…. we use the function read.csv(), which “Reads a file in table format and creates a data frame from it, with cases corresponding to lines and variables to fields in the file.”\n\n\nmovies <- read.csv(\"/Users/tmechera/Dropbox/PostDoc_LMU/LMU_Statistik_Website/HollywoodMovies.csv\")\n\n\nColumns of the Dataset:\nMovie: The title of the movie. This is the primary identifier for each entry in our dataset.\nLeadStudio: The production studio responsible for the movie. This helps in analyzing studio-wise performance and trends in the industry.\nRottenTomatoes: The Rotten Tomatoes score for each movie, representing the percentage of positive reviews from critics. It’s a crucial metric for understanding critical reception.\nAudienceScore: This score reflects the average rating given by the audience, usually on a scale of 1 to 100. It offers a direct insight into viewer preferences and acceptance.\nStory: A brief description or category of the movie’s storyline, which can be used to analyze the popularity of different types of stories.\nGenre: The genre(s) of the movie. This is important for understanding genre-specific trends and preferences.\nTheatersOpenWeek: The number of theaters in which the movie was released during its opening week. This indicates the scale of its initial release.\nOpeningWeekend: The box office collection of the movie during its opening weekend.\nA critical metric for gauging initial audience interest and marketing effectiveness.\nBOAvgOpenWeekend: Average box office earnings per theater during the opening weekend. This provides a more nuanced view of the movie’s initial success.\nDomesticGross: Total box office collection within the movie’s country of origin.\nForeignGross: Total box office collection in international markets.\nWorldGross: Combined global box office collections, a key indicator of a movie’s worldwide success.\nBudget: The estimated production and marketing budget of the movie. This helps in evaluating the financial planning behind the movie.\nProfitability: An index or calculation based on the movie’s earnings versus its budget, offering a direct measure of its financial success.\nOpenProfit: Profitability during the opening weekend, providing an early indicator of the movie’s commercial performance.\nYear: The year of the movie’s release, crucial for analyzing trends over time.\nWe will start with the descriptive statistics.\nStatistical Terms Definitions\nMean:\nDefinition: The mean, often referred to as the average, is a measure of central tendency. It is calculated by adding up all the values in a dataset and then dividing by the number of values.\nFormula: If you have a set of values X = {x1, x2, ..., xn}, the mean (μ) is calculated as μ = (Σxi) / n, where Σ denotes summation and n is the number of values in the dataset.\n\nMedian:\nDefinition: The median is the middle value in a dataset when the values are arranged in ascending or descending order. If there’s an odd number of observations, the median is the middle number. If there’s an even number of observations, the median is the average of the two middle numbers.\nCalculation: For a dataset X sorted in ascending order, if n is odd, the median is the value at position (n+1)/2. If n is even, the median is the average of the values at positions n/2 and n/2 + 1.\n\nMode:\nDefinition: The mode is the value that appears most frequently in a dataset. A dataset can have one mode (unimodal), more than one mode (bimodal or multimodal), or no mode at all.\nIdentification: In a dataset X, the mode is the value(s) that occurs with the highest frequency. Unlike mean and median, the mode can be used with nominal (categorical) data.\n\nRelative Frequency:\nDefinition: Relative frequency is a measure of the number of times a certain value or category appears in a dataset relative to the total number of values or observations. It is often expressed as a fraction or a percentage.\nCalculation: For a value x in a dataset, the relative frequency is calculated as f/n, where f is the frequency of x (the number of times x appears in the dataset) and n is the total number of observations in the dataset.\n\n\n\n#install.packages(\"modeest\")\nlibrary(modeest)  # for mode calculation\n# Calculate Mean\nmean_budget <- mean(movies$Budget, na.rm = TRUE)\n\n# Calculate Median\nmedian_budget <- median(movies$Budget, na.rm = TRUE)\n\n# Calculate Mode (using modeest package)\nmode_budget <- mfv(movies$Budget, na_rm = TRUE)\n\n# Display the results\nprint(paste(\"Mean Budget:\", mean_budget))\n\n[1] \"Mean Budget: 56.1171683389075\"\n\nprint(paste(\"Median Budget:\", median_budget))\n\n[1] \"Median Budget: 35\"\n\nprint(paste(\"Mode Budget:\", mode_budget))\n\n[1] \"Mode Budget: 20\"\n\nWe can additionally explore the frequencies within our data. This approach offers another method to characterize our dataset and its variables.\nAbsolute Frequency\nDefinition:\nAbsolute frequency refers to the count of the number of times a particular value or category occurs in a dataset.\n\nIt’s a simple tally of occurrences without any normalization or comparison to the total dataset size.\nExample:\nIf you have a dataset of movie genres and there are 30 action movies, the absolute frequency of action movies is 30.\n\nRelative Frequency\nDefinition:\nRelative frequency is the proportion or percentage of times a value or category appears in a dataset relative to the total number of observations. It’s calculated by dividing the absolute frequency of a category by the total number of data points.\n\nExample:\nIf you have a dataset of movie genres with a total of 100 movies, and 30 of them are action movies, the relative frequency of action movies is calculated as 30 / 100 = 0.3 or 30%.\n\n\n# Create a table of relative frequencies for a categorical variable like Genre\nfreq_genre <- table(movies$Genre)\nrelative_freq_genre = prop.table(freq_genre)\n\n\nprint(\"Relative Frequency of Genres:\")\n\n[1] \"Relative Frequency of Genres:\"\n\nprint(round(relative_freq_genre, digit = 2))\n\n\n                 Action   Adventure   Animation   Biography \n       0.29        0.17        0.03        0.05        0.01 \n     Comedy       Crime Documentary       Drama     Fantasy \n       0.18        0.02        0.01        0.11        0.01 \n     Horror     Musical     Mystery     Romance    Thriller \n       0.05        0.00        0.01        0.02        0.04 \n\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill for R Markdown at https://rstudio.github.io/distill.\n\n\n\n",
      "last_modified": "2023-11-14T16:12:24+01:00"
    },
    {
      "path": "descriptive_stat.html",
      "title": "Basic Concepts in R: Descriptive Statistics",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nIn diesem Blog konzentrieren wir uns auf zwei grundlegende Konzepte, die das Rückgrat der statistischen Analyse bilden: Deskriptive Statistik und Inferenzstatistik.\nDiese beiden dienen unterschiedlichen Zwecken.\nDeskriptive Statistik\nDeskriptive Statistik beinhaltet das Zusammenfassen und Organisieren von Daten, sodass sie leicht verstanden werden können.\nIm Wesentlichen geht es darum, das im Datensatz Beobachtete zu beschreiben.\nDiese Statistiken beschreiben die grundlegenden Merkmale eines Datensatzes und bieten einfache Zusammenfassungen über die Stichprobe und die Maße.\nInferenzstatistik\nInferenzstatistik beinhaltet die Verwendung einer Stichprobe von Daten, um Schlussfolgerungen oder Vorhersagen über eine größere Population zu treffen.\nSie geht über das bloße Beschreiben der Stichprobendaten hinaus, um Verallgemeinerungen über die Population zu machen, aus der die Stichprobe gezogen wurde.\n\nZusammenfassend kann man sagen, dass deskriptive Statistiken das Zusammenfassen und Beschreiben der Merkmale eines Datensatzes beinhalten, während Inferenzstatistiken diese Daten verwenden, um Vorhersagen, Entscheidungen und Schlussfolgerungen über eine größere Population oder zukünftige Trends zu treffen.\n\nLet’s get down to business….\nIn unserer bevorstehenden Analyse konzentrieren wir uns auf einen sorgfältig kuratierten Datensatz der meistverkauften Filme, die zwischen den Jahren 2007 und 2013 veröffentlicht wurden.\nLerne deinen Datensatz kennen…\nSchritt 1: Laden des Datensatzes\nZunächst müssen wir unseren Datensatz in R laden.\nWir verwenden einen echten Datensatz HollywoodMovies.csv (.csv ist …), der Filmtitel, Budgets, Einnahmen und Herkunftsländer für Filme enthält, die in den angegebenen Jahren 2007-2013 veröffentlicht wurden.\nWie ruft man einen Datensatz in R auf?\nLink zum Github Repo\n1. Klicke auf die Schaltfläche Herunterladen, um diesen Datensatz herunterzuladen\n\n2. Was ist in diesem Datensatz? Lass es uns aufrufen….\nWir verwenden die Funktion read.csv(), die “eine Datei im Tabellenformat liest und daraus einen Datenrahmen erstellt, wobei die Fälle den Zeilen und die Variablen den Feldern in der Datei entsprechen.”\n\n\nmovies <- read.csv(\"/Users/tmechera/Dropbox/PostDoc_LMU/LMU_Statistik_Website/HollywoodMovies.csv\")\n\n\nColumns of the Dataset:\nMovie: The title of the movie. This is the primary identifier for each entry in our dataset.\nLeadStudio: The production studio responsible for the movie. This helps in analyzing studio-wise performance and trends in the industry.\nRottenTomatoes: The Rotten Tomatoes score for each movie, representing the percentage of positive reviews from critics. It’s a crucial metric for understanding critical reception.\nAudienceScore: This score reflects the average rating given by the audience, usually on a scale of 1 to 100. It offers a direct insight into viewer preferences and acceptance.\nStory: A brief description or category of the movie’s storyline, which can be used to analyze the popularity of different types of stories.\nGenre: The genre(s) of the movie. This is important for understanding genre-specific trends and preferences.\nTheatersOpenWeek: The number of theaters in which the movie was released during its opening week. This indicates the scale of its initial release.\nOpeningWeekend: The box office collection of the movie during its opening weekend.\nA critical metric for gauging initial audience interest and marketing effectiveness.\nBOAvgOpenWeekend: Average box office earnings per theater during the opening weekend. This provides a more nuanced view of the movie’s initial success.\nDomesticGross: Total box office collection within the movie’s country of origin.\nForeignGross: Total box office collection in international markets.\nWorldGross: Combined global box office collections, a key indicator of a movie’s worldwide success.\nBudget: The estimated production and marketing budget of the movie. This helps in evaluating the financial planning behind the movie.\nProfitability: An index or calculation based on the movie’s earnings versus its budget, offering a direct measure of its financial success.\nOpenProfit: Profitability during the opening weekend, providing an early indicator of the movie’s commercial performance.\nYear: The year of the movie’s release, crucial for analyzing trends over time.\nWir beginnen mit der deskriptiven Statistik.\nDefinitionen statistischer Begriffe\nMittelwert:\nDefinition: Der Mittelwert, oft als Durchschnitt bezeichnet, ist ein Maß für die zentrale Tendenz. Er wird berechnet, indem alle Werte in einem Datensatz addiert und dann durch die Anzahl der Werte geteilt werden.\nFormel: Wenn Sie eine Menge von Werten X = {x1, x2, ..., xn} haben, wird der Mittelwert (μ) als μ = (Σxi) / n berechnet, wobei Σ die Summation darstellt und n die Anzahl der Werte im Datensatz ist.\n\nMedian:\nDefinition: Der Median ist der mittlere Wert in einem Datensatz, wenn die Werte in aufsteigender oder absteigender Reihenfolge angeordnet sind. Bei einer ungeraden Anzahl von Beobachtungen ist der Median die mittlere Zahl. Bei einer geraden Anzahl von Beobachtungen ist der Median der Durchschnitt der beiden mittleren Zahlen.\nBerechnung: Für einen in aufsteigender Reihenfolge sortierten Datensatz X, wenn n ungerade ist, ist der Median der Wert an der Position (n+1)/2. Wenn n gerade ist, ist der Median der Durchschnitt der Werte an den Positionen n/2 und n/2 + 1.\n\nModus:\nDefinition: Der Modus ist der Wert, der am häufigsten in einem Datensatz vorkommt. Ein Datensatz kann einen Modus (unimodal), mehr als einen Modus (bimodal oder multimodal) oder überhaupt keinen Modus haben.\nIdentifikation: In einem Datensatz X ist der Modus der Wert bzw. die Werte, der/die am häufigsten vorkommt/vorkommen. Im Gegensatz zu Mittelwert und Median kann der Modus auch bei nominellen (kategorischen) Daten verwendet werden.\n\n\n\n#install.packages(\"modeest\")\nlibrary(modeest)  # for mode calculation\n# Calculate Mean\nmean_budget <- mean(movies$Budget, na.rm = TRUE)\n\n# Calculate Median\nmedian_budget <- median(movies$Budget, na.rm = TRUE)\n\n# Calculate Mode (using modeest package)\nmode_budget <- mfv(movies$Budget, na_rm = TRUE)\n\n# Display the results\nprint(paste(\"Mean Budget:\", mean_budget))\n\n[1] \"Mean Budget: 56.1171683389075\"\n\nprint(paste(\"Median Budget:\", median_budget))\n\n[1] \"Median Budget: 35\"\n\nprint(paste(\"Mode Budget:\", mode_budget))\n\n[1] \"Mode Budget: 20\"\n\nWir können zusätzlich die Frequenzen innerhalb unserer Daten erforschen. Dieser Ansatz bietet eine weitere Methode, um unseren Datensatz und seine Variablen zu charakterisieren.\nAbsolute Häufigkeit\nDefinition:\nDie absolute Häufigkeit bezieht sich auf die Anzahl der Male, dass ein bestimmter Wert oder eine Kategorie in einem Datensatz vorkommt. Es handelt sich um eine einfache Zählung von Vorkommen ohne jegliche Normalisierung oder Vergleich zur Gesamtgröße des Datensatzes.\n\nBeispiel:\nWenn Sie einen Datensatz von Filmgenres haben und es gibt 30 Actionfilme, dann ist die absolute Häufigkeit der Actionfilme 30.\n\nRelative Häufigkeit\nDefinition:\nDie relative Häufigkeit ist der Anteil oder Prozentsatz der Male, dass ein Wert oder eine Kategorie in einem Datensatz im Verhältnis zur Gesamtzahl der Beobachtungen erscheint. Sie wird berechnet, indem die absolute Häufigkeit einer Kategorie durch die Gesamtzahl der Datenpunkte geteilt wird.\n\nBeispiel:\nWenn Sie einen Datensatz von Filmgenres mit insgesamt 100 Filmen haben und 30 davon sind Actionfilme, wird die relative Häufigkeit der Actionfilme als 30 / 100 = 0,3 oder 30% berechnet.\n\n\n\n# Erstellen Sie eine Tabelle der relativen Häufigkeiten für eine kategoriale Variable wie Genre\nfreq_genre <- table(movies$Genre)\nrelative_freq_genre = prop.table(freq_genre)\n\nprint(\"Relative Häufigkeit der Genres:\")\n\n[1] \"Relative Häufigkeit der Genres:\"\n\nprint(round(relative_freq_genre, digit = 2))\n\n\n                 Action   Adventure   Animation   Biography \n       0.29        0.17        0.03        0.05        0.01 \n     Comedy       Crime Documentary       Drama     Fantasy \n       0.18        0.02        0.01        0.11        0.01 \n     Horror     Musical     Mystery     Romance    Thriller \n       0.05        0.00        0.01        0.02        0.04 \n\nTasks\nTask 1: Extract the needed columns\n\n1. Extract the 'Name' and 'Profitability' Columns.\n\n2. Assign these two columns to the object \"profit_df\"\n\n3. Display the first few rows using head()\n\n\n\n\nTask 2: Create a Histogram of Profitability\n\n1. Visualize the distribution of the 'Profitability' column.\n\nUse the hist() function to create a histogram of the 'Profitability' column. \n\n-- Example can be seen in the blog on Indexing & Filtering\n\n\n\n\nTask 3: Calculate the Mean of Profitability\n\n1. Calculate the Mean of Profitability.\n\nUse the mean() function to calculate the mean of the 'Profitability' column. \n\nEnsuring to handle any missing values with the na.rm = TRUE argument inside the mean() fucntion.\n\n\n\n\nTask 4: Calculate the Median of Profitability\n\n1. Use the median() function to calculate the median of the 'Profitability' column, again handling missing values appropriately.\n\n\n\n\nTask 5: create a frequency table for this column\n\n\n\nBonus Task: Annotate the Histogram with Mean and Median\n\n1. Extract only the open profits that were <2000 from the original dataset (i.e., movies).\n\n2. Create a Histogramm of this data. \n\n3. Add the mean and median of this column to the histogram you created. \n\nInstructions: using the abline() function (to know exactly what it does, type ?abline in the console and read the help page on the bottom-left side of your Rstudio). This will visually represent how the mean and median relate to the distribution.\n\n\n\ncondition_Max2000 = movies$OpenProfit < 400\nmovies_Max2000 = movies[condition_Max2000,]\n  \nmean_openProfit = mean(movies_Max2000$OpenProfit, na.rm = TRUE)\nmedian_openProfit = median(movies_Max2000$OpenProfit, na.rm = TRUE)\n# Re-create the histogram\nhist(movies_Max2000$OpenProfit, \n     main=\"Histogram of Open Profitability (< 400) with Mean and Median\",\n     xlab=\"Profitability (<400)\", breaks = 100,\n     border = \"purple\", col = \"#C3B1E1\")\n\n# Add a vertical line for the mean\nabline(v = mean_openProfit, col = \"red\", lwd = 2)\n\n# Add a vertical line for the median\nabline(v = median_openProfit, col = \"black\", lwd = 2)\n\n\n\n\n\n\n",
      "last_modified": "2023-11-14T16:12:26+01:00"
    },
    {
      "path": "effectsize_lm.html",
      "title": "Effectsize(s) in Simple Linear Regression",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nA short introduction to effect sizes and their role in linear regression:\nIn linear regression analysis, effect sizes provide valuable information about the strength and direction of the relationship between the dependent variable and the independent variables. They quantify the magnitude of the effect of the independent variables on the outcome of interest.\nBefore we dive into the task of calculating the effect size for our linear models I must remind you of 4 assumptions we rely on for running linear models:\nLinearity: Linearity: The relationship between the dependent variable and the independent variables is assumed to be linear. This means that the effect of each independent variable on the dependent variable is constant and additive.\nNormality: The residuals (the differences between the observed values and the predicted values) are assumed to follow a normal distribution. This assumption implies that the errors are normally distributed with a mean of zero.\nHomoscedasticity: The variance of the errors (residuals) is assumed to be constant across all levels of the independent variables. In other words, the spread or dispersion of the residuals should be roughly the same for all values of the independent variables.\nNo outliers\nWe shall discuss these at the end of this blog so you’ll need to be patient 😃\n\n\n\nOK! \nBack\nto\nEffect Sizes\n\n\n\nOne commonly used effect size in linear regression is R-squared (R²). This is the effect size that you will find in your R-output when you calculate your linear regression in R (e.g., with the lm(AV ~ UV, data = datensatz))\nImportant facts aboput R squared\n\\(R^2\\) represents the proportion of variance in the dependent variable that can be explained by the independent variables in the regression model.\n\\(R^2\\) ranges from 0 to 1, with higher values indicating a stronger relationship between the variables.\n\\(R^2\\) provides an overall measure of the goodness of fit of the regression model and helps assess the amount of variation in the dependent variable that is accounted for by the independent variables.\nTo compute \\(R^2\\) for the regression model we need 3 pieces of information:\nThe OBSERVED values of the observed dependent variable \\(y\\) for each participant\n\\(\\bar{y}\\) which is the mean across all observed \\(\\y\\) values.\n\\(\\hat{y}\\) the predicted value of \\(y\\) given the regression line.\nHere is the formula to compute the \\(R^2\\):\n\\(R² = \\frac{\\hat{\\sigma_{\\mu_{i}}}^2}{\\hat{\\sigma_{tot}}^2} = QS_{residuen} / QS_{total}\\)\nAs always, we try to break it down to better understadn the componennts and their effect in the resulted value of \\(R^2\\).\n\\(QS_{residuen} = Σ(\\hat{y} - \\bar{y})²\\) SSR quantifies the amount of unexplained or residual variation in the dependent variable OR how far are the points from the regression line.\n\\(QS_{total} = Σ(yᵢ - \\bar{y})²\\) is a measure of the total variability in the dependent variable OR how far away the observed y values are from the .\nExample (which we worked through last week 😃):\nThe data set:\n\n  Height Weight\n1    170     60\n2    180     75\n3    167     59\n4    165     88\n5    191     91\n\nPLOTTING TIME !!\n\n\n\nMODELLING TIME!\nWe start with running the model with the (good, old known) unstandardized variables\n\n\nlm <-  lm(dat$Weight ~ dat$Height)\nsum_lm <-  summary(lm)\nsum_lm\n\n\nCall:\nlm(formula = dat$Weight ~ dat$Height)\n\nResiduals:\n      1       2       3       4       5 \n-11.382  -3.377 -10.284  20.115   4.928 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -47.5307   121.1507  -0.392    0.721\ndat$Height    0.6995     0.6928   1.010    0.387\n\nResidual standard error: 15.01 on 3 degrees of freedom\nMultiple R-squared:  0.2536,    Adjusted R-squared:  0.00482 \nF-statistic: 1.019 on 1 and 3 DF,  p-value: 0.387\n\nWe continue with computing the confidence interval for the \\(\\alpha\\) and \\(\\beta\\)\n\n\nconfint(lm)\n\n                  2.5 %     97.5 %\n(Intercept) -433.086166 338.024785\ndat$Height    -1.505342   2.904319\n\nGreat, here are the \\(y\\) values:\n\n\ny_values <- dat$Weight \ny_values\n\n[1] 60 75 59 88 91\n\nAnd the \\(\\bar{y}\\) value:\n\n\nmean_y <-  mean(dat$Weight)\nmean_y\n\n[1] 74.6\n\nAnd the \\(\\hat{y}\\) value:\n\n\npredicted_y <-  predict.lm(lm(dat$Weight ~ dat$Height))\npredicted_y\n\n       1        2        3        4        5 \n71.38235 78.37724 69.28389 67.88491 86.07161 \n\nWe are now ready to calculate the \\(\\color{red}{R^2}\\):\n\n\nqs_res <-  (predicted_y - mean_y)**2\nqs_res <-  sum(qs_res)\nqs_tot <- (y_values - mean_y)**2\nqs_tot <-  sum(qs_tot)\nr_squared <- qs_res / qs_tot\n\nr_squared\n\n[1] 0.2536148\n\nInterpretation of the \\(R^2\\) effect size is:\nIn a simple linear regression, the \\(R^2\\) provides insight into the proportion of variance in the dependent variable (AV) that can be explained by the independent variable (UV), indicating the model’s goodness of fit and the strength of the relationship between the variables.\nAnother way to calculaet effect size for the estimated slope in Simple Linear Regression Models.\nThe method relies on the standardized data.\nLets see what changes in the relationship between the AV and the UV if we scale both (spoiler: absolutely nothing….):\n\n\ng_standardized <-  \n  ggplot(data = dat, aes(x = scale(Height), y = scale(Weight)))+\n  geom_point(color = \"darkgreen\", size = 4)+\n  geom_smooth(method = \"lm\")+\n  theme_classic()\n\n\ncombined_plot <- grid.arrange(g_unstandardized, g_standardized, nrow = 1)\n\n\n\nThe Advantages to This Method:\nA standardized beta allows for a direct comparison of the relative importance of different predictor variables within a regression model. Since both the predictor and criterion variables are standardized, the magnitude of the standardized beta represents the change in the criterion variable in terms of standard deviations when the predictor variable changes by one standard deviation.\nUnit Independence: The standardized beta is not influenced by the specific units of measurement used for the predictor and criterion variables. This makes it easier to compare the effects of different variables, even if they are measured on different scales or have different units.\nGeneralizability: The effect size (\\(\\beta_{z}\\)) represents the magnitude of the relationship between the predictor and criterion variables in standardized units. This allows for better generalizability across different samples, populations, or studies, as it is not dependent on the specific measurement units used.\nComparability: Standardized betas and effect sizes can be compared across different studies or analyses, providing a standardized measure of the strength of the relationships. This comparability facilitates meta-analyses or synthesis of results from multiple studies.\nHere’s a Step-by-Step Guide to Salculating \\(\\beta_{z}\\):\n1) Compute the mean (\\(\\hat{x}\\)) of the SCALED independent variable (UV).\n\n\nscale_mean_x <-  mean(scale(dat$Height))\n\nscale_mean_x\n\n[1] 4.996004e-16\n\n2) Compute the mean (\\(\\hat{y}\\)) of the SCALED dependent variable (AV).\n\n\nscaled_mean_y <-  mean(scale(dat$Weight))\n\nscaled_mean_y\n\n[1] 4.066192e-16\n\n3) Calculate the covariance between scaled (standardized) AV and the scaled (standardized) UV using the formula:\n\\[cov(x,y) = \\frac{\\sum{(x_{i}-\\hat{x})\\times (y_{i}-\\hat{y})}}{n-1}\\]\n\n\ncov <-  sum((scale.default(dat$Height) - scale_mean_x) * (scale(dat$Weight) - scaled_mean_y))\ncov <-  cov/3\n\ncov\n\n[1] 0.6714691\n\n4) Calculate the variance of UV using the formula:\n\\[var(x) = \\frac{\\sum{(x_{i}-\\hat{x})^2}}{n-1}\\]\n\n\nvar_x <-  sum((scale(dat$Height) - scale_mean_x)^2)\n\nvar_x <-  var_x/3\n\nvar_x\n\n[1] 1.333333\n\n5) Calculate the standardized beta (\\(\\beta_{z}\\)):\n\n\nbeta_z <-  cov/var_x\nbeta_z\n\n[1] 0.5036018\n\n6) Compare your results with R-output:\n\n\nlm_z <-  lm(scale(dat$Weight) ~ scale(dat$Height))\n\nsummary(lm_z)\n\n\nCall:\nlm(formula = scale(dat$Weight) ~ scale(dat$Height))\n\nResiduals:\n      1       2       3       4       5 \n-0.7566 -0.2245 -0.6836  1.3371  0.3276 \nattr(,\"scaled:center\")\n[1] 74.6\nattr(,\"scaled:scale\")\n[1] 15.04\n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)\n(Intercept)       3.536e-17  4.461e-01    0.00    1.000\nscale(dat$Height) 5.036e-01  4.988e-01    1.01    0.387\n\nResidual standard error: 0.9976 on 3 degrees of freedom\nMultiple R-squared:  0.2536,    Adjusted R-squared:  0.00482 \nF-statistic: 1.019 on 1 and 3 DF,  p-value: 0.387\n\n7) Note that the \\(R^2\\) did not change either!\nWe continue with computing the confidence interval for the \\(\\alpha_{z}\\) and \\(\\beta_{z}\\)\n\n\nconfint(lm_z)\n\n                      2.5 %   97.5 %\n(Intercept)       -1.419799 1.419799\nscale(dat$Height) -1.083782 2.090986\n\n\n\n\nNow\n.\n.\n.\n\n\n\nWe know that since we have “collected” only 5 observations, this experiment might suffer from low power 🥵🦾… We even have a good reason to believe so because the relationship between height and weight may hold in reality…\nWe can determine the sample size we will need to achieve significant results\nBUT Careful! this method is no magic! 🪄 we will only be likely to obtain significant results only if there is, indeed, a relationship between height and weight)\nWe have to first calculate the\n\n\nrho_squared = coef(lm_z)[\"scale(dat$Height)\"]^2\n\nf_squared = rho_squared/(1-rho_squared)\n\nf_squared\n\nscale(dat$Height) \n        0.3397908 \n\nWe use the power calculation function pwr.f2.test(). It takes the following arguments: \\(u\\) which is the number of predictors we have in our model. In a simple linear regression we always ahve only 1, \\(f2\\) which is the effect size, f squared, which we calculated above. The other arguments are \\(sig.level\\) that determines the significance level (Type I error probability) and \\(power\\) represents the power we wish to achieve (1 minus Type II error probability)\n\n\nlibrary(pwr)\npwr.f2.test(u = 1, f2 = f_squared, sig.level = 0.005, power = 0.8)\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 41.23813\n             f2 = 0.3397908\n      sig.level = 0.005\n          power = 0.8\n\nInterpretation: Because 𝜈 = 𝑛 − 2 (we estimate 2 variables), we must add those 2 to know what the required sample (i.e., 𝑣 + 2). In our case, we will need 43 subjects (41. + ).\n\n\n\nThings\nto\nremember\nabout \\({R^2}\\)\n\n\n\nRanges between 0 and 1.\n1.1) A value closer to 1 indicates a stronger relationship between the independent (UV) and dependent (AV) variables, meaning that more of the variance in the dependent variable can be explained by the independent variable(s).\n1.2) Conversely, a value closer to 0 indicates a weaker relationship.\nThe estimated value \\(r^2\\) (which is the realised value of \\(R^2\\)) within the scope of the SLR (simple linear regression) is equivalent to the squared Pearson correlation.\n\\(r^2\\) is also referred to as the coefficient of determination.\nInterpretation of the standardized \\(\\beta_{z}\\): If the predictor variable (AV) increases by one standard deviation, the criterion variable (UV), on average, increases by \\(\\beta_{z}\\) standard deviations.\nBack to the summptions I mentioned at the beginning of this blog:\nLinearity. We will test this assumption with a simple scatter plot. To make the point here, I will create a new, richer data set.\n\n\n\nIn order to examine those assumptions, we will runa. linear regression (we will need it to exmain the distribution of the residuals).\n\n\nlibrary(gridExtra)\n\nlm_2 <-  lm(Weight ~ Height, data = dat_rich)\n \nsummary(lm_2)\n\n\nCall:\nlm(formula = Weight ~ Height, data = dat_rich)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.38928 -0.54807 -0.05402  0.52387  2.18491 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 12.79619    9.57534   1.336    0.185    \nHeight       0.52192    0.09574   5.451 3.74e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9153 on 98 degrees of freedom\nMultiple R-squared:  0.2327,    Adjusted R-squared:  0.2248 \nF-statistic: 29.72 on 1 and 98 DF,  p-value: 3.738e-07\n\nLets plot all the figures we need to judge of the Linear regression assumptions are met.\n\n\nlibrary(ggplot2)\nlibrary(gridExtra)\nstand_res <-  as.data.frame(rstandard(lm_2))\n\n\np_1 <-  ggplot(data = dat_rich, aes(x = Height, y = Weight)) +\n  geom_point(col = \"lightblue\", size = 3) +\n  geom_smooth(method = \"lm\", ) +\n  ggtitle(\"Linearity Assumption\")+\n  theme_classic()\n\n\n\npredicted <-  as.data.frame(predict(lm_2, interval = \"prediction\", level = .95))\npredicted$stand_res <-  stand_res$`rstandard(lm_2)`\n\np_2 <-   ggplot(data = predicted, aes(fit, stand_res))+\n  geom_point(col = \"lightblue\")+\n  ggtitle(\"Homoskedasticity Assumption\") +\n  geom_hline(yintercept = 0) + \n  xlab(\"Predicted Value of DV\") +\n  ylab(\"Standardized Residuals\") +\n  theme_classic()\n\ncook_dat <-  as.data.frame(round(cooks.distance(lm_2), digits = 3))\ncook_dat$id <-  cbind(1:sample_size)\n\np_3 <-   ggplot(data = cook_dat, aes(x = id, y = `round(cooks.distance(lm_2), digits = 3)`))+\n  geom_point(col = \"lightblue\")+\n  ggtitle(\"Outliers Assumption\") +\n  geom_hline(yintercept = 0.04) + \n  xlab(\"Observation IDs\") +\n  ylab(\"Cooks Distances\") +\n  theme_classic()\n\n# Arrange the plots in a grid\ncombined_plots <- grid.arrange(p_1, p_2, p_3, layout_matrix = rbind(c(1, NA), c(2, 3)), \n                               heights = c(3,3), widths = c(5, 5))\n\n\n\nA quick reminder of the way Cooks Distance is computed:\nCook’s D(i) = \\(\\frac{{\\Delta \\hat{y_i}^2}}{{p}} \\times \\frac{{h_{ii}}}{{(1 - h_{ii})}}\\)\nAnd lastly, (for the statistics nurds among us….😉), here is the model with the standardized coefficients:\n\n\nscaled_dat_rich <-  as.data.frame(round(scale(dat_rich), digits = 2))\nlm_z_2 <-  lm(scaled_dat_rich$Weight ~scaled_dat_rich$Height)\n\nsummary(lm_z_2)\n\n\nCall:\nlm(formula = scaled_dat_rich$Weight ~ scaled_dat_rich$Height)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.29424 -0.52910 -0.05363  0.50291  2.09918 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            0.000307   0.088005   0.003    0.997    \nscaled_dat_rich$Height 0.482464   0.088433   5.456 3.67e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.88 on 98 degrees of freedom\nMultiple R-squared:  0.233, Adjusted R-squared:  0.2251 \nF-statistic: 29.76 on 1 and 98 DF,  p-value: 3.667e-07\n\nconfint.lm(lm_z_2)\n\n                            2.5 %    97.5 %\n(Intercept)            -0.1743353 0.1749493\nscaled_dat_rich$Height  0.3069708 0.6579576\n\n\n\n\n",
      "last_modified": "2023-11-14T16:12:30+01:00"
    },
    {
      "path": "effectsize_mlr.html",
      "title": "Effectsize(s) and Collinearity in Multiple Linear Regression",
      "description": "Linear Regression\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nIn this blog we will examine the effect sizes in MLR and the correlation of predictors (a.k.a. multicollinearity)\nFirst, to set the stage, lets remind ourselves of the general model equation:\n\\[Y_{i} =𝛼+ \\beta_{1} \\times X_{i1} + \\beta_{2} \\times X_{i2} + ... + \\beta_{n} \\times X_{in} + \\epsilon_{i}  \\; \\;  \\; when \\; \\; \\;  \\epsilon \\sim 𝑁(0,\\sigma^2)\\]\n• \\(\\alpha\\): The predicted value, when all other predictors are qual 0.\n• \\(\\beta_{1}, \\beta_{2}, ..., \\beta_{k}\\): Slopes (Steigungsparameter, Regressionsgewichte), the expected change in the dependent variable (AV), when the predictor \\(X_{1}, X_{2} ... X_{k}\\) increase by one unit, while keeping all other variables contstant.\n• \\(\\epsilon_{i}\\) : Error term, describes the deviation of a randomly samples person \\(i\\) from their predicted value.\nIn multiple linear regression there are two types of effect sizes.\nThe effect sizes that describes the strength of the linear relationship between the all variables and the Dependent variable (AV). This effect size is called \\(\\rho^2\\) and its estimator is:\n\\[ \\hat{\\rho^2} = R^2 = \\frac{\\hat{\\sigma^2_{\\mu_{i}}}}{\\sigma^2_{total}} = \\frac{1/n \\times \\sum_{i=1}^{n} \\times (\\hat{Y_{i} - \\bar{Y}})^2}{1/n \\times \\sum_{i=1}^{n} \\times (Y_{i} - \\bar{Y})^2}\\]\nNote that the difference to simple linear regression, this effect size will refer to predicted value \\(\\hat{Y_{i}}\\) that are calculated based on MULTIPLE independent variables (mehrere UVs). That is the reason for its other known name multiple correlation, which is calculated as \\(\\sqrt{\\rho^2}\\).\nLets take an example:\nLet’s denote the dependent variable as \\(Y\\)(happiness with the statistics lecture) and the predictor variables as \\(X_1\\) (time spent learning) and \\(X_2\\) (level of subjective belief in math skills).\nOur model equation, is, therefore, \\[Y_{i} =𝛼+ \\beta_{time.prep} \\times X_{i1} + \\beta_{subj.belief} \\times X_{i2} + + \\epsilon_{i}  \\; \\;  \\; when \\; \\; \\;  \\epsilon \\sim 𝑁(0,\\sigma^2)\\]\nAnd here is this model’s R-Output:\n\n\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate sample data\nn <- 100  # Number of observations\ntime_spent <- runif(n, 0, 10)  # Time spent learning\nbelief_math <- runif(n, 1, 5)  # Subjective belief in math skills\nhappiness <- 2 + 0.5 * time_spent + 1.5 * belief_math + rnorm(n, 0, 1)  # Dependent variable (happiness)\n\n# Create a data frame\ndata <- data.frame(time_spent, belief_math, happiness)\n\n# Perform multiple linear regression\nmodel <- lm(happiness ~ time_spent + belief_math, data=data)\n\n# Print the regression summary\nsummary(model)\n\n\nCall:\nlm(formula = happiness ~ time_spent + belief_math, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8994 -0.6821 -0.1086  0.5749  3.3663 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.86662    0.35956   5.191 1.15e-06 ***\ntime_spent   0.50973    0.03457  14.746  < 2e-16 ***\nbelief_math  1.49259    0.09337  15.986  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9765 on 97 degrees of freedom\nMultiple R-squared:  0.8177,    Adjusted R-squared:  0.814 \nF-statistic: 217.6 on 2 and 97 DF,  p-value: < 2.2e-16\n\nNote the “Multiple R-squared” value, which is .8177.\nThis value is interpreted as the percentage of variance of “happiness” that can be explained by “time_spent” and by “belief_math”.\nAnd here is a plot of this data & model:\n\n\n\nThe effect sizes that describes the strength of the linear relationship between the every single variable and the Dependent variable (AV). A standardized effect size is called \\(\\beta_{zj}\\) and is similar to the \\(\\beta_{z}\\) discussed in the blog “Effectsize(s) in Simple Linear Regression”.\nTo calculate this effect size, one has to, first, z-Standardise all predictor variables (UVs) an the dependent variable (AV) and then calculate the model again.\n\n\nCall:\nlm(formula = scale.happiness. ~ scale.time_spent. + scale.belief_math., \n    data = data_scale)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.83899 -0.30131 -0.04796  0.25395  1.48692 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        1.465e-16  4.313e-02    0.00        1    \nscale.time_spent.  6.417e-01  4.351e-02   14.75   <2e-16 ***\nscale.belief_math. 6.956e-01  4.351e-02   15.99   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4313 on 97 degrees of freedom\nMultiple R-squared:  0.8177,    Adjusted R-squared:  0.814 \nF-statistic: 217.6 on 2 and 97 DF,  p-value: < 2.2e-16\n\nHow shoul I understand these values?\nIf a student will increase their time spent for studying by one standard deviation, holding their belief in math skills variable constant, their happiness with statistics class will increase on average by 0.6417 standard deviations\nMulticollinearity\nMulticollinearity refers to a situation in multiple linear regression where two or more predictor variables are highly correlated with each other.\nIt indicates a strong linear relationship between the predictor variables, which can cause issues in the regression analysis.\nIn the context of the example with the variables “time spent learning” and “belief in math skills” as predictor variables for happiness with the statistics lecture, multicollinearity would occur if these two variables are highly correlated. That is, for example, when those students, who have a strong belief in their math skills will also very likely to spend much time preparing for this lecture and vice versa for those students with less belief in their math skills.\nFor example, let’s consider a dataset with 100 observations. Here are the correlation coefficients between the predictor variables:\n\n            time_spent belief_math happiness\ntime_spent   1.0000000  -0.0872707 0.5809671\nbelief_math -0.0872707   1.0000000 0.6396235\nhappiness    0.5809671   0.6396235 1.0000000\n\nThe dangers in running a multiple (or any) linear regression without checking for multicoliniarity\nAffects the estimated standard errors for the \\(\\beta\\) (i.e., slopes, Steigungsparameter) negatively\nAs a result, multicollinearity also has a negative impact on the confidence intervals and hypothesis tests for the slope parameters:\n• Larger confidence intervals\n• Lower power of hypothesis tests\nThe parameter that are NOT affected by multicoliniarity are:\n• Confidence intervals for \\(\\rho^2\\)\n• Omnibustest for multiple linear regression\nWhat to do to examine if we have a multicoliniarity in our model?\nVariance Inflation Factor (VIF) - VIF helps us understand how much the variance (or uncertainty) of one predictor variable is increased due to its correlation with other predictor variables in the model.\nA high VIF value indicates a high degree of multicollinearity and suggests that the variable’s contribution to the regression model may be unreliable.\nThe formula to compute VIF is\n\\[VIF_{j} = \\frac{1}{1-\\color{red}{r_{j}^2}} \\; \\;  \\; when \\; \\; \\;  j =  time \\;spent, belief \\;math  \\; skills\\]\n\\[\\color{red}{r_{j}^2} = the \\; estimated \\; variance \\; explained \\; by \\; one \\; predictor \\; through \\; all \\; other \\; predictor \\; variables\\]\nIn other words, the variance inflation factor (VIF) of a predictor (e.g., time spent on studying) indicates **by what factor the variance of the estimated coefficient \\(\\hat{beta}_{j}\\) (e.g., \\(\\hat{beta}_{time \\; spent \\; studying}\\)) is larger relative to the other predictor .\n• The larger the VIF of a regression weight, the worse the estimator for that particular regression weight.\n\n\n\nImportant\nTo \nNote\n!!\n\n\n\nThe negative effects of collinearity can be mitigated by large sample sizes through decreasing the standard error (thereby increasing the precision of the estimated parameters) of the variables.\nHere is an example:\n\n\nlibrary(car)  # for the vif() function\n\n# Set the seed for reproducibility\nset.seed(123)\n\n# Generate a sample dataset with correlated predictor variables\nn_small <- 50  # Small sample size\nn_large <- 5000  # Large sample size\n\n# Generate correlated predictor variables\nx1 <- rnorm(n_large)\nx2 <- 0.8 * x1 + rnorm(n_large, sd = 0.2)\n\n# Generate the response variable\ny <- 2 * x1 + 3 * x2 + rnorm(n_large)\n\n# Function to calculate VIF for a given dataset and predictor variables\ncalculate_vif <- function(data, predictors) {\n  vif_values <- vif(lm(as.formula(paste(\"y ~\", paste(predictors, collapse = \"+\"))), data = data))\n  return(vif_values)\n}\n\n# Calculate VIF for the small sample size\npredictors <- c(\"x1\", \"x2\")\nvif_small <- calculate_vif(data.frame(x1[1:n_small], x2[1:n_small], y[1:n_small]), predictors)\ncat(\"VIF values (small sample size):\", vif_small, \"\\n\")\n\nVIF values (small sample size): 16.69219 16.69219 \n\n# Calculate VIF for the large sample size\nvif_large <- calculate_vif(data.frame(x1, x2, y), predictors)\ncat(\"VIF values (large sample size):\", vif_large, \"\\n\")\n\nVIF values (large sample size): 16.69219 16.69219 \n\n# Fit linear regression models with different sample sizes\nmodel_small <- lm(y[1:n_small] ~ x1[1:n_small] + x2[1:n_small])\nmodel_large <- lm(y ~ x1 + x2)\n\n# Calculate standard errors of the coefficients\nse_small <- summary(model_small)$coefficients[, \"Std. Error\"]\nse_large <- summary(model_large)$coefficients[, \"Std. Error\"]\n\n# Print the standard errors\ncat(\"Standard errors (small sample size):\", se_small, \"\\n\")\n\nStandard errors (small sample size): 0.1470152 0.6560189 0.7674668 \n\ncat(\"Standard errors (large sample size):\", se_large, \"\\n\")\n\nStandard errors (large sample size): 0.01415777 0.05816415 0.07060016 \n\nWith a larger sample size, the estimates of the regression coefficients tend to have lower standard errors, which means they become more precise. Consequently, the effects of multicollinearity on the significance and interpretation of the coefficients may become less pronounced.\n\n\n\n",
      "last_modified": "2023-11-14T16:12:34+01:00"
    },
    {
      "path": "functions.html",
      "title": "Basic Concepts in R: Functions",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nIn diesem Blog werden wir uns damit beschäftigen, wie wir Objekte, wie z.B. Vektoren, mit Funktionen manipulieren und verändern können.\nFunktionen in R sind wie ein persönlicher Assistent in der Welt des Codes:\n–> Man delegiert eine Aufgabe, indem man ihnen spezifische Anweisungen (Argumente) gibt, und sie liefern das fertige Produkt (Ausgabe), wodurch Ihnen die Mühe erspart bleibt, die eintönige Arbeit selbst zu erledigen.\nIn der folgenden Diskussion werden wir uns mit den spezifischen Funktionen befassen, die in R verfügbar sind, um Objekte effizient zu manipulieren.\n\nFunktionen\n\nFür die Statistik benötigen wir vor allem Funktionen, die Daten transformieren (z.B., Zuordnen von Zeichenketten zu bestimmten Werten, Erstellen neuer Variablen, Laden von Datensätzen, Speichern von Datensätzen usw.) und Funktionen, die statistische Berechnungen durchführen (z.B., Berechnung von Mittelwerten, Durchführung von Regressionen usw.).\n\n\nHier ist ein Beispiel dafür, wie Funktionen die Form und die Struktur Ihrer Daten ändern können:\n\nConsider the “jokeGenerator” function:\n\n\njokeGenerator <- function(name) {\n  return(paste(\"Why did\", name, \"get scared of the dataset? Because it had too many bytes!\"))\n}\n\njokeGenerator(\"the computer\")\n\n[1] \"Why did the computer get scared of the dataset? Because it had too many bytes!\"\n\nAnd a bad joke…..\n\n\nschlechterWitzGenerator <- function(beruf) {\n  return(paste(\"Was sagt ein\", beruf, \"wenn er in R programmiert? 'Ich glaube, ich habe den Faden verloren!'\"))\n}\n\nschlechterWitzGenerator(\"Statistiker\")\n\n[1] \"Was sagt ein Statistiker wenn er in R programmiert? 'Ich glaube, ich habe den Faden verloren!'\"\n\nlets get down to business!\nWir werden in R die Funktion “mean” verwenden, um den Durchschnitt oder arithmetischen Mittelwert einer Menge numerischer Werte zu berechnen.\nSie summiert alle Zahlen in einem Datensatz auf und teilt dann diese Summe durch die Gesamtanzahl der Zahlen.\nDadurch erhalten Sie einen einzigen Wert, der die zentrale Tendenz oder den typischen Wert der Daten darstellt.\n\n\n# ein vector\nzahlen = c(15, 23, 89, 66, 120)\n\n# Den Mittelwert berechnen\ndurchschnitt = mean(zahlen)\n\n# Das Ergebnis ausgeben\nprint(durchschnitt)\n\n[1] 62.6\n\nIn diesem Beispiel wird die Mittelwert-Funktion die Zahlen (5 + 10 + 15 + 20 + 25) addieren und die Summe durch die Gesamtanzahl (in diesem Fall 5) teilen, was zu einem Durchschnitt von 15 führt.\nOben haben wir gesehen, wie man die eingebaute Funktion mean() in R verwendet.\nWenn wir jedoch unsere eigene Funktion erstellen möchten, die genau dasselbe tut, müssen wir die folgenden Schritte ausführen:\nZuerst müssen wir eine Funktion definieren, indem wir function() verwenden, gefolgt von den Argumenten, die die Funktion akzeptiert.\nIn unserem Fall erwartet die Funktion einen Vektor als Argument.\nDann schreiben wir den Code, der innerhalb der Funktion ausgeführt wird, um den Durchschnitt zu berechnen.\nSchließlich geben wir das Ergebnis mit return() zurück.\nAuf diese Weise haben wir unsere eigene Funktion zur Berechnung des Durchschnitts erstellt, die genauso funktioniert wie mean(), aber wir haben die Kontrolle über den Prozess.\n\n\ncalculate_mean <- function(vector) {\n  # Calculate the mean\n  result <- sum(vector) / length(vector)\n  \n  return(result)\n}\n\n\n\nHier sind einige Beispiele dafür, wie Funktionen Daten generieren können:\n\nFunktionen können auch Daten und Objekte generieren. Zum Beispiel gibt es Funktionen, die Zufallszahlen erstellen.\nDie seq()-Funktion erzeugt eine Sequenz zwischen zwei Zahlen.\nSie die folgende Argumente:\n“from” (von): Dieses Argument gibt den Startwert der Sequenz an. Es ist die erste Zahl in der generierten Sequenz.\n“to” (bis): Hiermit legen Sie den Endwert der Sequenz fest. Dies ist die letzte Zahl in der generierten Sequenz.\n“by” (Schrittweite): Die Schrittweite bestimmt den Abstand zwischen den aufeinanderfolgenden Zahlen in der Sequenz. Wenn nicht angegeben, wird die Standard-Schrittweite von 1 verwendet.\n\n\n\nseq(from = 1, to = 10, by = 1)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(from = 1, to = 10, length.out = 5)\n\n[1]  1.00  3.25  5.50  7.75 10.00\n\nDie rnorm() Funktion zieht zufällig 20 Werte aus der Normalverteilung\nmit einem Mittelwert von 0 und einer Standardabweichung von 1.\n\n\nrnorm(n = 10, mean = 0, sd = 1) \n\n [1] -1.3538489 -0.5793773 -0.8610442  0.9726783  0.6191458  1.3854457\n [7] -1.4872704  0.6391927  0.3747349  0.3701879\n\nDie Funktion hist() erstellt ein Histogramm aus dem Vektor normalverteilter Variablen. Dies ist ein Beispiel dafür, wie Funktionen auch Grafiken aus Objekten generieren können.\n\n\nnormalverteilt = rnorm(n = 100, mean = 100, sd = 15)\nhist(normalverteilt) \n\n\n\nSequenzen generieren.\nEine Sequenz in R ist eine aufeinanderfolgende Abfolge von Zahlen. Sie kann verwendet werden, um eine Liste von Zahlen zu erstellen, die in einer bestimmten Reihenfolge angeordnet sind. Sequenzen sind nützlich, wenn Sie schnell eine Abfolge von Zahlen generieren müssen, ohne sie manuell eingeben zu müssen.\n\n\n# Eine Sequenz von Zahlen generieren\nsequenz <- 1:10\nprint(sequenz)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n# Eine Sequenz mit festgelegtem Schritt erstellen\nsequenz_schritt <- seq(1, 10, by = 2)\nprint(sequenz_schritt)\n\n[1] 1 3 5 7 9\n\nEingebaute Funktionen verwenden\n\n\n# Die Quadratwurzel berechnen\nsqrt_ergebnis <- sqrt(25)\nprint(sqrt_ergebnis)\n\n[1] 5\n\n# Den absoluten Wert berechnen\nabsolutwert <- abs(-10)\nprint(absolutwert)\n\n[1] 10\n\n# Das Maximum und Minimum finden\nmaximalwert <- max(3, 7, 1, 8, 5)\nminimalwert <- min(3, 7, 1, 8, 5)\nprint(maximalwert)\n\n[1] 8\n\nprint(minimalwert)\n\n[1] 1\n\n\nHomework\n\nNotenberechnung\nErstelle einen numerischen Vektor mit dem Namen ‘noten’ für zehn Schüler.\nWeise jeder Note eine Punktzahl von 1 bis 5 zu, wobei 1 die beste Note ist und 5 die schlechteste.\nBerechne den Durchschnitt dieser Noten und gib die Ausgabe (Output) aus (use the print() function).\n\n\n\nHinweise:\nVerwende die Funktion c(), um den Vektor noten zu erstellen.\nUm den Durchschnitt zu berechnen, verwende die Funktion mean(vector).\nGib den Durchschnitt mit print().\nBenotung von Schülern\nErstelle einen Vektor mit den Namen von fünf Schülern in Ihrer Klasse. Nenne den Vektor ‘namen’.\n\n\n\nKlassenanwesenheit\nErstelle einen logischen Vektor mit dem Namen ‘anwesenheit’, der anzeigt, welche Schüler an einem bestimmten Tag in der Schule anwesend waren (TRUE) und welche abwesend waren (FALSE).\n\n\n\nVerwende die cbind()-Funktion, um alle von dir erstellten Vektoren zu kombinieren. Stelle sicher, dass du alle Vektoren in einer einzigen Datenstruktur zusammenführen, um die gewünschten Ergebnisse zu erhalten\nNotendiagramm erstellen\nVerwende den Vektor noten aus Aufgabe 1, um ein Balkendiagramm zu erstellen, das die Verteilung der Noten in den verschiedenen Fächern zeigt.\n\n\n\nHinweise:\nVerwende die Funktion plot(), um ein Streudiagram zu erstellen. Gib den Vektor noten als Argument an, z.B.: barplot(noten).\nDie Funktion plot() hat die folgende Argumente:\nx: Legt die Daten fest, die auf der x-Achse dargestellt werden sollen.\ny: Legt die Daten fest, die auf der y-Achse dargestellt werden sollen.\ntype: Bestimmt den Typ des Diagramms, das erstellt werden soll, z.B. “p” für Punkte, “l” für Linien, “b” für beides (Punkte und Linien), “h” für Hochdichtelinien usw.\nmain: Ermöglicht das Festlegen des Haupttitels des Diagramms.\nxlab: Legt die Beschriftung für die x-Achse fest.\nylab: Legt die Beschriftung für die y-Achse fest.\nxlim: Setzt die Begrenzungen für die x-Achse.\nylim: Setzt die Begrenzungen für die y-Achse.\n\n\n\n",
      "last_modified": "2023-11-14T16:12:35+01:00"
    },
    {
      "path": "index.html",
      "title": "Willkommen!",
      "author": [],
      "contents": "\n\n          \n          \n          LMU Statistik\n          \n          \n          Home\n          \n          \n          Statistik I\n           \n          ▾\n          \n          \n          Basics in R - Download R & R Studio\n          Basics in R - Create Project\n          Basics in R - Variables\n          Basics in R - Variables II\n          Basics in R - Indexing & Filtering\n          Basics in R - Descriptive Statistics\n          \n          \n          \n          \n          Statistik II\n           \n          ▾\n          \n          \n          Q & A - Summary\n          ANOVA - I\n          ANOVA - II\n          Simple Linear Models\n          Multiple Linear Models\n          Multiple Linear Models - Part II\n          Simple Linear Models: Effectsize\n          Multiple Linear Models: Effectsize\n          Statistical Tests - Decision Diagramm\n          \n          \n          ☰\n          \n          \n      \n        \n          Willkommen!\n          \n            \n                \n                  \n                    Moodle\n                  \n                \n              \n                            \n                \n                  \n                    Tehilla’s Email\n                  \n                \n              \n                          \n        \n\n        \n          \n      \n      \n        \n          This website is dedicated ❤️ to all the AMAZING\n          Schulpsychologie students, who will sit/sat a\n          Statistik exam @LMU!\n          Enjoy your journey to understanding the topics we cover in\n          the courses Statistik I and Statistik II!\n          Im very much looking forward to hear both about what you\n          think about this website and about our seminar!\n          You can 📧 me at any time (see button above)\n        \n      \n    \n\n    \n      \n        \n          \n            \n              \n            \n              Willkommen!\n            \n            \n              \n                \n                                    \n                    \n                      Moodle\n                    \n                  \n                                    \n                    \n                      Tehilla’s Email\n                    \n                  \n                                  \n              \n            \n            \n              This website is dedicated ❤️ to all the AMAZING\n              Schulpsychologie students, who will sit/sat a\n              Statistik exam @LMU!\n              Enjoy your journey to understanding the topics we cover\n              in the courses Statistik I and Statistik II!\n              Im very much looking forward to hear both about what\n              you think about this website and about our seminar!\n              You can 📧 me at any time (see button above)\n            \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2023-11-14T16:12:37+01:00"
    },
    {
      "path": "indexing_filtering.html",
      "title": "Basic Concepts in R: Indexing & Filtering",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nIndexing in R ist ein wichtiges Konzept, das es uns ermöglicht, auf bestimmte Elemente oder Teilmengen von Daten in Vektoren, Matrizen und Datenrahmen zuzugreifen.\nIn diesem Blog werden wir die Indexing anhand von Beispielen für jede dieser Datenstrukturen (i.e., Objekte) behandeln.\nUm unser Leben zu erleichtern und den Inhalt dieses Blogs verständlicher zu machen, werden wir ein alltägliches Beispiel verwenden. Das Beispiel basiert auf einem Datensatz (keine Sorge, es ist ein erfundener, den ich zum Zwecke dieses Blogs erstellt habe), der eine Liste von Kleidungsstücken enthält. Hier konzentrieren wir uns auf Jeans, die verschiedene Attribute aufweisen, über die wir indexieren können.\nIn unserem Kleider-Datensatz können wir die “perfekte” Jeans suchen. Dafür wereden wir Indexierungsmethoden in R verwenden, um unsere Suche zu verfeinern und genau das zu finden, ohne jeden Eintrag einzeln durchsehen zu müssen.\nDurch die Verwendung von Indexierungsoperationen können wir beispielsweise alle Jeans einer bestimmten Preis herausfiltern.\nWir könnten auch komplexere Abfragen durchführen, wie das Suchen aller Jeans, die weniger als 50 Euro kosten.\nIm Verlauf dieses Blogs werden wir sehen, wie solche Indexierungen in R effizient durchgeführt werden können, sodass Sie die volle Kontrolle über Ihre Daten und deren Abfragen haben.\nBonus\nZusätzlich, als Bonus und um zu veranschaulichen, wie wir vom Filtern und Indexieren zur Visualisierung übergehen, werden wir auch die Funktion hist() verwenden, um eine Histogramm-Grafik zu erstellen.\nDies wird nicht nur unser Datenverständnis vertiefen, sondern uns auch ermöglichen, unsere Daten zusammenzufassen und visuell darzustellen.\nEin Histogramm ist eine großartige Möglichkeit, die Verteilung der Daten zu zeigen. Beispielsweise könnten wir ein Histogramm der Jeanspreise erstellen, um zu sehen, wie viele Jeans in verschiedene Preiskategorien fallen.\nDies kann besonders hilfreich sein, um Muster in unseren Daten schnell zu erkennen, wie etwa das Vorherrschen bestimmter Preisbereiche oder das Erkennen, ob es Ausreißer gibt.\nIndem wir die hist() Funktion auf unseren gefilterten Datensatz anwenden – zum Beispiel, nachdem wir alle Jeans einer bestimmten Marke ausgewählt haben – können wir die Verteilung der Attribute wie Preis oder Größe innerhalb dieser Auswahl betrachten. Dies bietet einen direkten Einblick in die Charakteristiken unserer Daten und unterstützt uns dabei, bessere Entscheidungen und Analysen zu treffen.\nIndexing a Vector:\nIn R ist ein Vektor ein eindimensionales Array von Datenelementen (e.g., x = 3). Sie können einen Vektor ‘indizieren’, um auf bestimmte Elemente zuzugreifen, indem Sie eckige Klammern [] verwenden. Die Indexierung von Vektoren beginnt bei 1 (In anderen Programmiersprachen beginnt die ‘Indexing’ bei 0).\nHere’s how to index a vector:\n\n\n# Create a vector\nmy_vector = c(10, 20, 30, 40, 50)\n\n# Access the first element\nelement_1 = my_vector[1]\nprint(element_1)  # Output: 10\n\n[1] 10\n\n# Access the third element\nelement_3 = my_vector[3]\nprint(element_3)  # Output: 30\n\n[1] 30\n\nDu kannst auch vektorisierte Indexing verwenden, um auf mehrere Elemente gleichzeitig zuzugreifen:\n\n\n# Access the first three elements\nelements_1_to_3 = my_vector[1:3]\nprint(elements_1_to_3)  # Output: 10 20 30\n\n[1] 10 20 30\n\nIndexing a Matrix:\nEine Matrix ist ein zweidimensionales Array mit Zeilen und Spalten.\nDu kannst Elemente einer Matrix ‘indizieren’, indem du die eckige Klammern mit Zeilen- und Spaltenindizes verwendest, die durch ein Komma getrennt sind.\n\nSo indizierst du eine Matrix:\n\n\n# Create a matrix\nmy_matrix = matrix(1:9, nrow = 3, ncol = 3)\n\n# Access the element in the second row and third column\nelement_2_3 = my_matrix[2, 3]\nprint(element_2_3)  # Output: 6\n\n[1] 8\n\nSo ‘indizierst’ du eine Matrix:\n\n\n# Access the second row\nrow_2 = my_matrix[2, ]\nprint(row_2)  # Output: 4 5 6\n\n[1] 2 5 8\n\n# Access the first column\ncol_1 = my_matrix[, 1]\nprint(col_1)  # Output: 1 2 3\n\n[1] 1 2 3\n\nIndexing a DataFrame:\nEin Data Frame ist eine zweidimensionale Datenstruktur in R, ähnlich wie eine Tabelle in einer Datenbank oder ein Excel Spreadsheet.\nDu kannst ein Data Frame mit eckigen Klammern ‘indizieren’ und dabei die Zeilen- und Spaltennamen oder -indizes angeben.\nSo wird ein Data Frame ‘indiziert’:\n\n\n# Create a data frame\nmy_dataframe = data.frame(\n  ItemType = rep(c(\"jeans\"), 100),\n  Price = round(rnorm(100, 60, 20), digits = 0),\n  Cut = rep(c(\"boot\", \"7/8\", \"skinny\", \"wide leg\", \"boyfriend\"), 20)\n)\n\n# get a glimpse of the data\nhead(my_dataframe)\n\n  ItemType Price       Cut\n1    jeans    57      boot\n2    jeans    26       7/8\n3    jeans    74    skinny\n4    jeans    62  wide leg\n5    jeans   117 boyfriend\n6    jeans    66      boot\n\n# Access the second row\nrow_2 = my_dataframe[2, ]\nprint(row_2)\n\n  ItemType Price Cut\n2    jeans    26 7/8\n\n# Access the entire first column\ncol_2 = my_dataframe[ ,2]\ncol_2[1:50]\n\n [1]  57  26  74  62 117  66  76 111  44  50  37  45  59  41  58  70\n[17]  86  13  35  71 109  37  40  91  31  86  35  15  48  89  61  59\n[33]  67  57 106  74  66  38  53  33  45  45  41  44  57  81  89  53\n[49]  91  83\n\n# Access the 'Age' column using column name\nallCuts = my_dataframe$Cut\nallCuts[1:20]  # Output: 25 30 22\n\n [1] \"boot\"      \"7/8\"       \"skinny\"    \"wide leg\"  \"boyfriend\"\n [6] \"boot\"      \"7/8\"       \"skinny\"    \"wide leg\"  \"boyfriend\"\n[11] \"boot\"      \"7/8\"       \"skinny\"    \"wide leg\"  \"boyfriend\"\n[16] \"boot\"      \"7/8\"       \"skinny\"    \"wide leg\"  \"boyfriend\"\n\n# get the type of the Item Type \ntypeof(allCuts)\n\n[1] \"character\"\n\n# get the categories within Item Type\nlevels(allCuts) # returns NUll....why?\n\nNULL\n\n# turn the variable into factor \nmy_dataframe$Cut = factor(my_dataframe$Cut)\n\n\n# get the categories within Item Type\nlevels(my_dataframe$Cut)\n\n[1] \"7/8\"       \"boot\"      \"boyfriend\" \"skinny\"    \"wide leg\" \n\nIndexing by condition\nAngenommen, du hast einen Data Frame namens ‘my_dataframe’ wie folgt:\n\n\ncondition_boot = my_dataframe$Cut == \"boot\"\n\nmy_dataframe_boot = my_dataframe[condition_boot,]\n\n#get a glimpse of the new dataframe using the head(). Use the ? mark to learn about the function head (or any other function)\nhead(my_dataframe_boot, n = 20) # n stands for the number of top observations you wish to see out of the dataframe \n\n   ItemType Price  Cut\n1     jeans    57 boot\n6     jeans    66 boot\n11    jeans    37 boot\n16    jeans    70 boot\n21    jeans   109 boot\n26    jeans    86 boot\n31    jeans    61 boot\n36    jeans    74 boot\n41    jeans    45 boot\n46    jeans    81 boot\n51    jeans    89 boot\n56    jeans    38 boot\n61    jeans    82 boot\n66    jeans    56 boot\n71    jeans    80 boot\n76    jeans    49 boot\n81    jeans   101 boot\n86    jeans    59 boot\n91    jeans    61 boot\n96    jeans    71 boot\n\n# bonus...lets see the distribution of the prices of boot cuts \nhist(my_dataframe[condition_boot, 2],\n      col = \"lightblue\", border = \"#227FCE\",\n     main = paste(\"Histogram of Boot-Cut Jeans\"),\n     xlab = \"prices of wide-leg jeans\")\n\n\n\nand a bonus filtering task\n\n\n# bonus...lets see the distribution of the prices of wide leg cuts looks similar....\ncondition_wide = my_dataframe$Cut == \"wide leg\"\n\nhist(my_dataframe[condition_wide, 2],\n     col = \"lightblue\", border = \"black\",\n     main = paste(\"Histogram of Wide-leg Jeans\"),\n     xlab = \"prices of wide-leg jeans\"\n     )\n\n\n\nIndexing/Filtering by multiple conditions\nStellen wir uns vor, wir wollen Zeilen für alle “Wide leg” Jeans extrahieren UND die mehr als 39 EUR kosten. Hier ist ein Beispiel, wie du den Datenrahmen einrichten und die Operation durchführen könntest:\nWir können mehrerer Bedingungen kombinieren mithilfe von logischen Operatoren wie & (UND) und | (ODER).\nZum Beispiel,\nwenn du Zeilen filtern möchtest, in denen ‘price’ größer als 39 UND ein “wide leg” cut ist.\n\n\n# condition that will select both wide cuts and those that cost more than 59Euro\ncondition_wide_39 = my_dataframe$Cut == \"wide leg\" & my_dataframe$Price > 39\nhead(condition_wide_39)\n\n[1] FALSE FALSE FALSE  TRUE FALSE FALSE\n\ndata_wide_39 = my_dataframe[condition_wide_39,]\n\nhist(data_wide_39[,2],\n     col = \"lightblue\", border = \"#227FCE\",\n     main = paste(\"Histogram of > 39Euros & Wide-leg Cut\"),\n     xlab = \"prices of 'wide-leg & price > 39' jeans\"\n     )\n\n\n\nwenn du Zeilen filtern möchtest, in denen ‘price’ größer als 39 ODER ein “wide leg” cut ist.\n\n\n# condition that will select both wide cuts and those that cost more than 59Euro\ncondition_wide_or_39 = my_dataframe$Cut == \"wide leg\" | my_dataframe$Price > 39\nhead(condition_wide_or_39)\n\n[1]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n\ndata_wide_or_39 = my_dataframe[condition_wide_or_39,]\n\nhist(data_wide_or_39[,2],\n     col = \"lightblue\", border = \"#227FCE\",\n     main = paste(\"Histogram of > 39Euros & Wide-leg Cut\"),\n     xlab = \"prices of 'wide-leg & price > 39' jeans\"\n     )\n\n\n\nImportant to know:\nIn R, the ‘$’ sign is used to access a specific column (variable) within a data frame or list. It allows you to extract or work with a single column of data from a larger data structure, such as a data frame, without having to reference the entire data frame each time.\nTask 1: Access a Specific Element in a Matrix/Dataframe\n\nUse indexing to access and display the element in the second row and third column in the dataframe presented earlier in this blogpost.\nPrint the value of the accessed element.\n\n\n\n# Access the element in the second row and third column\nelement = my_dataframe[2, 3]\nprint(element)\n\n[1] 7/8\nLevels: 7/8 boot boyfriend skinny wide leg\n\nTask 2: Extend and index a Matrix/Dataframe\n\nAdd another column to the dataframe, which represents the brand category. It will be the following vector: rep(c(“expensive”,“top”, “middle”, “affordable”, “cheap”), 20). *learn about that rep() does by typing ?rep in the console!\nUse indexing to extract and display the first 10 rows of this dataframe and see if the rep() did what it was suppose to.\n\n\n\n# Extend the dataframe\nmy_dataframe$Brand = rep(c(\"expensive\",\"top\", \"middle\", \"affordable\", \"cheap\"), 20)\n\n# Extract the entire second row\ndataframe_first10rows = my_dataframe[1:10, ]\nprint(dataframe_first10rows)\n\n   ItemType Price       Cut      Brand\n1     jeans    57      boot  expensive\n2     jeans    26       7/8        top\n3     jeans    74    skinny     middle\n4     jeans    62  wide leg affordable\n5     jeans   117 boyfriend      cheap\n6     jeans    66      boot  expensive\n7     jeans    76       7/8        top\n8     jeans   111    skinny     middle\n9     jeans    44  wide leg affordable\n10    jeans    50 boyfriend      cheap\n\nTask 3: Extract rows in Matrix/Dataframe by condition\n\nExtract the rows in the dataframe, which represents only the “affordable” category.\nPrint it.\nBonus: create a histogram to show the distribution of the prices in this category\n\n\n\n# Define condition\ncondition_brandCategory = my_dataframe$Brand == \"affordable\"\n\n# filter according to this condition\nfiltered_cat = my_dataframe[condition_brandCategory,]\n\n# print the filtered dataset\nprint(filtered_cat)\n\n   ItemType Price      Cut      Brand\n4     jeans    62 wide leg affordable\n9     jeans    44 wide leg affordable\n14    jeans    41 wide leg affordable\n19    jeans    35 wide leg affordable\n24    jeans    91 wide leg affordable\n29    jeans    48 wide leg affordable\n34    jeans    57 wide leg affordable\n39    jeans    53 wide leg affordable\n44    jeans    44 wide leg affordable\n49    jeans    91 wide leg affordable\n54    jeans    12 wide leg affordable\n59    jeans    63 wide leg affordable\n64    jeans    52 wide leg affordable\n69    jeans    54 wide leg affordable\n74    jeans    52 wide leg affordable\n79    jeans    57 wide leg affordable\n84    jeans    52 wide leg affordable\n89    jeans   114 wide leg affordable\n94    jeans    60 wide leg affordable\n99    jeans    64 wide leg affordable\n\n# create a histogram\n#hist(filtered_cat$Price)\n\n\nTask 4: Filter by multiple conditions\n\nExtract the rows in the dataframe, which represents only the “cheap” category and where prices are below 30.\nPrint it.\nBonus: create a histogram to show the distribution of the prices in these two conditions.\n\n\n\n# Define condition\ncondition_brand_price = my_dataframe$Brand == \"affordable\" & my_dataframe$Price < 30\n\n# filter according to this condition\nfiltered_ctgs = my_dataframe[condition_brand_price,]\n\n# print the filtered dataset\nprint(filtered_ctgs)\n\n   ItemType Price      Cut      Brand\n54    jeans    12 wide leg affordable\n\n# create a histogram\n# hist(filtered_ctgs$Price)\n\n\nTask 5: Add a column and index\n\nExtend the my_dataframe data frame with a new column named ‘ID’. Use the ‘$’ to do so. Hin: we have 600 rows in this dataframe. Use dim() to find our what the dimensions of this dataframe is to knwo how many rows we have.\nUse indexing to access and display the first 100 rows.\nPrint your results.\n\n\n\n# Extend the data frame with 'item ID' column\nmy_dataframe$ID <- c(1:100)\n\n# take a plimse of the data with the head() function\n\nhead(my_dataframe)\n\n  ItemType Price       Cut      Brand ID\n1    jeans    57      boot  expensive  1\n2    jeans    26       7/8        top  2\n3    jeans    74    skinny     middle  3\n4    jeans    62  wide leg affordable  4\n5    jeans   117 boyfriend      cheap  5\n6    jeans    66      boot  expensive  6\n\n# Select only the ID 1-100\nid_1_100 <- my_dataframe[1:100, ]\nid_1_100 <- my_dataframe[1:100, ]\n\nprint(id_1_100)\n\n    ItemType Price       Cut      Brand  ID\n1      jeans    57      boot  expensive   1\n2      jeans    26       7/8        top   2\n3      jeans    74    skinny     middle   3\n4      jeans    62  wide leg affordable   4\n5      jeans   117 boyfriend      cheap   5\n6      jeans    66      boot  expensive   6\n7      jeans    76       7/8        top   7\n8      jeans   111    skinny     middle   8\n9      jeans    44  wide leg affordable   9\n10     jeans    50 boyfriend      cheap  10\n11     jeans    37      boot  expensive  11\n12     jeans    45       7/8        top  12\n13     jeans    59    skinny     middle  13\n14     jeans    41  wide leg affordable  14\n15     jeans    58 boyfriend      cheap  15\n16     jeans    70      boot  expensive  16\n17     jeans    86       7/8        top  17\n18     jeans    13    skinny     middle  18\n19     jeans    35  wide leg affordable  19\n20     jeans    71 boyfriend      cheap  20\n21     jeans   109      boot  expensive  21\n22     jeans    37       7/8        top  22\n23     jeans    40    skinny     middle  23\n24     jeans    91  wide leg affordable  24\n25     jeans    31 boyfriend      cheap  25\n26     jeans    86      boot  expensive  26\n27     jeans    35       7/8        top  27\n28     jeans    15    skinny     middle  28\n29     jeans    48  wide leg affordable  29\n30     jeans    89 boyfriend      cheap  30\n31     jeans    61      boot  expensive  31\n32     jeans    59       7/8        top  32\n33     jeans    67    skinny     middle  33\n34     jeans    57  wide leg affordable  34\n35     jeans   106 boyfriend      cheap  35\n36     jeans    74      boot  expensive  36\n37     jeans    66       7/8        top  37\n38     jeans    38    skinny     middle  38\n39     jeans    53  wide leg affordable  39\n40     jeans    33 boyfriend      cheap  40\n41     jeans    45      boot  expensive  41\n42     jeans    45       7/8        top  42\n43     jeans    41    skinny     middle  43\n44     jeans    44  wide leg affordable  44\n45     jeans    57 boyfriend      cheap  45\n46     jeans    81      boot  expensive  46\n47     jeans    89       7/8        top  47\n48     jeans    53    skinny     middle  48\n49     jeans    91  wide leg affordable  49\n50     jeans    83 boyfriend      cheap  50\n51     jeans    89      boot  expensive  51\n52     jeans    85       7/8        top  52\n53     jeans    64    skinny     middle  53\n54     jeans    12  wide leg affordable  54\n55     jeans    67 boyfriend      cheap  55\n56     jeans    38      boot  expensive  56\n57     jeans    59       7/8        top  57\n58     jeans    81    skinny     middle  58\n59     jeans    63  wide leg affordable  59\n60     jeans    66 boyfriend      cheap  60\n61     jeans    82      boot  expensive  61\n62     jeans    46       7/8        top  62\n63     jeans    98    skinny     middle  63\n64     jeans    52  wide leg affordable  64\n65     jeans    53 boyfriend      cheap  65\n66     jeans    56      boot  expensive  66\n67     jeans    78       7/8        top  67\n68     jeans    80    skinny     middle  68\n69     jeans    54  wide leg affordable  69\n70     jeans    69 boyfriend      cheap  70\n71     jeans    80      boot  expensive  71\n72     jeans    56       7/8        top  72\n73     jeans    60    skinny     middle  73\n74     jeans    52  wide leg affordable  74\n75     jeans    70 boyfriend      cheap  75\n76     jeans    49      boot  expensive  76\n77     jeans    90       7/8        top  77\n78     jeans    56    skinny     middle  78\n79     jeans    57  wide leg affordable  79\n80     jeans    78 boyfriend      cheap  80\n81     jeans   101      boot  expensive  81\n82     jeans    68       7/8        top  82\n83     jeans   102    skinny     middle  83\n84     jeans    52  wide leg affordable  84\n85     jeans    70 boyfriend      cheap  85\n86     jeans    59      boot  expensive  86\n87     jeans   104       7/8        top  87\n88     jeans    54    skinny     middle  88\n89     jeans   114  wide leg affordable  89\n90     jeans    69 boyfriend      cheap  90\n91     jeans    61      boot  expensive  91\n92     jeans    51       7/8        top  92\n93     jeans    46    skinny     middle  93\n94     jeans    60  wide leg affordable  94\n95     jeans    49 boyfriend      cheap  95\n96     jeans    71      boot  expensive  96\n97     jeans    81       7/8        top  97\n98     jeans    60    skinny     middle  98\n99     jeans    64  wide leg affordable  99\n100    jeans    69 boyfriend      cheap 100\n\n\n\n\n",
      "last_modified": "2023-11-14T16:12:39+01:00"
    },
    {
      "path": "lm_ii_ii.html",
      "title": "Multiple Linear Regression Analysis - Part II",
      "description": "Linear Regression\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nIntroduction:\nWhile the traditional understanding of linear regression assumes continuous independent variables, there are scenarios where the independent variables (UV) may be discrete in nature.\nIn this blog post we will explore the concept of multiple linear regression with discrete variables and understand how it can be applied to real-world problems.\nWhat are discrete variables?\nA discrete variable is a type of variable that takes on specific, distinct values with no intermediate values possible between them. These values are typically counted or categorized rather than measured on a continuous scale\nCan you give me some examples?\nGender (e.g., either Male 🧍🏼‍♂️ vs Female🧍🏾‍♀️)\nEthnicity (e.g., Caucasian, African American, and Asian)\nOccupation (e.g., “Engineer” 👩🏻‍💻 vs. “Teacher” 👨🏻‍🏫 vs. “Salesperson” 🧑🏽‍💼)\nIn case we are looking at the effect of such variables, you will need to modify the multiple linear regression model to account for them.\nCool, but how do I code the variables expressions?\nWell, depending on how many levels/groups there are, this may change.\nA case of a variable with two categorical expressions:\nIn the case of a discrete predictor with two categorical expressions (e.g., male vs. female), we first establish an arbitrary expression (e.g., male) as the reference category. This reference group will be coded with a 0. For example, if we decided that “male” is our reference group, we will insert 0 if the participant is a male and a 1 if they are identified as female.\nSo, we define a dummy variable \\(D_{i}\\) as follows:\n\\(D_{i} = 1\\), if the person is a female\nand\n\\(D_{i} = 0\\), if the person is a male\nA practical example:\nAnd here is an example of a data set, which includes information about a group of individuals, with each individual identified by a unique ID.\nID column: This variable represents the unique identifier assigned to each individual in the data set.\nGender Dummy column: This dummy coded variable indicates whether each person has the color red. It takes a value of 1 if the person has the color red and 0 if not.\nIQ column: This variable represents the IQ scores of individuals. It provides a measure of intellectual ability or cognitive capacity, with higher values indicating higher IQ scores.\n**Monthly Salary _Score column**: This variable represents the scores on Monthly Salary for each individual. It reflects the subjective assessment of Monthly Salary , with higher values indicating higher levels of self-reported Monthly Salary .\n\n   ID Gender_dummy  IQ Salary_month\n1   1            1 120         8000\n2   2            0 110         7000\n3   3            1 125         9000\n4   4            0 105         6000\n5   5            0  95         5000\n6   6            1 115         4000\n7   7            0 100         2500\n8   8            0 130         9000\n9   9            1 125         3000\n10 10            0 105         2000\n\nHow does the linear model look like with a discrete variable with two expressions?\nThe general equation is:\n\\[Y_{i} = \\alpha + \\beta_{1} \\times D_{1i} +... + \\beta_{2} \\times D_{2i} +... +\\beta_{k-1} \\times D_{k-1}  + \\epsilon_{i} \\]\nFor our specific example, therefore, it will be:\n1) For the reference group (we decided on “male”), the model will be:\n\\[Y_{i} = \\alpha + \\beta \\times 0 + \\epsilon_{i}\\]\n\\[ = \\alpha + \\epsilon_{i}\\]\nIt follows that \\(\\alpha\\) is the predicted value for this group.\n2) For the second group (will be “female”), the model will be:\n\\[Y_{i} = \\alpha + \\beta \\times 1 + \\epsilon_{i}\\]\n\\[ = \\alpha + \\beta +\\epsilon_{i}\\]\nIt follows that \\(\\alpha + \\beta\\) is the predicted value for this group.\nIt also implies that:\nIf \\(𝛽 < 0\\), then the predicted value for individuals in the reference category is greater than in the other category.\nIf \\(𝛽> 0\\) , then the predicted value for individuals in the reference category is smaller than in the other category.\nIf \\(𝛽 = 0\\), then the predicted values for both categories are equal.\nLets run a model to learn about the estimated varibales and their interpretation:\n\n\nmodel_2 <- lm(Salary_month ~ Gender_dummy, data = data_2)\n  \nsummary(lm(Salary_month ~ Gender_dummy, data = data_2))\n\n\nCall:\nlm(formula = Salary_month ~ Gender_dummy, data = data_2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -3250  -2562    250   1938   3750 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)   \n(Intercept)      5250       1135   4.624   0.0017 **\nGender_dummy      750       1795   0.418   0.6871   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2781 on 8 degrees of freedom\nMultiple R-squared:  0.02135,   Adjusted R-squared:  -0.101 \nF-statistic: 0.1745 on 1 and 8 DF,  p-value: 0.6871\n\nconfint(model_2)\n\n                 2.5 %   97.5 %\n(Intercept)   2631.835 7868.165\nGender_dummy -3389.683 4889.683\n\nWe can see the results and learn that:\n\\(\\alpha\\)(intercept) = 5,250.\n\\(\\alpha\\) represents the predicted Monthly Salary value for male participants.\nThe confidence interval for the intercept is [2631.835, 7868.165], which does not include the 0, indicating, alongside \\(p-value = .0017\\) that there our predicted value for male participants is non-zero and that the plausible monthly salary for males lies between 2631.835 and 7868.165.\nGender_dummy (\\(\\beta_{gender}\\)) = 750.\nThis means that the predicted Monthly Salary value for females participants is (\\(\\alpha + \\beta_{gender}\\)) 5250 + (1 * 750) = 6,000.\n\\(\\beta_{gender}\\) represents the difference in the predicted monthly salary fro females and males (the reference group).\nThe confidence interval for the slope is [-3389.683, 4889.683], which does include the 0, indicating, alongside \\(p-value = .687\\) that there our predicted value for female participants could potentially be zero (which is inline with \\(H_{0}\\)) and that the plausible monthly salary for males lies between -3389.683 and 4889.683.\nA case of a variable with MORE THAN two categorical expressions:\nA practical example:\nLet’s consider an example of a discrete variable with three categories: “Education Level”.\nWe’ll define and code the categories as - “High School”\n- “Bachelor’s Degree”\n- “Master’s Degree.”\nWhen we create dummy variables to represent these categories, we assign a value of 1 if an individual belongs to that category and a value of 0 if they do not.\nIn the resulting data frame below, if a row has a 0 in both columns “Dummy_HS” and “Dummy_Bachelor” columns, it means that the person does not have a High School education nor a Bachelor’s Degree. Essentially, they are not in either of those categories. Instead, they may have a different education level, such as a Master’s Degree or some other qualification not represented in the dummy variables.\nTherefore, these rows with two zeros (as on rows 3, 6 and 8) indicate individuals who do not fall into the specified categories and should be understood as having a different education level or belonging to an unrepresented category (which is in this case the “high schools degree”).\n\n   ID   Education_Level  IQ Salary_month Dummy_HS Dummy_Bachelor\n1   1 Bachelor's Degree 120         8000        1              0\n2   2       High School 110         7000        0              0\n3   3   Master's Degree 125         9000        0              1\n4   4       High School 105         6000        0              0\n5   5 Bachelor's Degree  95         5000        1              0\n6   6   Master's Degree 115         4000        0              1\n7   7       High School 100         2500        0              0\n8   8   Master's Degree 130         9000        0              1\n9   9 Bachelor's Degree 125         3000        1              0\n10 10       High School 105         2000        0              0\n\nFor our specific example, therefore, the equations will be:\nFor the reference group (we decided on “high school”), the model will be:\n\\[Y_{i} = \\alpha + \\beta_{bachelor} \\times 0 + \\beta_{masters} \\times 0 + \\epsilon_{i} \\]\n\\[ = \\alpha + \\epsilon_{i}\\]\nIt follows that \\(\\alpha\\) is the predicted value for this group.\nFor the second group (will be “Bachelor’s Degree”), the model will be:\n\\[Y_{i} = \\alpha + \\beta_{bachelor} \\times 1 + \\beta_{masters} \\times 0 + \\epsilon_{i}\\]\n\\[ = \\alpha + \\beta_{bachelor} + \\epsilon_{i}\\]\nIt follows that \\(\\alpha + \\beta_{bachelor}\\) is the predicted value for this group.\nFor the third group (will be “Master’s Degree”), the model will be:\n\\[Y_{i} = \\alpha + \\beta_{bachelor} \\times 0 + \\beta_{masters} \\times 1 + \\epsilon_{i}\\]\n\\[ = \\alpha + \\beta_{masters} + \\epsilon_{i}\\]\nIt follows that \\(\\alpha + \\beta_{masters}\\) is the predicted value for this group.\nLets run a model to learn about the estimated varibales and their interpretation:\n\n[1] \"High School\"       \"Bachelor's Degree\" \"Master's Degree\"  \n\nCall:\nlm(formula = Salary_month ~ Education_Level, data = data_3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3333.3 -2218.8   645.8  1666.7  2666.7 \n\nCoefficients:\n                                 Estimate Std. Error t value Pr(>|t|)\n(Intercept)                        4375.0     1309.6   3.341   0.0124\nEducation_LevelBachelor's Degree    958.3     2000.4   0.479   0.6465\nEducation_LevelMaster's Degree     2958.3     2000.4   1.479   0.1827\n                                  \n(Intercept)                      *\nEducation_LevelBachelor's Degree  \nEducation_LevelMaster's Degree    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2619 on 7 degrees of freedom\nMultiple R-squared:  0.2405,    Adjusted R-squared:  0.02347 \nF-statistic: 1.108 on 2 and 7 DF,  p-value: 0.3819\n                                     2.5 %   97.5 %\n(Intercept)                       1278.308 7471.692\nEducation_LevelBachelor's Degree -3771.941 5688.608\nEducation_LevelMaster's Degree   -1771.941 7688.608\n\n\\(\\alpha\\)(intercept) = 4,375.\n\\(\\alpha\\) represents the predicted Monthly Salary value for participants that have graduated Gymnasium.\nThe confidence interval for the intercept is [1278.308, 7471.692], which does not include the 0, indicating, alongside \\(p-value = .0124\\) that there our predicted value for participants, who graduated Gymnasium is non-zero and that the plausible monthly salary for those lies between 1278.308 and 7471.692.\nBachelor’s degree (\\(\\beta_{BA}\\)) = 958.3.\nThis means that the predicted Monthly Salary value for participants, who have a BA degree is (\\(\\alpha + \\beta_{BA}\\)) 4,375 + (1 * 958.3) = 5,333.3.\n\\(\\beta_{BA}\\) represents the difference in the predicted monthly salary for participants, who have BA degree and those, who have graduated Gymnasium (the reference group).\nThe confidence interval for this slope is [-3771.941, 5688.608], which does include the 0, indicating, alongside \\(p-value = .6465\\) that BA graduates do not differ in their salaries from those, who have a BA degree. Since this range includes a zero (which is inline with \\(H_{0}\\)) and the plausible monthly salary for BA graduates lies between -3389.683 and 4889.683, we can conclude that we will accept the \\(H_{0}\\).\nBachelor’s degree (\\(\\beta_{MA}\\)) = 2958.3.\nThis means that the predicted Monthly Salary value for participants, who have a MA degree is (\\(\\alpha + \\beta_{MA}\\)) 4,375 + (1 * 2958.3) = 7,333.3.\n\\(\\beta_{MA}\\) represents the difference in the predicted monthly salary for participants, who have MA degree and those, who have graduated Gymnasium (the reference group).\nThe confidence interval for this slope is [-1771.941, 7688.608], which does include the 0, indicating, alongside \\(p-value = .1827\\) that MA graduates do not differ in their salaries from those, who have a MA degree. Since this range includes a zero (which is inline with \\(H_{0}\\)) and the plausible monthly salary for MA graduates lies between -1771.941 and 7688.608, we can conclude that we will accept the \\(H_{0}\\).\nWhat about a model that combines both discrete and continous variables?\nGreat question!\nThe following statistical model or analysis has two types of predictors:\none predictor is discrete, meaning it has two possible manifestations or categories.\na second predictor is continuous, meaning it takes on a range of numerical values.\nWe can use the dataset we have create before and add the IQ scores to predicts participant’s hapinness.\nThe full model in this case will be:\n\\[Y_{i} = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{education} \\times D_{i} + \\epsilon_{i} \\]\nThe specifications of all the other models are:\n1) For the reference group, which has a high school education, the regression model will be:\n\\[Y_{i} = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{education} \\times 0 + \\epsilon_{i} \\]\nWhich implies that the predicted value of this group’s Monthly Salary is\n\\[ = \\alpha + \\beta_{IQ} \\times X_{i}\\]\n2) For the second group, which has a becholar’s degree , the regression model will be:\n\\[Y_{i} = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{becholar} \\times 1 + \\epsilon_{i} \\]\nWhich implies that the predicted value of this group’s Monthly Salary is\n\\[ = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{becholar} \\times 1\\]\n3) For the third group, which has a masters’s degree , the regression model will be:\n\\[Y_{i} = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{masters} \\times 1 + \\epsilon_{i} \\]\nWhich implies that the predicted value of this group’s Monthly Salary is\n\\[ = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{masters} \\times 1\\]\nThe last question for today is: 🥁\nWhat about a good-old interaction effect between two independent variable (UVs)?\nLets revert back to our first above, including an interaction effect in a a linear regression model helps us to explore how the relationship between “gender” (reminder: 2 levels - male vs. female) and “Monthly Salary” changes, depending on the gender.\nMore specifically, in our data set we have gender and IQ as potential predictors. By including an interaction term, we can assess whether the relationship between gender and Monthly Salary differs for individuals with different IQ levels. It could show us, for example, that gender has a stronger positive effect on Monthly Salary for individuals with avergae IQ scores compared to those with lower/higher IQ scores.\nOverall, the interaction term helps us understand if the relationship between gender and Monthly Salary is dependent on an individual’s IQ level and if the two factors interact in influencing Monthly Salary .\nLet us define the general model equation:\n\\[Y_{i} = \\alpha + \\beta_{gender} \\times D_{i} + \\beta_{IQ} \\times X_{i} + \\beta_{interaction} \\times (X_{i} \\times D_{i})+ \\epsilon_{i} \\]\n1) For the reference group, where \\(D_{i} = 0\\), the model will be:\n\\[Y_{i} = \\alpha + \\beta_{gender} \\times D_{i} + \\beta_{IQ} \\times X_{i} + \\beta_{interaction} \\times (X_{i} \\times D_{i})+ \\epsilon_{i} \\]\n\\[ = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{gender} \\times 0 + \\beta_{interaction}(X_{i} \\times 0) \\]\n\\[ = \\alpha + \\beta_{IQ} \\times X_{i} + \\epsilon_{i}\\]\nWhich implies that the predicted Monthly Salary value for the reference group is:\n\\[= \\alpha + \\beta_{IQ} \\times X_{i}\\]\n2) For the second group, where \\(D_{i} = 1\\), the model will be:\n\\[Y_{i} = \\alpha + \\beta_{gender} \\times D_{i} + \\beta_{IQ} \\times X_{i} + \\beta_{interaction} \\times (X_{i} \\times D_{i})+ \\epsilon_{i} \\]\n\\[ = \\alpha + \\beta_{IQ} \\times X_{i} + \\beta_{gender} \\times 1 + \\beta_{interaction}(X_{i} \\times 1) \\]\n\\[ = (\\alpha + \\beta_{gender}) + (\\beta_{IQ} + \\beta_{interaction}) \\times X_{i} + \\epsilon_{i}\\]\nWhich implies that the predicted Monthly Salary value for the second group is:\n\\[ = (\\alpha + \\beta_{gender}) + (\\beta_{IQ} + \\beta_{interaction}) \\times X_{i}\\]\nLets run a model to learn about the estimated varibales and their interpretation:\n\n\nCall:\nlm(formula = Salary_month ~ Gender_dummy + IQ + (Gender_dummy * \n    IQ), data = data_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3545.5 -1429.5   639.8  1658.9  2454.5 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)\n(Intercept)     -12059.32   10440.53  -1.155    0.292\nGender_dummy       422.96   39782.57   0.011    0.992\nIQ                 161.02      96.61   1.667    0.147\nGender_dummy:IQ    -15.56     330.84  -0.047    0.964\n\nResidual standard error: 2624 on 6 degrees of freedom\nMultiple R-squared:  0.3468,    Adjusted R-squared:  0.02017 \nF-statistic: 1.062 on 3 and 6 DF,  p-value: 0.4323\n                       2.5 %     97.5 %\n(Intercept)     -37606.38747 13487.7434\nGender_dummy    -96921.48242 97767.3992\nIQ                 -75.37631   397.4102\nGender_dummy:IQ   -825.09363   793.9688\n\nThe interpretation of the parameters is as follows:\n• \\(\\alpha\\): This is the predicted value of \\(Y_i\\) for people in the reference category. In the current category, it represents the predicted salary for male participants, when IQ equal 0…\n• \\(\\alpha + \\beta_{gender}\\): This is the intercept in the female category and it represents the predicted value for female participants, when their IQ equals 0….\n• \\(\\beta_{gender}\\): This is the slope parameter for gender and it represents the expected increase in Monthly Salary for a male participant when IQ is increased by 1 point.\n• \\(\\beta_{IQ}\\): This is the slope parameter for IQ and it represents the expected difference in Monthly Salary between male and female participants when their IQ = 0.\n• \\(\\beta_{interaction}\\): This is the difference in slope parameters between the reference category (male) and other category (female). It reflects the difference in the “contribution” of IQ for male vs. female participants on their Monthly Salary.\n\n\n\n",
      "last_modified": "2023-11-14T16:12:40+01:00"
    },
    {
      "path": "lm_ii.html",
      "title": "Multiple Linear Regression Analysis",
      "description": "Linear Regression\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nLinear models can be extended to include more than one independent variable, and the equation becomes:\n\\(\\color{red}{Y{i}} = \\color{blue}{{\\alpha}} + \\color{green}{{\\beta_{1} \\times X_{1} + \\beta_{2} \\times X_{2} + … + \\beta_{n} \\times X_{n}} + \\color{orange}{{\\epsilon_{i}}}}\\)\nWhere \\(\\color{red}{Y_{i}}\\) is the dependent variable (what we aim to predict).\n\\(\\color{blue}\\alpha\\) is the intercept (the point at which the regression line crosses the y axis).\n\\(\\color{green}{{X_{1}, X_{2}, X_{n}}}\\) are the independent variables (what we measure).\n\\(\\color{green}{{\\beta_{1}, \\beta_{2}, ..., \\beta_{n}}}\\) are the slopes of the respective independent variables (tells us how important the respective variable is).\nSince we also are interested in the distribution of the residuals, we also estimate \\(\\color{orange}{{\\sigma^2}}\\), which represents the standard deviation of the error term \\(\\color{orange}\\epsilon\\).\nIn MLR (Multiple Linear Regression) analysis we ask the following questions:\n## 1) How much variance, relative to the total variability of the criterion, can all predictors explain together?\n## 2) Which predictor has the largest predictive contribution?\n## 3) What is the magnitude of the independent predictive contribution of a predictor?\n## 4) Does the strength, direction, and interpretation of the effect of a predictor change when considering another predictor compared to the multiple regression?\nWe are going to use a fun(ny) example for this topic.\nIn a parallel universe, in which Psychology students party during the semester and prior to their exams, a study was conducted…\n\nThe researchers wanted to know if the amount of alcohol 🍺 consumed consumed a day prior to an exam but also the number of hours spent on preparation 📚📖 affected the student’s exam results 🥇.\nAnd so they asked students to confess about the amount of ml of alcohol (0 to ∞😂) they had the day before the exam and the number hours (0-100) they spent studying and how much they scored in the exam (0-💯%).\nModel Equation:\nTherefore, the model equation they were about to test is:\n\\(\\color{red}{Test.Score_{i}} = \\color{blue}{{\\alpha}} + \\color{green}{{\\beta_{alcohol_ml} \\times X_{1} + \\beta_{Study.hours} \\times X_{2}} + \\color{orange}{{\\epsilon_{i}}}}\\)\nHere is the data they obtained:\n\n    Test_Score Drink_ml Hours_Studied\n1         99.9     65.1          66.2\n2        100.0     65.6          66.6\n3         99.9     63.8          64.1\n4        100.0     66.1          65.6\n5        100.0     65.1          64.6\n6        100.0     65.4          63.8\n7        100.0     64.5          64.1\n8         99.3     63.8          64.0\n9         99.2     63.5          65.1\n10       100.0     64.4          65.9\n11       100.0     65.1          65.9\n12       100.0     66.2          66.1\n13       100.0     65.0          64.9\n14        99.4     64.2          64.5\n15        99.8     67.0          66.4\n16        98.7     64.2          64.1\n17       100.0     65.2          65.4\n18       100.0     65.3          66.2\n19        99.8     65.9          63.9\n20       100.0     65.3          65.4\n21       100.0     64.6          64.7\n22        98.8     63.8          64.7\n23       100.0     65.4          66.0\n24       100.0     64.5          63.3\n25       100.0     64.4          64.5\n26       100.0     64.7          64.1\n27       100.0     64.9          65.0\n28       100.0     65.0          65.6\n29       100.0     65.6          66.0\n30       100.0     65.2          66.1\n31       100.0     65.9          65.7\n32        99.6     65.9          64.7\n33       100.0     67.0          65.8\n34        98.9     64.1          64.7\n35        99.4     64.3          64.0\n36        99.3     63.8          63.2\n37        99.7     65.6          64.6\n38       100.0     63.7          64.7\n39       100.0     65.5          65.3\n40        98.9     63.7          63.6\n41        99.7     64.0          64.3\n42       100.0     66.4          64.9\n43       100.0     64.9          64.2\n44       100.0     66.4          65.8\n45        99.3     64.0          63.0\n46       100.0     64.2          65.6\n47       100.0     64.4          65.8\n48        98.9     63.1          63.1\n49        98.0     63.6          63.1\n50       100.0     66.8          64.6\n51       100.0     66.0          65.7\n52        99.0     64.5          64.4\n53       100.0     65.1          65.9\n54        99.7     65.6          64.2\n55        99.6     67.5          65.2\n56       100.0     65.5          64.8\n57       100.0     65.4          65.1\n58       100.0     65.6          67.0\n59        99.0     63.8          64.5\n60       100.0     65.4          64.8\n61        99.3     65.8          64.7\n62        99.1     64.5          64.5\n63       100.0     65.6          66.0\n64        99.5     65.0          64.6\n65        99.5     63.8          63.6\n66       100.0     65.8          64.6\n67        99.7     64.4          66.5\n68       100.0     65.3          65.8\n69        99.3     64.2          64.0\n70        99.9     66.3          65.3\n71       100.0     65.6          66.2\n72        99.7     64.4          66.1\n73        99.0     63.9          63.5\n74        98.8     64.3          65.0\n75       100.0     65.9          65.2\n76        99.7     64.2          64.2\n77       100.0     64.9          66.8\n78        99.8     65.0          64.4\n79        99.0     63.6          64.3\n80       100.0     65.2          64.5\n81        99.6     64.7          66.0\n82        99.4     65.1          66.4\n83        99.3     63.5          64.0\n84       100.0     64.6          64.5\n85       100.0     65.4          66.4\n86       100.0     66.1          65.9\n87        99.3     63.4          64.1\n88        99.9     65.3          66.1\n89       100.0     67.0          66.0\n90        98.3     64.2          64.5\n91       100.0     66.3          66.1\n92        98.8     65.3          64.4\n93       100.0     65.2          65.5\n94        99.8     63.7          65.2\n95       100.0     64.9          65.1\n96       100.0     65.7          66.6\n97       100.0     65.4          66.1\n98       100.0     66.2          65.1\n99       100.0     66.2          67.3\n100       99.1     65.0          65.8\n\nIn this parallel universe the rules are often different to ours but plotting your data is just like in our universe, simply a must!\nAnd so they did!\n\n\n\n\nThey did not forget to test the MLR assumptions before making public statements about their results…\n1) They started off with the LINEARITY assumption. To examine this assumption they plotted the residuals of each UV (alcohol in ml and the number of hours spent on studying)\n1.1) First they ran the linear model to be able to tell something about the distribution of the residuals. \n\n\nmod <-  lm(Test_Score ~ Drink_ml + Hours_Studied, data = dat)\nsummary(mod)\n\n\nCall:\nlm(formula = Test_Score ~ Drink_ml + Hours_Studied, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.16667 -0.13925  0.04314  0.22749  0.64136 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   78.59565    2.81839  27.887  < 2e-16 ***\nDrink_ml       0.18755    0.04572   4.102 8.52e-05 ***\nHours_Studied  0.13691    0.04417   3.099  0.00254 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3593 on 97 degrees of freedom\nMultiple R-squared:  0.3667,    Adjusted R-squared:  0.3537 \nF-statistic: 28.09 on 2 and 97 DF,  p-value: 2.38e-10\n\n\n\nlibrary(car)\ncrPlots(mod, ylab = \"residuals\")\n\n\n\nThe blue line represents a linear trend.\nThe pink line represents a flexible function that describes the data as closely as possible.\nWhen both lines are close to each other, the linearity assumption can be considered fulfilled (in our example, none of the variables win this competition).\nAgain,\nThe blue dashed line shows the expected residuals if the relationship between the predictor and response variable was linear.\nThe pink line shows the actual residuals.\nIf the two lines are significantly different, then this is evidence of a nonlinear relationship.\n2) The next assumption they tested was the NORMALITY. They plotted a histogram of the residuals. Before plotting these, they remembered (of course) to standardize them.\n\n\nlibrary(ggplot2)\nstand_residuals <-  data.frame(\n                    \"stand_resid\" = c(rstandard(mod)))\n\n# Create a histogram with density line using ggplot2\nggplot(stand_residuals, aes(x = stand_resid)) +\n  geom_histogram(fill = \"lightblue\", color = \"black\", bins = 15) +\n  ylim(0, 10) +\n  labs(x = \"residuals (standardized)\", y = \"Frequency\", \n       title = \"Histogram to Examine Homoskedasticity\") +\n  theme_classic()\n\n\n\nThis plot did not make them super happy either….\n3) lastly, they examined the presence of outliers:\n\n\ncooks_dist <-cooks.distance(model = mod)\n\nplot(cooks_dist, col=\"lightblue\", pch=19, cex=2)\n#add labels to every point\ntext(x = cooks_dist[1:100], labels=c(1:100))\nabline(h=4/sample_size)\n\n\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2023-11-14T16:12:42+01:00"
    },
    {
      "path": "lm.html",
      "title": "Simple Linear Regression Analysis",
      "description": "Linear Regression\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nWelcome to the post about linear models in statistics!\nLinear models are an essential tool in statistics used to model the relationship between a dependent variable and one or more independent variables.\nIn this post, we’ll explore linear models in more detail and introduce an interactive Shiny app plot that will help you visualize the concepts.\nBefore we dive into the interactive part of this post (in which you get to explorer different regression lines using different variables), let’s first define what a linear model is.\nA linear model is a mathematical equation that represents a linear relationship between two or more variables.\nThe simplest form of a linear model is a straight line equation of the form:\n\\(Y_{i} = \\alpha_{0} + \\beta_{1} \\times X_{1}\\)\nWhere \\(Y_{i}\\) is the dependent variable and represent the expected value of all \\(y_{i}\\) (all single data points) given a specific value of \\(X_{i}\\)\nWe are going to use a very simple example. We will try to fit a model that aims to predict the relationship between individuals height and weight.\nhere is a fun illustration of the simple model:\nLet us look at a simple example with little number of data points (just so we can get the feeling of whats going on under the hood)\n\n  Height Weight\n1    170     60\n2    180     75\n3    167     59\n4    165     88\n5    191     91\n\nHere are the averages of both the weights and heights. We will need those to calculate the intercpet and the slope of the model.\n\n[1]  74.6 174.6\n\nRemember how I told you that I am a fan of plotting the data?\nWe will use a scatterplot this time.\n\n\nlibrary(ggplot2)\nggplot(data = dat, aes(y = Weight,x = Height)) +\n  geom_point(alpha = .7, color = \"red\") +\n  geom_hline(yintercept = mean_weight, color=\"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = mean_height, color = \"green\", linetype = \"dashed\") + \n  #geom_smooth(method = \"lm\") +\n  theme_classic() \n\n\n\nHow is the \\(\\beta\\) calculated?\nA quick reminder: β represents how much the regression line will rise or fall.\nWe interpret the \\(\\beta\\) as the average increase in the Dependent variable (AV) when we increase the independent variable (UV) by one unit. For Example, if we increase the Height by 1cm, the average predicted increase of weight is …..\n\\[\\frac{\\sum_{i=1}^{n}(x_{i} -\\hat{x})\\times(y_{i}-\\hat{y})}{\\sum_{i=1}^{n}(x_{1} - \\hat{x})^2}\\]\nWhich can be also written as:\n\\[\\frac{cov(X,Y)}{S_{x}^2}\\]\nLets start!\n\n  Height Weight xi - mean(x) yi - mean(y) sqr(xi - mean(x)\n1    170     60         -4.6        -14.6            21.16\n2    180     75          5.4          0.4            29.16\n3    167     59         -7.6        -15.6            57.76\n4    165     88         -9.6         13.4            92.16\n5    191     91         16.4         16.4           268.96\n  (xi - mean(x)) X (yi - mean(y))\n1                           67.16\n2                            2.16\n3                          118.56\n4                         -128.64\n5                          268.96\n\nNow we have everything to compute the \\(cov(X, Y)\\) and the \\(S_{X}^2\\)\nFor \\(cov(X, Y)\\) we just need to compute the sum of the 6th column\n\n\nsum(dat[,\"(xi - mean(x)) X (yi - mean(y))\"])\n\n[1] 328.2\n\nFor \\(S_{x}^2\\) we just need to compute the sum of the 5th column\n\n\nsum(dat[, \"sqr(xi - mean(x)\"])\n\n[1] 469.2\n\nThe slope, is therefore, 0.6994885\n\n[1] 0.6994885\n\n\\(\\beta = \\frac{328.2}{469.2} = .69\\)\nThis means that, for every 1cm increase in height we expect to see an increase of .69KG.\nHow does the \\(\\alpha\\) calculated?\n\\[\\overline{y} = \\alpha_{0} + .69\\times \\overline{x}\\]\nWe know both means of \\(x\\) and \\(y\\) (from the calculation above)\nIf we rearrange the equation to solve for \\(\\alpha_{0}\\), we get\n\\[-\\alpha_{0} = -\\overline{y} + .69\\times \\overline{x}\\]\nLets rearrange the equation such that it will look nicer…\n\\[\\alpha_{0} = \\overline{y} - .69\\times \\overline{x}\\]\n\n\nalpha_0 = mean_weight - .6994885 * (mean_height)\nalpha_0\n\n[1] -47.53069\n\nOK, enough with the hard, tiring work of calculating everything by hand…. for exactly this reason we have R (😃).\nWe will use the function lm(), which which for Linear Model. We will wrap it with the function summary(), which provides us with the result summary of our model’s results.\n\n\nsum_lm <-  summary(lm(dat$Weight ~ dat$Height))\nsum_lm\n\n\nCall:\nlm(formula = dat$Weight ~ dat$Height)\n\nResiduals:\n      1       2       3       4       5 \n-11.382  -3.377 -10.284  20.115   4.928 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -47.5307   121.1507  -0.392    0.721\ndat$Height    0.6995     0.6928   1.010    0.387\n\nResidual standard error: 15.01 on 3 degrees of freedom\nMultiple R-squared:  0.2536,    Adjusted R-squared:  0.00482 \nF-statistic: 1.019 on 1 and 3 DF,  p-value: 0.387\n\nAccording to the model output, the intercept and the slope are not significant.\nAccording to the hypotheses of this mode, the intercept is compared against 0. That is, does -47.5307 is significantly different to 0. Looking at the estimated value itself it may seem odd that this value is not significant differernt to 0. To resolve this mysotory lets take a llok at the confidence intervals of the two:\n\n\nconfint.lm(lm(dat$Weight ~ dat$Height))\n\n                  2.5 %     97.5 %\n(Intercept) -433.086166 338.024785\ndat$Height    -1.505342   2.904319\n\nHow is the cofidence interval calculated?\nFor example: 95% C.I. for \\(\\beta_{1}\\): \\[b_{1} ± t_{1-α/2, n-2} * se(b_{1})\\]\nFirst we need to find the t value that “sits” at the lower and upper 2.5% of the t-distribution. We will use the function qt() and will provide us with the t value at 2.5% from a t-ditribution with 3 df.\n\n\nt_value <-  qt(p = .975, df =3, lower.tail = TRUE)\n\n\nWe now have all the unknowns to arrive at the solution.\nLets plug the numbers in and compare it to the 95% confidence interval we obtained above.\n\n\nupper_CI <-  0.6995 + (t_value * 0.6928) \nlower_CI <-  0.6995 - (t_value * 0.6928) \n\nupper_CI\n\n[1] 2.904299\n\nlower_CI\n\n[1] -1.505299\n\nLooking good!\nLets add this line to our scatterplot from before.\n\n\nggplot(data = dat, aes(y = Weight, x = Height)) +\n  geom_point(alpha = .7, color = \"red\") +\n  geom_hline(yintercept = mean_weight, color=\"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = mean_height, color = \"green\", linetype = \"dashed\") + \n  geom_smooth(method = \"lm\") +\n  theme_classic() \n\n\n\nLastly, here is how the standard error term is calculated. This term tells us the distance between the data points with respect to their y values and the best fitting line.\n\\[\\sigma^2 = S^2 = \\frac{\\sum_{i=1}^n(Y_{i} - \\hat{Y})}{n-2}\\]\nWe need the predicted value for each person in our dataset. We will use the predict.lm() function that will output the predicted values fro every person based on the model lm(weight ~ Height.\n\n\nlm <-  lm(dat$Weight ~ dat$Height)\ndat[,\"predicted_y\"] <-  predict.lm(lm)\n\ndat[,\"predicted_y\"] \n\n[1] 71.38235 78.37724 69.28389 67.88491 86.07161\n\ndat[, \"sqr(y-perd(y))\"] <- (dat$Weight - dat$predicted_y)**2\n\ndat[, \"sqr(y-perd(y))\"]\n\n[1] 129.55796  11.40574 105.75834 404.61683  24.28902\n\nsum <- sum(dat[, \"sqr(y-perd(y))\"])\n\nsqrt(sum/3)\n\n[1] 15.00697\n\nHere is how the t-distribution under the \\(H_{0}\\) looks like:\n\n\nlibrary(latex2exp)\nset.seed(123)\nx <- rt(1000, df = 3)\nx <-  round(x, digit = 3)\ncuts <-  quantile(x , c(0.000, .05, .95, .99999)) \n\n# Create data\nmy_variable = x\n \n# Calculate histogram, but do not draw it\nmy_hist = hist(x , breaks = 190  , plot = F)\n\n \n# Color vector\nmy_color= ifelse(my_hist$breaks <= 2.273526, \"lightgrey\",\n          ifelse(my_hist$breaks >= -2.273526, \"red\", rgb(0.2,0.2,0.2,0.2)))\n \n# Final plot\nplot(my_hist, \n     col = my_color, \n     border = F,\n     freq = FALSE, \n     main = (TeX('Histogram for a $\\\\t$-distribution with 3 degrees of freedom (df)')),\n     xlab = \"possible t values\", \n     xlim = c(-6,6), \n     ylim = c(0,.7), \n    cex.main=0.9)\ncurve(dt(x, df = 3), from = -5, to = 5, n = 500, col = 'red', lwd = 1, add = T)\nabline(v = -0.392, col = \"darkgreen\", lwd = 2,  lty = 'dashed')\nabline(v = 1.010, col = \"blue\", lwd = 2,  lty = 'dashed')\nabline(v = 2.273526, col = \"red\", lwd = 3)\ntext(x = 2.2, y = .7, TeX('t value for $\\\\alpha$'), col = \"blue\")\ntext(x = -1.6, y = .35,  TeX('t value for $\\\\beta$'),  col = \"darkgreen\")\ntext(x = 3.2, y = .15, substitute(paste(bold(\"t critical\"))),  col = \"red\")\n\n\n\nA Note on t-values:\n\\(t-value = \\frac{Estimated Parameter}{SF}\\)\nThe estimate represents the effect or impact that the independent variable has on the dependent variable. The standard error, on the other hand, quantifies the uncertainty or variability associated with that estimate.\nBy dividing the estimate by the standard error, we obtain the t-value. Essentially, the t-value tells us how many standard errors the estimate is away from zero. It helps us assess whether the estimate is statistically significant or just a result of random variation\nAnd a small toy-example for you to play around with 😉\nLet’s move onto the interactive Shiny app plot. The plot allows you to visualize the relationship between two variables and fit a linear model to the data. To use the app, follow these steps:\nChoose the variables from the dropdown menu.\nObserve how the line changes to fit the data points better.\nObserve how the changes in:\nIntercept\nSlope(s)\np-values\nR-squared value as you adjust the model.\nThe app plot is an excellent way to see how the slope and intercept of a linear model can impact the fit of the model to the data. You can also see how the R-squared value changes as you adjust the model, which is a measure of how well the model fits the data. The closer the R-squared value is to 1, the better the model fits the data.\nThe data Im using here is from a built-in dataset in R called “mtcar”.\n\n\nlibrary(knitr)\nkable(head(mtcars))\n\n\n\n\nmpg\n\n\ncyl\n\n\ndisp\n\n\nhp\n\n\ndrat\n\n\nwt\n\n\nqsec\n\n\nvs\n\n\nam\n\n\ngear\n\n\ncarb\n\n\nMazda RX4\n\n\n21.0\n\n\n6\n\n\n160\n\n\n110\n\n\n3.90\n\n\n2.620\n\n\n16.46\n\n\n0\n\n\n1\n\n\n4\n\n\n4\n\n\nMazda RX4 Wag\n\n\n21.0\n\n\n6\n\n\n160\n\n\n110\n\n\n3.90\n\n\n2.875\n\n\n17.02\n\n\n0\n\n\n1\n\n\n4\n\n\n4\n\n\nDatsun 710\n\n\n22.8\n\n\n4\n\n\n108\n\n\n93\n\n\n3.85\n\n\n2.320\n\n\n18.61\n\n\n1\n\n\n1\n\n\n4\n\n\n1\n\n\nHornet 4 Drive\n\n\n21.4\n\n\n6\n\n\n258\n\n\n110\n\n\n3.08\n\n\n3.215\n\n\n19.44\n\n\n1\n\n\n0\n\n\n3\n\n\n1\n\n\nHornet Sportabout\n\n\n18.7\n\n\n8\n\n\n360\n\n\n175\n\n\n3.15\n\n\n3.440\n\n\n17.02\n\n\n0\n\n\n0\n\n\n3\n\n\n2\n\n\nValiant\n\n\n18.1\n\n\n6\n\n\n225\n\n\n105\n\n\n2.76\n\n\n3.460\n\n\n20.22\n\n\n1\n\n\n0\n\n\n3\n\n\n1\n\n\nAfter looking at the different variables in this dataset, lets see how each of them can be modeled as a linear regression. The blue line represents the best-fitting line (i.e., the line that minimizes the distance between the data and the model – the line with the lowest RSS)\n\n\n\n\nIn conclusion, linear models are a powerful tool in statistics, and the interactive Shiny app plot provides an excellent way to explore the concepts. With the app, you can experiment with different linear models and see how they fit the data. I hope you found this post informative and helpful, and please don’t hesitate to leave any comments or questions below!\nHomework solution:\nHere is the dataaset:\n\n\ndat_salary <-  data.frame(\"iq\" = c(120, 110,  100, 135, 140),\n                          \"monthly salary\" = c(2500, 2300, 2400, 3000, 2100))\n\ndat_salary\n\n   iq monthly.salary\n1 120           2500\n2 110           2300\n3 100           2400\n4 135           3000\n5 140           2100\n\nHere is a plot of the data, as per usual:\n\n\nggplot(dat_salary,  aes(x = iq, y = monthly.salary)) +\n  geom_point(col = \"blue\", alpha = .4) + \n  geom_smooth(method = \"lm\", col = \"red\") +\n  theme_minimal()\n\n\n\nAnd the model’s output:\n\n\nsummary(lm(dat_salary$monthly.salary ~ dat_salary$iq))\n\n\nCall:\nlm(formula = dat_salary$monthly.salary ~ dat_salary$iq)\n\nResiduals:\n       1        2        3        4        5 \n  43.304 -123.661    9.375  493.750 -422.768 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept)   2060.268   1394.855   1.477    0.236\ndat_salary$iq    3.304     11.441   0.289    0.792\n\nResidual standard error: 382.9 on 3 degrees of freedom\nMultiple R-squared:  0.02704,   Adjusted R-squared:  -0.2973 \nF-statistic: 0.08338 on 1 and 3 DF,  p-value: 0.7916\n\n\n\n\n",
      "last_modified": "2023-11-14T16:12:44+01:00"
    },
    {
      "path": "q_a_summary.html",
      "title": "Q & A  - Summary ",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nHere is the summary of your questions and answers\n\n\n\n\n\n\n\n",
      "last_modified": "2023-11-14T16:12:45+01:00"
    },
    {
      "path": "statistik_i.html",
      "title": "Statistik 1",
      "description": "This tab will contain the topics covered in the course Statistik I \n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\n\n\n",
      "last_modified": "2023-11-14T16:12:46+01:00"
    },
    {
      "path": "statistik_ii.html",
      "title": "Untitled",
      "description": "A new article created using the Distill format.\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\n\n\n",
      "last_modified": "2023-11-14T16:12:46+01:00"
    },
    {
      "path": "vec_fun_homework.html",
      "title": "Homework:  Functions & Objects",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nHausaufgaben\nNachdem wir uns in dieser Sitzung intensiv mit den Grundlagen von R und R Studio beschäftigt haben, ist es an der Zeit, das Gelernte zu vertiefen und zu üben. Hier sind einige Aufgaben, die dir helfen sollen, dich weiter mit den Inhalten vertraut zu machen.\nNatürlich! Hier sind die Aufgaben als ungelöste Aufgaben für die Hausaufgaben:\nPart 1 - Grundlegende arithmetische Operationen\nFühre die folgenden arithmetischen Operationen in R Studio aus und gebe die Ergebnisse aus:\nAddiere 5 und 3.\nSubtrahiere 4 von 10.\nMultipliziere 6 mit 7.\nTeile 15 durch 3.\nPotenziere 2 mit 3.\n2. Variablenzuweisung und Verwendung\nWeise Werte den Variablen x und y zu. Verwende dann diese beiden Variablen, um:\nIhre Summe zu berechnen.\nIhre Differenz zu berechnen.\nGib die Ergebnisse aus.\n3. Arbeiten mit Vektoren\nErstelle zwei Vektoren. Der erste sollte die Zahlen 1 bis 5 und der zweite die Zahlen 6 bis 10 enthalten.\nFüge diese beiden Vektoren elementweise zusammen.\nMultipliziere diese beiden Vektoren elementweise.\nGib beide Ergebnisse aus.\n4. Sequenzen generieren\nGeneriere in R:\nEine Sequenz von Zahlen von 1 bis 10.\nEine Sequenz von Zahlen von 1 bis 10, aber in Schritten von 2.\n5. Eingebaute Funktionen verwenden\nVerwende die folgenden eingebauten Funktionen und gib das Ergebnis aus:\nFinde die Quadratwurzel von 25.\nFinde den absoluten Wert von -10.\nFinde den maximalen und den minimalen Wert aus der Liste: 3, 7, 1, 8, 5\n# Part 2\n### 1. Notenberechnung\nErstelle einen numerischen Vektor namens ‘noten’ für zehn Schüler. Weise jeder Note eine Punktzahl von 1 bis 5 zu, wobei 1 die beste Note ist und 5 die schlechteste. Berechne dann den Durchschnitt dieser Noten und gib das Ergebnis mit der print() Funktion aus.\n\n[1] 3\n\n### 2. Benotung von Schülern\nErstelle einen Vektor namens ‘namen’ mit den Namen von fünf Schülern deiner Klasse.\n\n[1] \"Anna\"  \"Ben\"   \"Clara\" \"David\" \"Eva\"\n\n### 3. Klassenanwesenheit\nErstelle einen logischen Vektor ‘anwesenheit’, der angibt, welche Schüler an einem bestimmten Tag anwesend waren (TRUE) und welche abwesend waren (FALSE).\n\n[1]  TRUE FALSE  TRUE  TRUE FALSE\n\n### 4. Vektoren kombinieren\nVerwende die cbind()-Funktion, um die Vektoren ‘namen’, ‘noten’ und ‘anwesenheit’ in einem Data Frame zu kombinieren.\n\nnamen   noten anwesenheit [1,] \"Anna\"  \"1\"   \"TRUE\" [2,] \"Ben\"   \"5\"   \"FALSE\" [3,] \"Clara\" \"3\"   \"TRUE\" [4,] \"David\" \"2\"   \"TRUE\" [5,] \"Eva\"   \"4\"   \"FALSE\" [6,] \"Anna\"  \"2\"   \"TRUE\" [7,] \"Ben\"   \"3\"   \"FALSE\" [8,] \"Clara\" \"4\"   \"TRUE\" [9,] \"David\" \"1\"   \"TRUE\" [10,] \"Eva\"   \"5\"   \"FALSE\"\n\n### 5. Notendiagramm erstellen\nVerwende den Vektor ‘noten’ aus Aufgabe 1, um ein Balkendiagramm zu erstellen, das die Verteilung der Noten zeigt.\n\n\n## Viel Erfolg!\n\n\n\n",
      "last_modified": "2023-11-14T16:12:47+01:00"
    }
  ],
  "collections": []
}
