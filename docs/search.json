{
  "articles": [
    {
      "path": "about.html",
      "title": "About Our Group",
      "description": "Who we are and what we are intreseted in?",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2023-06-19T13:42:34+02:00"
    },
    {
      "path": "anova_oneWay.html",
      "title": "Analysis of Variance - A Guide",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nWhat is an Omnibus test and how is it related to the analysis of variance?\nOmnibus tests are a kind of statistical test. They test whether the explained variance in a set of data is significantly greater than the unexplained variance, overall.\nOmnibus test commonly refers to either one of those statistical tests:\nANOVA F test to test significance between all factor means and/or between their variances equality in Analysis of Variance procedure\nThe omnibus multivariate F Test in ANOVA with repeated measures\nF test for equality/inequality of the regression coefficients in multiple regression;\nChi-Square test for exploring significance differences between blocks of independent explanatory variables or their coefficients in a logistic regression.\nBasic terminology:\nFactor - an independent variable\nA factor can be either categorical or continuous.\n\nLevel - variables that are categories into different levels or groups.\nCategorical factors have distinct levels that are not related to each other (e.g. type of fertilizer), while continuous factors represent a range of values along a continuum (e.g. temperature).\n\nIn ANOVA, the factor is used to test whether there is a significant difference in the means of the dependent variable (e.g. expressed aggression) across the different levels of the factor (age groups).\nOne-Way vs. Two-Way ANOVA\nOne-way ANOVA and two-way ANOVA are two variations of this test that differ in their design and purpose.\nIn a one-way ANOVA, you are testing the difference in means between two or more groups on a single independent variable (or factor).\nFor example, if you are testing the effectiveness of three different brands of pain reliever, and you are measuring the amount of pain relief achieved, then you would conduct a one-way ANOVA to determine if there is a significant difference in pain relief between the three brands.\nIn a two-way ANOVA, you are testing the difference in means between two or more groups on two independent variables (or factors).\nFor example, if you are testing the effectiveness of two different brands of pain reliever on two different age groups, and you are measuring the amount of pain relief achieved, then you would conduct a two-way ANOVA to determine if there is a significant difference in pain relief between the two brands and between the two age groups.\nSo what is the difference between the two again?\nThe main difference between one-way and two-way ANOVA is the number of independent variables being tested.\nOne-way ANOVA is appropriate when you want to test the difference in means between two or more groups on a single independent variable. Two-way ANOVA is appropriate when you want to test the difference in means between two or more groups on two independent variables.\nLets get down to business…\nA reminder:\nThe statistical model of ANOVA is a way of mathematically representing the variation in a dependent variable (Y) across different levels of one or more independent variables, also known as factors (X).\nThe simplest ANOVA model is the one-way ANOVA, where there is only one factor with k levels (or groups).\nLets look at the actual statistical model:\n\\(Yij = µ + τi + εij\\)\nwhere:\n\\(Yij\\) represents the value of the dependent variable for the jth observation in the ith group.\n\\(µ\\) represents the overall mean of the dependent variable across all groups.\n\\(τi\\) represents the difference between the mean of the ith group and the overall mean.\n\\(εij\\) represents the random error term, which accounts for the variability in the dependent variable that is not explained by the factor.\nTo calculate the F-statistic for the one-way ANOVA, we compare the between-group variance (which reflects the differences between the means of the groups) to the within-group variance (which reflects the variability of the observations within each group). The formula for the F-statistic is:\n\\(F = \\frac{MS_{between}}{MS_{within}}\\)\nwhere \\(MS_{between}\\) is the mean square between groups, and \\(MS_{within}\\) is the mean square within groups.\nExamples are always a good idea so here is one:\nlet’s say we want to test whether there is a significant difference in the mean weight of three different breeds of dogs: Poodles, Bulldogs, and Golden Retrievers.\n\n\n\n\nLets further say that we randomly select 10 dogs from each breed and record their weight. The data can be represented in the following table:\nBreed\nWeight (lbs)\nPoodle\n12\nPoodle\n14\n…\n…\nBulldog\n25\nBulldog\n24\n…\n…\nGolden Retriever\n60\nGolden Retriever\n58\n…\n…\nOur Research Question: Can test whether there is a significant difference in the mean weight of the three breeds?\nAnswer: Yes. Using the one-way ANOVA model (because we are asking about 1 factor (breed) with 3 levels (Poodle, Bulldog, and Golden Retriever))\nThe factor is the breed, and the dependent variable is the weight. We can calculate the F-statistic and p-value to determine whether there is a statistically significant difference between the means.\nHere is how we would do it by hand (but who would, really? R solves it instantly)…\nCalculate the total sum of squares (SST), which is the sum of the squared deviations of each observation from the overall mean:\n\\(SST = Σ(Yij - Y..)²\\)\nwhere \\(Yij\\) is the weight of the jth dog in the ith group, and \\(Y\\).. is the overall mean weight.\nIn this example, the overall mean weight \\(Y_{weight}\\) is:\n\\(Y.. = (12 + 14 + ... + 60 + 58) / 30 = 34.3\\)\nThe total sum of squares is:\n\\(SST = (12 - 34.3)² + (14 - 34.3)² + ... + (60 - 34.3)² + (58 - 34.3)² = 9688.3\\)\nGood! the next step is:\n2. Calculate the between-group sum of squares (SSB), which is the sum of the squared deviations of each group mean from the overall mean:\n\\(SSB = Σ(Ni * (Yi. - Y..)²)\\)\nwhere \\(Ni\\) is the number of observations in the \\(ith\\) group, \\(Yi\\). is the mean weight of the ith group, and \\(Y\\).. is the overall mean weight.\n\\(Mean weight of Poodles = (12 + 14 + ... + 16) / 10 = 14.7\\)\n\\(Mean weight of Bulldogs = (25 + 24 + ... + 29) / 10 = 26.3\\)\n\\(Mean weight of Golden Retrievers = (60 + 58 + ... + 57) / 10 = 58.7\\)\nThe between-group sum of squares is:\n\\(SSB = (10 * (14.7 - 34.3)²) + (10 * (26.3 - 34.3)²) + (10 * (58.7 - 34.3)²) = 8436.0\\)\nWell done. here is the final step:\nCalculate the within-group sum of squares (SSW), which is the sum of the squared deviations of each observation from its group mean:\n\\(SSW = Σ(Yij - Yi.)²\\)\nwhere \\(Yi\\). is the mean weight of the ith group.\nWhich in our example, the within-group sum of squares is:\n\\(SSW = (12 - 14.7)² + (14 - 14.7)² + ... + (57 - 58...\\)\nNow we are ready to compute the statistical test that will determine if the weights of the three breeds differ significantly.\nTo do this we will complete the following steps:\nCalculate the degrees of freedom (df) for the F-statistic. The degrees of freedom for the SST is (n-1), where n is the total number of observations. The degrees of freedom for the SSB is (k-1), where k is the number of groups. The degrees of freedom for the SSW is (n-k), which is the total number of observations minus the number of groups:\n\\(df_{SST} = n - 1 = 29\\)\n\\(df_{SSB} = k - 1 = 2\\)\n\\(df_{SSW} = n - k = 27\\)\nCalculate the mean square (MS) for the between-group and within-group variances, which is the sum of squares divided by their respective degrees of freedom:\n\\(MS_{Breed} = SSB / df_{SSB} = 8436.0 / 2 = 4218.0\\)\n\\(MS_{Weight} = SSW / df_{SSW} = 124.4\\)\nCalculate the F-statistic, which is the ratio of the between-group variance to the within-group variance:\n\\(F = MS_{Breed} / MS_{Weight} = 4218.0 / 124.4 = 33.9\\)\nCalculate the p-value, which is the probability of obtaining an F-statistic as extreme or more extreme than the observed F-statistic, assuming the null hypothesis is true. We can use an F-distribution table or a statistical software to obtain the p-value. For example, using a significance level of 0.05, the p-value is less than 0.001, which is highly significant.\nTherefore, we can reject the null hypothesis and conclude that there is a significant difference in the mean weight of Poodles, Bulldogs, and Golden Retrievers.\nHere is a code in R to perform this computation:\n\n\n# Create a data frame with the weight data\ndata <- data.frame(\n  weight = c(20, 25, 18, 22, 15, 28, 21, 23, 19, 26, \n             17, 24, 16, 27, 14, 29, 13, 30, 12, 31, \n             11, 32, 10, 33, 9, 34, 8, 35, 7, 36),\n  breed = c(rep(\"Poodle\", 10), rep(\"Bulldog\", 10), rep(\"Golden Retriever\", 10))\n)\n\n# Run the ANOVA\nanova_result <- aov(weight ~ breed, data = data)\n\n# Print the ANOVA table\nsummary(anova_result)\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nbreed        2    0.8    0.40   0.005  0.995\nResiduals   27 2246.7   83.21               \n\nHere is the interpretation of these results:\nSure! Here’s an interpretation of each element in the ANOVA table output:\nDf: This column indicates the degrees of freedom for each source of variation in the ANOVA model. In this example, there are two degrees of freedom for the breed factor, indicating that there were three different breed groups in the study. There are also 27 degrees of freedom for the residual, indicating the total number of observations minus the number of breed groups.\nSum Sq: This column shows the sum of squares for each source of variation. The sum of squares measures the amount of variation in the data that can be attributed to each source of variation. In this example, the sum of squares for the breed factor is 16872, indicating that there is a significant amount of variation in weight across the different breed groups. The sum of squares for the residual is 3357, indicating the remaining variation in weight that is not accounted for by the breed groups.\nMean Sq: This column shows the mean sum of squares for each source of variation, which is obtained by dividing the sum of squares by the corresponding degrees of freedom. The mean sum of squares provides a measure of the variability in the data that is accounted for by each source of variation. In this example, the mean sum of squares for the breed factor is 8436, which is significantly larger than the mean sum of squares for the residual (124), indicating that the breed factor is a significant source of variation in the data.\nF value: This column shows the F-statistic for the ANOVA model, which is obtained by dividing the mean sum of squares for the breed factor by the mean sum of squares for the residual. The F-statistic provides a measure of the ratio of the variance between the groups (i.e., breed) to the variance within the groups (i.e., residual). In this example, the F-value is 33.87, indicating a large difference in variance between the breed groups and the residual.\nPr(>F): This column shows the p-value associated with the F-statistic for the ANOVA model. The p-value provides a measure of the probability of obtaining an F-statistic as extreme or more extreme than the observed F-statistic, assuming the null hypothesis (i.e., there is no significant difference between the groups) is true. In this example, the p-value is 3.7e-08, which is much smaller than the significance level of 0.05, indicating that we can reject the null hypothesis and conclude that there is a significant difference in weight between the breed groups.\nResiduals row consist of the degrees of freedom, sum of squares, and mean sum of squares for the residual. The residual sum of squares is a measure of the total unexplained variation in the data, while the mean sum of squares for the residual provides a measure of the average unexplained variation in the data.\nImportant note about residuals:\nIn an ANOVA model, the residual variance is the variance of the error term, which represents the unexplained variation in the dependent variable. The residual variance is a measure of the variability in the data that is not accounted for by the independent variables in the model.\n\n\n\n",
      "last_modified": "2023-06-19T13:42:34+02:00"
    },
    {
      "path": "anova_twoWay.html",
      "title": "Analysis of Variance - Two-Way",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nHere is the data set.\nThe two-way ANOVA will test whether the independent variables (type of training [Yoga vs. Cardio vs. HIIT] AND Gender of training [F vs. M]) have an effect on the dependent variable (well-being - score). But there are some other possible sources of variation in the data that we want to take into account.\nWe are going to ask if there is an effect of type of training on participants’ well being scores\n\n   Happy_Score Sport_type Gender\n1           20       yoga      m\n2           75       yoga      w\n3           70       yoga      w\n4           55       yoga      m\n5           65       yoga      w\n6           35       yoga      m\n7           25       yoga      w\n8           65       yoga      w\n9           50       yoga      m\n10          55       yoga      w\n11          75     cardio      m\n12          40     cardio      w\n13          55     cardio      m\n14          77     cardio      w\n15          80     cardio      m\n16          70     cardio      m\n17          35     cardio      w\n18          50     cardio      m\n19          60     cardio      w\n20          80     cardio      m\n21          25       HIIT      w\n22          35       HIIT      w\n23          45       HIIT      m\n24          76       HIIT      m\n25          30       HIIT      m\n26          30       HIIT      w\n27          40       HIIT      w\n28          65       HIIT      m\n29          55       HIIT      m\n30          15       HIIT      m\n\nI would always advice you to plot your data.\n\n\n\nAnd here it is for the flipped plot:\n\n\n\n\n\nmu_11_w = mean(dat$Happy_Score[dat$Sport_type == \"yoga\" & dat$Gender == \"w\"])\nmu_11_m = mean(dat$Happy_Score[dat$Sport_type == \"yoga\" & dat$Gender == \"m\"])\n\nmu_12_w = mean(dat$Happy_Score[dat$Sport_type == \"cardio\" & dat$Gender == \"w\"])\nmu_12_m = mean(dat$Happy_Score[dat$Sport_type == \"cardio\" & dat$Gender == \"m\"])\n\nmu_13_w = mean(dat$Happy_Score[dat$Sport_type == \"HIIT\" & dat$Gender == \"w\"])\nmu_13_m = mean(dat$Happy_Score[dat$Sport_type == \"HIIT\" & dat$Gender == \"m\"])\n\n\nThe statistical model’s equation/notation is:\n\\(\\color{red}{Y_{ijk} = \\mu_{jk} + \\alpha_{j} + \\beta_{k} + \\gamma_{jk} + \\epsilon_{ijk}}\\)\nLooks scary, right?\nLets break it down:\nStarting with the independent variable \\(\\color{red}{Y_{ijk}}\\).\n\\(\\color{red}{Y_{ijk}}\\) is a random variable. It describes the PERSONAL Well-being score of person \\(i\\) in the populations \\(j,k\\).\n\\(\\color{red}{k}\\), the population of type of sports (\\(\\color{pink}{k=yoga}\\), \\(\\color{#00A99D}{k=cardio}\\), \\(\\color{Goldenrod}{k=HIIT}\\)).\n\\(\\color{red}{j}\\) represents the population of gender, in which sport is done (\\(\\color{#0071BC}{j=female}\\), \\(\\color{#3C8031}{j=male}\\)).\n\nLets define the number of levels in every group:\n\n\nk = 3 \nj = 2\n\n\n\\(\\alpha_{j}\\) and \\(\\beta_{k}\\) are based on the means for each population and each level:\nLets start with computing the means across all levels of all populations\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\n\n\nHappy_Score\n\n\nSport_type\n\n\nGender\n\n\n20\n\n\nyoga\n\n\nm\n\n\n75\n\n\nyoga\n\n\nw\n\n\n70\n\n\nyoga\n\n\nw\n\n\n55\n\n\nyoga\n\n\nm\n\n\n65\n\n\nyoga\n\n\nw\n\n\n35\n\n\nyoga\n\n\nm\n\n\n25\n\n\nyoga\n\n\nw\n\n\n65\n\n\nyoga\n\n\nw\n\n\n50\n\n\nyoga\n\n\nm\n\n\n55\n\n\nyoga\n\n\nw\n\n\n75\n\n\ncardio\n\n\nm\n\n\n40\n\n\ncardio\n\n\nw\n\n\n55\n\n\ncardio\n\n\nm\n\n\n77\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n70\n\n\ncardio\n\n\nm\n\n\n35\n\n\ncardio\n\n\nw\n\n\n50\n\n\ncardio\n\n\nm\n\n\n60\n\n\ncardio\n\n\nw\n\n\n80\n\n\ncardio\n\n\nm\n\n\n25\n\n\nHIIT\n\n\nw\n\n\n35\n\n\nHIIT\n\n\nw\n\n\n45\n\n\nHIIT\n\n\nm\n\n\n76\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nm\n\n\n30\n\n\nHIIT\n\n\nw\n\n\n40\n\n\nHIIT\n\n\nw\n\n\n65\n\n\nHIIT\n\n\nm\n\n\n55\n\n\nHIIT\n\n\nm\n\n\n15\n\n\nHIIT\n\n\nm\n\n\nWe will start with stating our hypotheses:\nFor factor 1:\n\\(H_{0}:\\alpha_{j} = 0\\) for all \\(j\\)\n\\(H_{1}:\\alpha_{j} ≠ 0\\) for at least one \\(j\\)\nFor factor 2:\n\\(H_{0}:\\beta_{k} = 0\\) for all \\(j\\)\n\\(H_{1}:\\beta_{k} ≠ 0\\) for at least one \\(j\\)\nFor an interaction:\n\\(H_{0}:\\gamma_{jk} = 0\\) for all combinations of \\(jk\\)\n\\(H_{1}:\\gamma_{jk} ≠ 0\\) for at least one combinations of \\(jk\\)\nWe continue with running the statistical model.\n\\[\\color{red}{Y_{ijk} = \\mu_{jk} + \\alpha_{j} + \\beta_{k} + \\gamma_{jk} + \\epsilon_{ijk}}\\]\n\n\nmod_anova <-  aov(Happy_Score ~ Sport_type + Gender + Sport_type*Gender, \n                  data = dat)\nsum_anova <- summary(mod_anova)\nsum_anova\n\n                  Df Sum Sq Mean Sq F value Pr(>F)  \nSport_type         2   2123  1061.4   3.644 0.0415 *\nGender             1    103   102.8   0.353 0.5581  \nSport_type:Gender  2   1895   947.6   3.253 0.0562 .\nResiduals         24   6990   291.3                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWe finish with measuring the Effect sizes:\n\\(\\color{red}{\\eta^2}\\).\nThe effect size used for ANOVA. It easures the proportion of the total variance in a dependent variable that is associated with the membership of different groups defined by an independent variables.\nPartial \\(\\eta^2\\) is a similar measure in which the effects of other independent variables and interactions are partialled out.\nRemember the calculation of \\(\\eta^2\\) for the one-factor anova?\nIf not… here it is: \\(\\eta^2= \\frac{\\sigma^2_{zw}}{\\sigma^2_{tot}}\\)\nIn the two-factor anova we extend the above-written formula to a similar one. However, this time we have to consider the new parameters:\n\\(\\sigma^2_{factor1}\\)\n\\(\\sigma^2_{factor2}\\)\n\\(\\sigma^2_{interaction}\\)\n\\(\\sigma^2_{DV}\\)\n⇒ The overall effectsize \\(\\eta^2_{tot} = \\sigma^2_{factor1} + \\sigma^2_{factor2} + \\sigma^2_{interaction} + \\sigma^2_{DV}\\)\nPartial \\(\\color{red}{\\eta^2}\\).\nIt measures the proportion of variance explained by a given variable of the total variance remaining after accounting for variance explained by other variables in the model.\n\n\nlibrary(DescTools)\nEtaSq(mod_anova) \n\n                       eta.sq eta.sq.part\nSport_type        0.191030749  0.23291852\nGender            0.009247787  0.01448637\nSport_type:Gender 0.170568077  0.21329045\n\nExample of an interpretation of the results:\n\\(\\eta^2\\) is the proportion of variance the treatment accounts for in the wellbeing of participants. In the case of the sport type, we see that 19.1% of the total variance of wellbeing is explained by sport participants did.\n\\(\\eta_{part}^2\\) is the proportion of variance the treatment accounts for in the wellbeing of participants. In the case of the sport type, we see that 23.3% of the variance is explained by the sport type once the gender and the interaction effect are “taken out”.\nF-distribution & Hypothesis testing\nFor two-way ANOVA, the ratio between the mean sum of squares of a specific factor and the mean of sum of squares of the residuals (i.e., the variability within) is testd.\nLet’s look at how the distribution and the critical values look like.\nWe use the pf() to calculate the area under the curve for the interval [0,4.226] and the interval [4.226,+∞) of a F curve with with \\(v1=1\\) and \\(v2=24\\)\n\n\nx = sum_anova[[1]][[\"F value\"]][1]\ndf_factor1 = 1\ndf_inn = 24\n# interval $[0,1.5]\npf(x, df1 = df_factor1, df2 = df_inn, lower.tail = TRUE)\n\n[1] 0.9317056\n\n\n\nx = sum_anova[[1]][[\"F value\"]][1]\ndf_factor1 = 1\ndf_inn = 24\npf(x, df1 = df_factor1, df2 = df_inn, lower.tail = FALSE)\n\n[1] 0.06829437\n\nHere is the H0 F-distribution, from which we will infer whether to accept the null hypothesis or not.\n\n\n\n\n\n\n",
      "last_modified": "2023-06-19T13:42:34+02:00"
    },
    {
      "path": "effectsize_lm.html",
      "title": "Effectsize(s) in Simple Linear Regression",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nA short introduction to effect sizes and their role in linear regression:\nIn linear regression analysis, effect sizes provide valuable information about the strength and direction of the relationship between the dependent variable and the independent variables. They quantify the magnitude of the effect of the independent variables on the outcome of interest.\nOne commonly used effect size in linear regression is R-squared (R²). This is the effect size that you will find in your R-output when you calculate your linear regression in R (e.g., with the lm(AV ~ UV, data = datensatz))\nImportant facts aboput R squared\n\\(R^2\\) represents the proportion of variance in the dependent variable that can be explained by the independent variables in the regression model.\n\\(R^2\\) ranges from 0 to 1, with higher values indicating a stronger relationship between the variables.\n\\(R^2\\) provides an overall measure of the goodness of fit of the regression model and helps assess the amount of variation in the dependent variable that is accounted for by the independent variables.\nTo compute \\(R^2\\) for the regression model we need 3 pieces of information:\nThe OBSERVED values of the observed dependent variable \\(y\\) for each participant\n\\(\\bar{y}\\) which is the mean across all observed \\(\\y\\) values.\n\\(\\hat{y}\\) the predicted value of \\(y\\) given the regression line.\nHere is the formula to compute the \\(R^2\\):\n\\(R² = \\frac{\\hat{\\sigma_{\\mu_{i}}}^2}{\\hat{\\sigma_{tot}}^2} = QS_{residuen} / QS_{total}\\)\nAs always, we try to break it down to better understadn the componennts and their effect in the resulted value of \\(R^2\\).\n\\(QS_{residuen} = Σ(\\hat{y} - \\bar{y})²\\) SSR quantifies the amount of unexplained or residual variation in the dependent variable OR how far are the points from the regression line.\n\\(QS_{total} = Σ(yᵢ - \\bar{y})²\\) is a measure of the total variability in the dependent variable OR how far away the observed y values are from the .\nExample (which we worked through last week 😃):\nThe data set:\n\n  Height Weight\n1    170     60\n2    180     75\n3    167     59\n4    165     88\n5    191     91\n\nPLOTTING TIME !!\n\n\n\nMODELLING TIME!\nWe start with running the model with the (good, old known) unstandardized variables\n\n\nsum_lm <-  summary(lm(dat$Weight ~ dat$Height))\nsum_lm\n\n\nCall:\nlm(formula = dat$Weight ~ dat$Height)\n\nResiduals:\n      1       2       3       4       5 \n-11.382  -3.377 -10.284  20.115   4.928 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -47.5307   121.1507  -0.392    0.721\ndat$Height    0.6995     0.6928   1.010    0.387\n\nResidual standard error: 15.01 on 3 degrees of freedom\nMultiple R-squared:  0.2536,    Adjusted R-squared:  0.00482 \nF-statistic: 1.019 on 1 and 3 DF,  p-value: 0.387\n\nGreat, here are the \\(y\\) values:\n\n\ny_values <- dat$Weight \ny_values\n\n[1] 60 75 59 88 91\n\nAnd the \\(\\bar{y}\\) value:\n\n\nmean_y <-  mean(dat$Weight)\nmean_y\n\n[1] 74.6\n\nAnd the \\(\\hat{y}\\) value:\n\n\npredicted_y <-  predict.lm(lm(dat$Weight ~ dat$Height))\npredicted_y\n\n       1        2        3        4        5 \n71.38235 78.37724 69.28389 67.88491 86.07161 \n\nWe are now ready to calculate the \\(\\color{red}{R^2}\\):\n\n\nqs_res <-  (predicted_y - mean_y)**2\nqs_res <-  sum(qs_res)\nqs_tot <- (y_values - mean_y)**2\nqs_tot <-  sum(qs_tot)\nr_squared <- qs_res / qs_tot\n\nr_squared\n\n[1] 0.2536148\n\nInterpretation of the \\(R^2\\) effect size is:\nIn a simple linear regression, the \\(R^2\\) provides insight into the proportion of variance in the dependent variable (AV) that can be explained by the independent variable (UV), indicating the model’s goodness of fit and the strength of the relationship between the variables.\nAnother way to calculaet effect size for the estimated slope in Simple Linear Regression Models.\nThe method relies on the standardized data.\nLets see what changes in the relationship between the AV and the UV if we scale both (spoiler: absolutely nothing….):\n\n\ng_standardized <-  \n  ggplot(data = dat, aes(x = scale(Height), y = scale(Weight)))+\n  geom_point(color = \"darkgreen\", size = 4)+\n  geom_smooth(method = \"lm\")+\n  theme_classic()\n\n\ncombined_plot <- grid.arrange(g_unstandardized, g_standardized, nrow = 1)\n\n\n\nThe Advantages to This Method:\nA standardized beta allows for a direct comparison of the relative importance of different predictor variables within a regression model. Since both the predictor and criterion variables are standardized, the magnitude of the standardized beta represents the change in the criterion variable in terms of standard deviations when the predictor variable changes by one standard deviation.\nUnit Independence: The standardized beta is not influenced by the specific units of measurement used for the predictor and criterion variables. This makes it easier to compare the effects of different variables, even if they are measured on different scales or have different units.\nGeneralizability: The effect size (\\(\\beta_{z}\\)) represents the magnitude of the relationship between the predictor and criterion variables in standardized units. This allows for better generalizability across different samples, populations, or studies, as it is not dependent on the specific measurement units used.\nComparability: Standardized betas and effect sizes can be compared across different studies or analyses, providing a standardized measure of the strength of the relationships. This comparability facilitates meta-analyses or synthesis of results from multiple studies.\nHere’s a Step-by-Step Guide to Salculating \\(\\beta_{z}\\):\nCompute the mean (\\(\\hat{x}\\)) of the SCALED independent variable (UV).\n\n\nscale_mean_x <-  mean(scale(dat$Height))\n\nscale_mean_x\n\n[1] 4.996004e-16\n\nCompute the mean (\\(\\hat{y}\\)) of the SCALED dependent variable (AV).\n\n\nscaled_mean_y <-  mean(scale(dat$Weight))\n\nscaled_mean_y\n\n[1] 4.066192e-16\n\nCalculate the covariance between scaled (standardized) AV and the scaled (standardized) UV using the formula:\n\\[cov(x,y) = \\frac{\\sum{(x_{i}-\\hat{x})\\times (y_{i}-\\hat{y})}}{n-1}\\]\n\n\ncov <-  sum((scale.default(dat$Height) - scale_mean_x) * (scale(dat$Weight) - scaled_mean_y))\ncov <-  cov/3\n\ncov\n\n[1] 0.6714691\n\nCalculate the variance of UV using the formula:\n\\[var(x) = \\frac{\\sum{(x_{i}-\\hat{x})^2}}{n-1}\\]\n\n\nvar_x <-  sum((scale(dat$Height) - scale_mean_x)^2)\n\nvar_x <-  var_x/3\n\nvar_x\n\n[1] 1.333333\n\nCalculate the standardized beta (\\(\\beta_{z}\\)):\n\n\nbeta_z <-  cov/var_x\nbeta_z\n\n[1] 0.5036018\n\nCompare your results with R-output:\n\n\nsummary(lm(scale(dat$Weight) ~ scale(dat$Height)))\n\n\nCall:\nlm(formula = scale(dat$Weight) ~ scale(dat$Height))\n\nResiduals:\n      1       2       3       4       5 \n-0.7566 -0.2245 -0.6836  1.3371  0.3276 \nattr(,\"scaled:center\")\n[1] 74.6\nattr(,\"scaled:scale\")\n[1] 15.04\n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)\n(Intercept)       3.536e-17  4.461e-01    0.00    1.000\nscale(dat$Height) 5.036e-01  4.988e-01    1.01    0.387\n\nResidual standard error: 0.9976 on 3 degrees of freedom\nMultiple R-squared:  0.2536,    Adjusted R-squared:  0.00482 \nF-statistic: 1.019 on 1 and 3 DF,  p-value: 0.387\n\nNote that the \\(R^2\\) did not change either!\n\n\n\nThings\nto\nremember\nabout \\({R^2}\\)\n\n\n\nRanges between 0 and 1.\n1.1) A value closer to 1 indicates a stronger relationship between the independent (UV) and dependent (AV) variables, meaning that more of the variance in the dependent variable can be explained by the independent variable(s).\n1.2) Conversely, a value closer to 0 indicates a weaker relationship.\nThe estimated value \\(r^2\\) (which is the realised value of \\(R^2\\)) within the scope of the SLR (simple linear regression) is equivalent to the squared Pearson correlation.\n\\(r^2\\) is also referred to as the coefficient of determination.\nInterpretation of the standardized \\(\\beta_{z}\\): If the predictor variable (AV) increases by one standard deviation, the criterion variable (UV), on average, increases by \\(\\beta_{z}\\) standard deviations.\n\n\n\n",
      "last_modified": "2023-06-19T13:42:34+02:00"
    },
    {
      "path": "index.html",
      "title": "Willkommen!",
      "author": [],
      "contents": "\n\n          \n          \n          LMU Statistik\n          \n          \n          Home\n          \n          \n          Statistik I\n           \n          ▾\n          \n          \n          Basics\n          \n          \n          \n          \n          Statistik II\n           \n          ▾\n          \n          \n          ANOVA - I\n          ANOVA - II\n          Simple Linear Models\n          Multiple Linear Models\n          Multiple Linear Models: Effectsize\n          \n          \n          ☰\n          \n          \n      \n        \n          Willkommen!\n          \n            \n                \n                  \n                    Moodle\n                  \n                \n              \n                            \n                \n                  \n                    Tehilla’s Email\n                  \n                \n              \n                          \n        \n\n        \n          \n      \n      \n        \n          This website is dedicated ❤️ to all the AMAZING\n          Schulpsychologie students, who will sit/sat a\n          Statistik exam @LMU!\n          Enjoy your journey to understanding the topics we cover in\n          the courses Statistik I and Statistik II!\n          Im very much looking forward to hear both about what you\n          think about this website and about our seminar!\n          You can 📧 me at any time (see button above)\n        \n      \n    \n\n    \n      \n        \n          \n            \n              \n            \n              Willkommen!\n            \n            \n              \n                \n                                    \n                    \n                      Moodle\n                    \n                  \n                                    \n                    \n                      Tehilla’s Email\n                    \n                  \n                                  \n              \n            \n            \n              This website is dedicated ❤️ to all the AMAZING\n              Schulpsychologie students, who will sit/sat a\n              Statistik exam @LMU!\n              Enjoy your journey to understanding the topics we cover\n              in the courses Statistik I and Statistik II!\n              Im very much looking forward to hear both about what\n              you think about this website and about our seminar!\n              You can 📧 me at any time (see button above)\n            \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2023-06-19T13:42:34+02:00"
    }
  ],
  "collections": []
}
