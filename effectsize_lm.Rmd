---
title: "Effectsize(s) in Simple Linear Regression"
description: |
author:
  - name: Tehilla Ostrovsky
date: "`r Sys.Date()`"
output: distill::distill_article
css: "my_css.css"
---

## A short introduction to effect sizes and their role in linear regression:

In linear regression analysis, effect sizes provide valuable information about the strength and direction of the relationship between the dependent variable and the independent variables. They quantify the magnitude of the effect of the independent variables on the outcome of interest.


Before we dive into the task of calculating the effect size for our linear models I must remind you of 4 assumptions we rely on for running linear models:

1) Linearity: Linearity: The relationship between the dependent variable and the independent variables is assumed to be linear. This means that the effect of each independent variable on the dependent variable is constant and additive.
2) Normality: The residuals (the differences between the observed values and the predicted values) are assumed to follow a normal distribution. This assumption implies that the errors are normally distributed with a mean of zero. 
3) Homoscedasticity: The variance of the errors (residuals) is assumed to be constant across all levels of the independent variables. In other words, the spread or dispersion of the residuals should be roughly the same for all values of the independent variables.
4) No outliers

We shall discuss these at the end of this blog so you'll need to be patient 😃


<div>
<div class="container_new">
  <h5 class="title_new">
    <span class="title-word title-word-1">OK! </span>
    <span class="title-word title-word-2">Back</span>
    <span class="title-word title-word-3">to</span>
    <span class="title-word title-word-4">Effect Sizes</span>
  </h5>
</div>
</div>


One commonly used effect size in linear regression is R-squared (R²). This is the effect size that you will find in your R-output when you calculate your linear regression in R (e.g., with the lm(AV ~ UV, data = datensatz))


### Important facts aboput R squared 
  - $R^2$ represents the proportion of variance in the dependent variable that can be explained by the independent variables in the regression model. 

  - $R^2$ ranges from 0 to 1, with higher values indicating a stronger relationship between the variables. 

  - $R^2$ provides an overall measure of the goodness of fit of the regression model and helps assess the amount of variation in the dependent variable that is accounted for by the independent variables.

### **To compute $R^2$ for the regression model we need 3 pieces of information:**

  1. The OBSERVED values of the observed dependent variable $y$ for each participant
  2. $\bar{y}$ which is the mean across all observed $\y$ values. 
  3. $\hat{y}$ the predicted value of $y$ given the regression line. 
  
  
### Here is the formula to compute the $R^2$:

$R² = \frac{\hat{\sigma_{\mu_{i}}}^2}{\hat{\sigma_{tot}}^2} = QS_{residuen} / QS_{total}$



### As always, we try to break it down to better understadn the componennts and their effect in the resulted value of $R^2$.

  1)  $QS_{residuen} = Σ(\hat{y} - \bar{y})²$ SSR quantifies the amount of **unexplained or residual variation** in the dependent variable OR how far are the points from the regression line. 
  2)  $QS_{total} = Σ(yᵢ - \bar{y})²$ is a measure of the **total variability** in the **dependent variable** OR how far away the observed y values are from the .

## Example (which we worked through last week 😃):

The data set:

```{r echo=FALSE, warning=FALSE, message=FALSE}
dat <-  data.frame(
  "Height" = c(170, 180, 167, 165, 191),
  "Weight" = c(60, 75, 59, 88, 91))
dat
```

### PLOTTING TIME !!


```{r echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(gridExtra)

g_unstandardized <-  
  ggplot(data = dat, aes(x = Height, y = Weight))+
  geom_point(color = "darkgreen", size = 4)+
  geom_smooth(method = "lm")+
  theme_classic()

g_unstandardized
```




### MODELLING TIME!

We start with running the model with the (good, old known) unstandardized variables


```{r warning=FALSE, message=FALSE}
lm <-  lm(dat$Weight ~ dat$Height)
sum_lm <-  summary(lm)
sum_lm
```



We continue with computing the confidence interval for the $\alpha$ and $\beta$

```{r}
confint(lm)
```



### Great, here are the $y$ values: 

```{r warning=FALSE, message=FALSE}
y_values <- dat$Weight 
y_values
```


### And the $\bar{y}$ value: 

```{r warning=FALSE, message=FALSE}
mean_y <-  mean(dat$Weight)
mean_y
```

### And the $\hat{y}$ value: 

```{r warning=FALSE, message=FALSE}
predicted_y <-  predict.lm(lm(dat$Weight ~ dat$Height))
predicted_y
```


### We are now ready to calculate the $\color{red}{R^2}$:

```{r  warning=FALSE, message=FALSE}
qs_res <-  (predicted_y - mean_y)**2
qs_res <-  sum(qs_res)
qs_tot <- (y_values - mean_y)**2
qs_tot <-  sum(qs_tot)
r_squared <- qs_res / qs_tot

r_squared
```

###  Interpretation of the $R^2$ effect size is: 
In a simple linear regression, the $R^2$ provides insight into the proportion of variance in the dependent variable (AV) that can be explained by the independent variable (UV), indicating the **model's goodness of fit** *and* the **strength of the relationship between the variables**. 

---

## Another way to calculaet effect size for the estimated slope in Simple Linear Regression Models. 

The method relies on the standardized data. 

Lets see what changes in the relationship between the AV and the UV if we scale both (spoiler: absolutely nothing....):

```{r warning=FALSE, message=FALSE}
g_standardized <-  
  ggplot(data = dat, aes(x = scale(Height), y = scale(Weight)))+
  geom_point(color = "darkgreen", size = 4)+
  geom_smooth(method = "lm")+
  theme_classic()


combined_plot <- grid.arrange(g_unstandardized, g_standardized, nrow = 1)
```


### The Advantages to This Method:

A standardized beta allows for a direct comparison of the relative importance of different predictor variables within a regression model. Since both the predictor and criterion variables are standardized, the magnitude of the standardized beta represents the change in the criterion variable in terms of standard deviations when the predictor variable changes by one standard deviation.
  
  1) **Unit Independence**: The standardized beta is not influenced by the specific units of measurement used for the predictor and criterion variables. This makes it easier to compare the effects of different variables, even if they are measured on different scales or have different units.

  2) **Generalizability**: The effect size ($\beta_{z}$) represents the magnitude of the relationship between the predictor and criterion variables in standardized units. This allows for better generalizability across different samples, populations, or studies, as it is not dependent on the specific measurement units used.
  
  3) **Comparability**: Standardized betas and effect sizes can be compared across different studies or analyses, providing a standardized measure of the strength of the relationships. This comparability facilitates meta-analyses or synthesis of results from multiple studies.




### Here's a Step-by-Step Guide to Salculating $\beta_{z}$:


### 1) Compute the mean ($\hat{x}$) of the SCALED independent variable (UV).
  
```{r warning=FALSE, message=FALSE}
scale_mean_x <-  mean(scale(dat$Height))

scale_mean_x

```
  
### 2) Compute the mean ($\hat{y}$) of the SCALED dependent variable (AV).
  
```{r warning=FALSE, message=FALSE}
scaled_mean_y <-  mean(scale(dat$Weight))

scaled_mean_y

```
  
  
### 3) Calculate the covariance between scaled (standardized) AV and the scaled  (standardized) UV using the formula:

  $$cov(x,y) = \frac{\sum{(x_{i}-\hat{x})\times (y_{i}-\hat{y})}}{n-1}$$
  
```{r warning=FALSE, message=FALSE}
cov <-  sum((scale.default(dat$Height) - scale_mean_x) * (scale(dat$Weight) - scaled_mean_y))
cov <-  cov/3

cov
```

### 4)  Calculate the variance of UV using the formula:
  $$var(x) = \frac{\sum{(x_{i}-\hat{x})^2}}{n-1}$$

```{r  warning=FALSE, message=FALSE}
var_x <-  sum((scale(dat$Height) - scale_mean_x)^2)

var_x <-  var_x/3

var_x
```

### 5) Calculate the standardized beta ($\beta_{z}$):
  
```{r warning=FALSE, message=FALSE}
beta_z <-  cov/var_x
beta_z
```


### 6) Compare your results with R-output:
  
  
```{r warning=FALSE, message=FALSE}
lm_z <-  lm(scale(dat$Weight) ~ scale(dat$Height))

summary(lm_z)
```
  
### 7) Note that the $R^2$ did not change either!



### We continue with computing the confidence interval for the $\alpha_{z}$ and $\beta_{z}$

```{r}
confint(lm_z)
```


<div>
<div class="container_new">
  <h5 class="title_new">
    <span class="title-word title-word-1">Now</span>
    <span class="title-word title-word-2">.</span>
    <span class="title-word title-word-3">.</span>
    <span class="title-word title-word-4">.</span>
  </h5>
</div>
</div>


We know that since we have "collected" only 5 observations, this experiment might suffer from low power 🥵🦾... We even have a good reason to believe so because the relationship between height and weight may hold in reality...

We can determine the sample size we will need to achieve significant results 

**BUT Careful!** this method is no magic! 🪄 we will only be likely to obtain significant results **only if there is, indeed, a relationship between height and weight**)

We have to first calculate the 
```{r}
rho_squared = coef(lm_z)["scale(dat$Height)"]^2

f_squared = rho_squared/(1-rho_squared)

f_squared

```

We use the power calculation function pwr.f2.test(). It takes the following arguments: $u$ which is the number of predictors we have in our model. In a simple linear regression we always ahve only 1, $f2$ which is the effect size, f squared, which we calculated above. The other arguments are $sig.level$ that determines the significance level (Type I error probability) and $power$ represents the power we wish to achieve (1 minus Type II error probability)




```{r}
library(pwr)
pwr.f2.test(u = 1, f2 = f_squared, sig.level = 0.005, power = 0.8)
```

Interpretation: Because 𝜈 = 𝑛 − 2 (we estimate 2 variables), we must add those 2 to know what the required sample (i.e., 𝑣 + 2). In our case, we will need 43 subjects (41.<img src = "img/half_subj.png" style = "height:30px; width:20px; margin-top:2px"></img> + <img src = "img/full_face.png" style = "height:30px; width:30px; margin-top:2px"></img><img src = "img/full_face.png" style = "height:30px; width:30px; margin-top:5px"></img>). 

      


----


<div>
<div class="container_new">
  <h5 class="title_new">
    <span class="title-word title-word-1">Things</span>
    <span class="title-word title-word-2">to</span>
    <span class="title-word title-word-3">remember</span>
    <span class="title-word title-word-4">about ${R^2}$</span>
  </h5>
</div>
</div>

---


<!-- <div><h3>Things to remember about </h3></div> -->

1) Ranges between 0 and 1. 

    1.1) A value closer to 1 indicates a stronger relationship between the independent (UV) and dependent (AV) variables, meaning that more of the variance in the dependent variable can be explained by the independent variable(s). 
    
    1.2) Conversely, a value closer to 0 indicates a weaker relationship.


  2) The estimated value $r^2$ (which is the realised value of $R^2$) within the scope of the SLR (simple linear regression) is equivalent to the squared Pearson correlation.

  3) $r^2$ is also referred to as the coefficient of determination.
  
  4) Interpretation of the standardized $\beta_{z}$: If the predictor variable  (AV) increases by one standard deviation, the criterion variable (UV), on average, increases by $\beta_{z}$ standard deviations.
  
  
  
# Back to the summptions I mentioned at the beginning of this blog:

1) Linearity. We will test this assumption with a simple scatter plot. To make the point here, I will create a new, richer data set.



```{r echo=FALSE, warning=FALSE, message=FALSE}

# Load the required package
library(mvtnorm)

# Set the desired correlation coefficient
correlation <- 0.6

# Set the means and standard deviations for the variables
mean1 <- 100
sd1 <- 5
mean2 <- 65
sd2 <- 3

# Set the desired sample size
sample_size <- 100
# Generate the correlation matrix
cor_matrix <- matrix(c(1, correlation, correlation, 1), nrow = 2)
# Generate the data based on the correlation matrix
data <- rmvnorm(sample_size, mean = c(mean1, mean2), sigma = cor_matrix)

dat_rich <- as.data.frame(data)

colnames(dat_rich) <-  c("Height", "Weight")

```


### In order to examine those assumptions, we will runa. linear regression (we will need it to exmain the distribution of the residuals).



```{r echo=TRUE, warning=FALSE, message=FALSE}
library(gridExtra)

lm_2 <-  lm(Weight ~ Height, data = dat_rich)
 
summary(lm_2)
```


### Lets plot all the figures we need to judge of the Linear regression assumptions are met. 

```{r echo=TRUE, warning=FALSE, message=FALSE}
library(ggplot2)
library(gridExtra)
stand_res <-  as.data.frame(rstandard(lm_2))


p_1 <-  ggplot(data = dat_rich, aes(x = Height, y = Weight)) +
  geom_point(col = "lightblue", size = 3) +
  geom_smooth(method = "lm", ) +
  ggtitle("Linearity Assumption")+
  theme_classic()



predicted <-  as.data.frame(predict(lm_2, interval = "prediction", level = .95))
predicted$stand_res <-  stand_res$`rstandard(lm_2)`

p_2 <-   ggplot(data = predicted, aes(fit, stand_res))+
  geom_point(col = "lightblue")+
  ggtitle("Homoskedasticity Assumption") +
  geom_hline(yintercept = 0) + 
  xlab("Predicted Value of DV") +
  ylab("Standardized Residuals") +
  theme_classic()

cook_dat <-  as.data.frame(round(cooks.distance(lm_2), digits = 3))
cook_dat$id <-  cbind(1:sample_size)

p_3 <-   ggplot(data = cook_dat, aes(x = id, y = `round(cooks.distance(lm_2), digits = 3)`))+
  geom_point(col = "lightblue")+
  ggtitle("Outliers Assumption") +
  geom_hline(yintercept = 0.04) + 
  xlab("Observation IDs") +
  ylab("Cooks Distances") +
  theme_classic()

# Arrange the plots in a grid
combined_plots <- grid.arrange(p_1, p_2, p_3, layout_matrix = rbind(c(1, NA), c(2, 3)), 
                               heights = c(3,3), widths = c(5, 5))


```


### A quick reminder of the way Cooks Distance is computed:

Cook's D(i) = $\frac{{\Delta \hat{y_i}^2}}{{p}} \times \frac{{h_{ii}}}{{(1 - h_{ii})}}$
  


### And lastly, (for the statistics nurds among us....😉), here is the model with the standardized coefficients: 

```{r warning=FALSE, message=FALSE}
scaled_dat_rich <-  as.data.frame(round(scale(dat_rich), digits = 2))
lm_z_2 <-  lm(scaled_dat_rich$Weight ~scaled_dat_rich$Height)

summary(lm_z_2)

confint.lm(lm_z_2)
```