---
title: "Effectsizes in Linear regression"
description: |
author:
  - name: Tehilla Ostrovsky
date: "`r Sys.Date()`"
output: distill::distill_article
---


## A short introduction to effect sizes and their role in linear regression:

In linear regression analysis, effect sizes provide valuable information about the strength and direction of the relationship between the dependent variable and the independent variables. They quantify the magnitude of the effect of the independent variables on the outcome of interest.

One commonly used effect size in linear regression is R-squared (R²). 

R-squared represents the proportion of variance in the dependent variable that can be explained by the independent variables in the regression model. It ranges from 0 to 1, with higher values indicating a stronger relationship between the variables. R-squared provides an overall measure of the goodness of fit of the regression model and helps assess the amount of variation in the dependent variable that is accounted for by the independent variables.


### here is the formula to compute the $R^2$



To compute $R^2$ for the regression model we need 3 pieces of information:

  1. the values of the observed dependent variable $y$ for each participant
  2. the mean across all observed $y$ values. 
  3. the predicted value of $y$ given the regression line. 
  

#### The total sum of squares (SST) represents the total variability in the dependent variable. It measures how much the dependent variable varies without considering the independent variables. 


To calculate SST, sum the squared differences between each observed dependent variable value (y) and the mean of the dependent variable (ȳ):


$$SST = Σ(y - ȳ)²$$