---
title: "Simple Linear Regression Analysis"
description: |
  Linear Regression
author:
  - name: Tehilla Ostrovsky
    url: https://github.com 
    affiliation: LMU
    #affiliation_url: 
date: "`r Sys.Date()`"
output: distill::distill_article
google_analytics: "G-T7V19FRHDE"
---

### Welcome to the post about linear models in statistics! 

Linear models are an essential tool in statistics used to model the relationship between a dependent variable and one or more independent variables. 

In this post, we'll explore linear models in more detail and introduce an interactive Shiny app plot that will help you visualize the concepts.

Before we dive into the interactive part of this post (in which you get to explorer different regression lines using different variables), let's first define what a linear model is. 

A linear model is a mathematical equation that represents a **linear** relationship between two or more variables. 

The simplest form of a linear model is a straight line equation of the form:

$Y_{i} = \alpha_{0} + \beta_{1} \times X_{1}$

Where $Y_{i}$ is the dependent variable and represent the *expected value* of all $y_{i}$ (all single data points) given a specific value of $X_{i}$ 


### We are going to use a very simple example. We will try to fit a model that aims to predict the relationship between individuals height and weight. 

### here is a fun illustration of the simple model:

<video width="500" height="300" float = "left" controls><source src="img/CoordExmp.mp4" type="video/mp4"></video>



### Let us look at a simple example with little number of data points (just so we can get the feeling of whats going on under the hood)


```{r echo=FALSE, warning=FALSE, message=FALSE}

dat <-  data.frame(
  "Height" = c(170, 180, 167, 165, 191),
  "Weight" = c(60, 75, 59, 88, 91))
dat
```


#### Here are the averages of both the weights and heights. We will need those to calculate the intercpet and the slope of the model. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
mean_weight <-  mean(dat$Weight)

mean_height <-  mean(dat$Height)

mean <- c(mean_weight, mean_height) 

mean
```


#### Remember how I told you that I am a fan of plotting the data? 

#### We will use a scatterplot this time.

```{r}
library(ggplot2)
ggplot(data = dat, aes(y = Weight,x = Height)) +
  geom_point(alpha = .7, color = "red") +
  geom_hline(yintercept = mean_weight, color="blue", linetype = "dashed") +
  geom_vline(xintercept = mean_height, color = "green", linetype = "dashed") + 
  #geom_smooth(method = "lm") +
  theme_classic() 
```



### How is the $\beta$ calculated?

A quick reminder: 	Î² represents how much the regression line will rise or fall. 

We interpret the $\beta$  as the average increase in the Dependent variable (AV) when we increase the independent variable (UV) by one unit. For Example, if we increase the Height by 1cm, the average predicted increase of weight is .....

$$\frac{\sum_{i=1}^{n}(x_{i} -\hat{x})\times(y_{i}-\hat{y})}{\sum_{i=1}^{n}(x_{1} - \hat{x})^2}$$

Which can be also written as:


$$\frac{cov(X,Y)}{S_{x}^2}$$



#### Lets start!

```{r echo=FALSE, warning=FALSE, message=FALSE, }
dat[,"xi - mean(x)"] <-  dat$Height - mean_height
dat[,"yi - mean(y)"] <-  dat$Weight - mean_weight

dat[, "sqr(xi - mean(x)"] <- (dat$Height - mean_height)**2
dat[, "(xi - mean(x)) X (yi - mean(y))"] <-  dat[,"xi - mean(x)"] * dat[,"yi - mean(y)"] 

dat
```

#### Now we have everything to compute the $cov(X, Y)$ and the $S_{X}^2$

For $cov(X, Y)$ we just need to compute the sum of the **6th column**

```{r}
sum(dat[,"(xi - mean(x)) X (yi - mean(y))"])
```

For $S_{x}^2$ we just need to compute the sum of the **5th column**

```{r}
sum(dat[, "sqr(xi - mean(x)"])
```


#### The slope, is therefore, 0.6994885

```{r echo=FALSE}
328.2/469.2
```

$\beta = \frac{328.2}{469.2} = .69$ 

This means that, for every 1cm increase in height we expect to see an increase of .69KG. 



### How does the $\alpha$ calculated?

$$\overline{y} = \alpha_{0} + .69\times \overline{x}$$

We know both means of $x$ and $y$ (from the calculation above)


If we rearrange the equation to solve for $\alpha_{0}$, we get 

$$-\alpha_{0} = -\overline{y} + .69\times \overline{x}$$ 

Lets rearrange the equation such that it will look nicer...

$$\alpha_{0} = \overline{y} - .69\times \overline{x}$$



```{r}
alpha_0 = mean_weight - .6994885 * (mean_height)
alpha_0
```



OK, enough with the hard, tiring work of calculating everything by hand.... for exactly this reason we have R (ðŸ˜ƒ).

We will use the function lm(), which which for Linear Model. We will wrap it with the function summary(), which provides us with the result summary of our model's results. 



```{r}
sum_lm <-  summary(lm(dat$Weight ~ dat$Height))
sum_lm
```
According to the model output, the intercept and the slope are not significant. 

According to the hypotheses of this mode, the intercept is compared against 0. That is, does -47.5307 is significantly different to 0. Looking at the estimated value itself it may seem odd that this value is not significant differernt to 0. To resolve this mysotory lets take a llok at the confidence intervals of the two:

```{r}
confint.lm(lm(dat$Weight ~ dat$Height))
```
### How is the cofidence interval calculated?

For example: 95% C.I. for $\beta_{1}$: $$b_{1} Â± t_{1-Î±/2, n-2} * se(b_{1})$$

First we need to find the t value that "sits" at the lower and upper 2.5% of the t-distribution. We will use the function qt() and will provide us with the t value at 2.5% from a t-ditribution with 3 df. 

```{r}
t_value <-  qt(p = .975, df =3, lower.tail = TRUE)
```


### We now have all the unknowns to arrive at the solution. 

### Lets plug the numbers in and compare it to the 95% confidence interval we obtained above. 


```{r}
upper_CI <-  0.6995 + (t_value * 0.6928) 
lower_CI <-  0.6995 - (t_value * 0.6928) 

upper_CI
lower_CI
```

Looking good!



Lets add this line to our scatterplot from before.

```{r}
ggplot(data = dat, aes(y = Weight, x = Height)) +
  geom_point(alpha = .7, color = "red") +
  geom_hline(yintercept = mean_weight, color="blue", linetype = "dashed") +
  geom_vline(xintercept = mean_height, color = "green", linetype = "dashed") + 
  geom_smooth(method = "lm") +
  theme_classic() 
```



### Lastly, here is how the standard error term is calculated. This term tells us the distance between the data points with respect to their y values and the best fitting line.  

$$\sigma^2 = S^2 = \frac{\sum_{i=1}^n(Y_{i} - \hat{Y})}{n-2}$$


We need the predicted value for each person in our dataset. We will use the predict.lm() function that will output the predicted values fro every person based on the model lm(weight ~ Height.



```{r}
lm <-  lm(dat$Weight ~ dat$Height)
dat[,"predicted_y"] <-  predict.lm(lm)

dat[,"predicted_y"] 

dat[, "sqr(y-perd(y))"] <- (dat$Weight - dat$predicted_y)**2

dat[, "sqr(y-perd(y))"]

sum <- sum(dat[, "sqr(y-perd(y))"])

sqrt(sum/3)



```

---


Here is how the t-distribution under the  $H_{0}$ looks like:

```{r}
library(latex2exp)
set.seed(123)
x <- rt(1000, df = 3)
x <-  round(x, digit = 3)
cuts <-  quantile(x , c(0.000, .05, .95, .99999)) 

# Create data
my_variable = x
 
# Calculate histogram, but do not draw it
my_hist = hist(x , breaks = 150  , plot = F)

 
# Color vector
my_color= ifelse(my_hist$breaks <= 2.273526, "lightgrey",
          ifelse(my_hist$breaks >= -2.273526, "red", rgb(0.2,0.2,0.2,0.2)))
 
# Final plot
plot(my_hist, 
     col = my_color, 
     border = F,
     freq = FALSE, 
     main = (TeX('Histogram for a $\\t$-distribution with 3 degrees of freedom (df)')),
     xlab = "possible t values", 
     xlim = c(-5,5), 
    cex.main=0.9)
curve(dt(x, df = 3), from = -5, to = 5, n = 500, col = 'red', lwd = 1, add = T)
abline(v = 0.721, col = "green", lwd = 2,  lty = 'dashed')
abline(v = 0.387, col = "blue", lwd = 2,  lty = 'dashed')
abline(v = 2.273526, col = "red", lwd = 3)


```

---

#### And a small toy-example for you to play around with ðŸ˜‰


Let's move onto the interactive Shiny app plot. The plot allows you to visualize the relationship between two variables and fit a linear model to the data. To use the app, follow these steps:

1. Choose the variables from the dropdown menu.

2. Observe how the line changes to fit the data points better.

3. Observe how the changes in: 
  - Intercept
  - Slope(s)
  - p-values
  - R-squared value as you adjust the model.

The app plot is an excellent way to see how the slope and intercept of a linear model can impact the fit of the model to the data. You can also see how the R-squared value changes as you adjust the model, which is a measure of how well the model fits the data. The closer the R-squared value is to 1, the better the model fits the data.


The data Im using here is from a built-in dataset in R called "mtcar". 

```{r, layout="l-body-outset"}
library(knitr)
kable(head(mtcars))
```

After looking at the different variables in this dataset, lets see how each of them can be modeled as a linear regression. The blue line represents the best-fitting line (i.e., the line that minimizes the distance between the data and the model -- the line with the lowest RSS)

```{r echo=FALSE}
library(shiny)
library(shinyjs)

knitr::include_app("https://tehilla-mechera-ostrovsky.shinyapps.io/Scatter_Linear/?_ga=2.32885118.1162650381.1683277660-2121495814.1683277660", 
  height = "600px")
```



In conclusion, linear models are a powerful tool in statistics, and the interactive Shiny app plot provides an excellent way to explore the concepts. With the app, you can experiment with different linear models and see how they fit the data. I hope you found this post informative and helpful, and please don't hesitate to leave any comments or questions below!





### Homework solution:

#### Here is the dataaset:
```{r}
dat_salary <-  data.frame("iq" = c(120, 110,  100, 135, 140),
                          "monthly salary" = c(2500, 2300, 2400, 3000, 2100))

dat_salary
```


#### Here is a plot of the data, as per usual:


```{r}
ggplot(dat_salary,  aes(x = iq, y = monthly.salary)) +
  geom_point(col = "blue", alpha = .4) + 
  geom_smooth(method = "lm", col = "red") +
  theme_minimal()
```



#### And the model's output:


```{r}
summary(lm(dat_salary$monthly.salary ~ dat_salary$iq))
```



