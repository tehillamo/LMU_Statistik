---
title: "Basic concepts in Statistics"
description: |
  Linear Regression
author:
  - name: Tehilla Ostrovsky
    url: https://github.com 
    affiliation: LMU
    #affiliation_url: 
date: "`r Sys.Date()`"
output: distill::distill_article
---

### Welcome to the post about linear models in statistics! 

Linear models are an essential tool in statistics used to model the relationship between a dependent variable and one or more independent variables. 

In this post, we'll explore linear models in more detail and introduce an interactive Shiny app plot that will help you visualize the concepts.

Before we dive into the interactive part of this post (in which you get to explorer different regression lines using different variables), let's first define what a linear model is. 

A linear model is a mathematical equation that represents a **linear** relationship between two or more variables. 

The simplest form of a linear model is a straight line equation of the form:

$Y_{i} = \alpha_{0} + \beta_{1} \times X_{1}$

Where $Y_{i}$ is the dependent variable and represent the *expected value* of all $y_{i}$ (all single data points) given a specific value of $X_{i}$ 


### We are going to use a very simple example. We will try to fit a model that aims to predict the relationship between individuals height and weight. 

### here is a fun illustration of the simple model:

<video width="500" height="300" float = "left" controls><source src="img/CoordExmp.mp4" type="video/mp4"></video>



### Let us look at a simple example with little number of data points (just so we can get the feeling of whats going on under the hood)


```{r echo=FALSE, warning=FALSE, message=FALSE}

dat <-  data.frame(
  "Height" = c(170, 180, 167, 165, 191),
  "Weight" = c(60, 75, 59, 88, 91))
dat
```


#### Here are the averages of both the weights and heights. We will need those to calculate the intercpet and the slope of the model. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
mean_weight <-  mean(dat$Weight)

mean_height <-  mean(dat$Height)

mean <- c(mean_weight, mean_height) 

mean
```


#### Remember how I told you that I am a fan of plotting the data? 

#### We will use a scatterplot this time.

```{r}
library(ggplot2)
ggplot(data = dat, aes(y = Weight, x = Height)) +
  geom_point(alpha = .7, color = "red") +
  geom_hline(yintercept = mean_weight, color="blue", linetype = "dashed") +
  geom_vline(xintercept = mean_height, color = "green", linetype = "dashed") + 
  #geom_smooth(method = "lm") +
  theme_classic() 
```








### Here is how $\beta$ calculated. 
A quick reminder: 	Î² represents how much the regression line will rise or fall. 

We interpret the $\beta$  as the average increase in the Dependent variable (AV) when we increase the independent variable (UV) by one unit. For Example, if we increase the Height by 1cm, the average predicted increase of weight is .....

$$\frac{\sum_{i=1}^{n}(x_{i} -\hat{x})\times(y_{i}-\hat{y}))}{\sum_{i=1}^{n}(x_{1} - \hat{x})^2}$$

Which can be also written as:


$$\frac{cov(X,Y)}{S_{x}^2}$$



#### Lets start!

```{r echo=FALSE, warning=FALSE, message=FALSE, }
dat[,"xi - mean(x)"] <-  dat$Height - mean_height
dat[,"yi - mean(y)"] <-  dat$Weight - mean_weight

dat[, "sqr(xi - mean(x)"] <- (dat$Height - mean_height)**2
dat[, "(xi - mean(x)) X (yi - mean(y))"] <-  dat[,"xi - mean(x)"] * dat[,"yi - mean(y)"] 

dat
```

#### Now we have everything to compute the $cov(X, Y)$ and the $S_{X}^2$

For $cov(X, Y)$ we just need to compute the sum of the **6th column**

```{r}
sum(dat[,"(xi - mean(x)) X (yi - mean(y))"])
```
For $S_{x}^2$ we just need to compute the sum of the **5th column**

```{r}
sum(dat[, "sqr(xi - mean(x)"])
```


#### The slope, is therefore, 0.

```{r echo=FALSE}
328.2/469.2
```

$\beta = \frac{328.2}{469.2} = .69$ 

This means that, for every 1cm increase in height we expect to see an increase of .69KG. 



### here is how $\alpha$ is calculated: 
$$\overline{y} = \alpha_{0} + .69\times \overline{x}\beta$$

We know both means of $x$ and $y$ (from the calculation above)


If we rearrange the equation to solve for $\alpha_{0}$, we get 

$$-\alpha_{0} = -\overline{y} + .69\times \overline{x}$$ 
--> $$\alpha_{0} = \overline{y} - .69\times \overline{x}$$



```{r}
alpha_0 = mean_weight - .6994885*(mean_height)
alpha_0
```



OK, enough with the hard, tiring work of calculating everything by hand.... for exactly this reason we have R (ðŸ˜ƒ).

We will use the function lm(), which which for Linear Model. We will wrap it with the function summary(), which provides us with the result summary of our model's results. 



```{r}
sum_lm <-  summary(lm(dat$Weight ~ dat$Height))
sum_lm
```



Lets add this line to our scatterplot from before.

```{r}
ggplot(data = dat, aes(y = Weight, x = Height)) +
  geom_point(alpha = .7, color = "red") +
  geom_hline(yintercept = mean_weight, color="blue", linetype = "dashed") +
  geom_vline(xintercept = mean_height, color = "green", linetype = "dashed") + 
  geom_smooth(method = "lm") +
  theme_classic() 
```



### Lastly, here is how the standard error term is calculated. This term tells us the distance between the data points with respect to their y values and the best fitting line.  

$$\sigma^2 = S^2 = \frac{\sum_{i=1}^n(Y_{i} - \hat{Y})}{n-2}$$


We need the predicted value for each person in our dataset. We will use the predict.lm() function that will output the predicted values fro every person based on the model lm(weight ~ Height.



```{r}
lm <-  lm(dat$Weight ~ dat$Height)
dat[,"predicted_y"] <-  predict.lm(lm)

dat[,"predicted_y"] 

dat[, "sqr(y-perd(y))"] <- (dat$Weight - dat$predicted_y)**2

dat[, "sqr(y-perd(y))"]

sum <- sum(dat[, "sqr(y-perd(y))"])

sqrt(sum/3)



```

---


Here is how the t-distribution under the  $H_{0}$ looks like:

```{r}
library(latex2exp)
x <- rt(100000, df = 3)
hist(x, 
     breaks = 'Scott', 
     freq = FALSE, 
     xlim = c(-5,5), 
     ylim = c(0,0.4),
     xlab = '', 
     col = "lightgrey",
     main = (TeX('Histogram for a $\\t$-distribution with 3 degrees of freedom (df)')), cex.main=0.9)

curve(dt(x, df = 3), from = -5, to = 5, n = 5000, col = 'red', lwd = 3, add = T)
abline(v = 0.721, col = "green", lwd = 2,  lty = 'dashed')
abline(v = 0.387, col = "blue", lwd = 2,  lty = 'dashed')

```

---

#### And a small toy-example for you to play around with ðŸ˜‰


Let's move onto the interactive Shiny app plot. The plot allows you to visualize the relationship between two variables and fit a linear model to the data. To use the app, follow these steps:

1. Choose the variables from the dropdown menu.

2. Observe how the line changes to fit the data points better.

3. Observe how the changes in: 
  - Intercept
  - Slope(s)
  - p-values
  - R-squared value as you adjust the model.

The app plot is an excellent way to see how the slope and intercept of a linear model can impact the fit of the model to the data. You can also see how the R-squared value changes as you adjust the model, which is a measure of how well the model fits the data. The closer the R-squared value is to 1, the better the model fits the data.


The data Im using here is from a built-in dataset in R called "mtcar". 

```{r, layout="l-body-outset"}
library(knitr)
kable(head(mtcars))
```

After looking at the different variables in this dataset, lets see how each of them can be modeled as a linear regression. The blue line represents the best-fitting line (i.e., the line that minimizes the distance between the data and the model -- the line with the lowest RSS)

```{r echo=FALSE}
library(shiny)
library(shinyjs)

knitr::include_app("https://tehilla-mechera-ostrovsky.shinyapps.io/Scatter_Linear/?_ga=2.32885118.1162650381.1683277660-2121495814.1683277660", 
  height = "600px")
```



In conclusion, linear models are a powerful tool in statistics, and the interactive Shiny app plot provides an excellent way to explore the concepts. With the app, you can experiment with different linear models and see how they fit the data. I hope you found this post informative and helpful, and please don't hesitate to leave any comments or questions below!
